{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag c памятью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\22456229\\AppData\\Local\\Temp\\ipykernel_15256\\1139157137.py:5: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  file_path = \"C:\\Work\\Rag\\papers\\нацстратегия.pdf\"\n"
     ]
    }
   ],
   "source": [
    "#llm и loadder\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "credentials = os.getenv('credentials')\n",
    "llm = GigaChat(auth_url = 'https://sm-auth-sd.prom-88-89-apps.ocp-geo.ocp.sigma.sbrf.ru/api/v2/oauth',credentials=credentials, verify_ssl_certs=False)\n",
    "\n",
    "file_path = \"C:\\Work\\Rag\\papers\\нацстратегия.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22456229\\AppData\\Local\\Temp\\ipykernel_15256\\4073150009.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# embeddings and retriever\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    "    )\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст в истории чата,\"\n",
    "    \"сформулируйте отдельный вопрос, который можно понять \"\n",
    "    \"без истории чата. НЕ отвечайте на вопрос - просто переформулируйте его,\"\n",
    "    \"если нужно, а в противном случае верните как есть. Отвечай на языке, на котором был задан вопрос\"\n",
    "    # \"Given a chat history and the latest user question \"\n",
    "    # \"which might reference context in the chat history, \"\n",
    "    # \"formulate a standalone question which can be understood \"\n",
    "    # \"without the chat history. Do NOT answer the question, \"\n",
    "    # \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"Вы являетесь помощником при выполнении заданий по поиску ответов на вопросы.\"\n",
    "    \"Используйте приведенные ниже фрагменты из извлеченного контекста для ответа\"\n",
    "    \"на вопрос. Если вы не знаете ответа, скажите, что вы\"\n",
    "    \"не знаете. Используйте максимум три предложения и старайтесь,\"\n",
    "    \"чтобы ответ был кратким. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### История чата ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Цель этого документа'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Цель этого документа',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 5}, page_content='изменяющимся условиям, является сложной научно -технической проблемой, решение которой \\nнаходится на пересечении различных сфер научного знания - естественно -научной, технической и \\nсоци ально -гуманитарной. Решение этой проблемы может привести не только к позитивным \\nизменениям в ключевых сферах жизнедеятельности, но и к негативным последствиям, вызванным \\nсоциальными и технологическими изменениями, которые сопутствуют развитию технологий \\nискусственного интеллекта.  \\n10. Стремительное развитие технологий искусственного интеллекта сопровождается \\nсущественным ростом как государственных, так и частных инвестиций в их развитие, а также в \\nразработку прикладных технологических решений на основе искус ственного интеллекта. По \\nоценкам международных экспертов, инвестиции в технологии искусственного интеллекта выросли \\nс 2014 по 2017 год в три раза и составили около 40 млрд. долларов США. В 2018 году мировой рынок'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 5}, page_content='развития искусственного интеллекта, что позволит российским технологиям искусственного \\nинтеллекта занять значительную долю  мирового рынка.  \\n13. Российская Федерация обладает существенным потенциалом для того, чтобы стать одним \\nиз международных лидеров в развитии и использовании технологий искусственного интеллекта. \\nЭтому способствуют высокий уровень базового физико -математичес кого образования, сильная \\nестественно -научная школа, наличие компетенций в области моделирования и программирования.'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 7}, page_content='секунды создают изображения на любую тему по заданному текстовому описанию или наброску, \\nчто создает угрозу распространения запрещенной информации, нарушения авторских прав и \\nгенерации ошибочных сведений.  \\n(п. 17(4) введен Указом  Президента РФ от 15.02.2024 N  124)  \\n17(5). Искусственный интеллект окажет существенное влияние на экономический рост в мире. \\nПо оценкам экспертов, дальнейшее развитие больших генеративных моделей может вызвать \\nрезкое повышение производительности труда, которое приведет к увеличению мир ового валового \\nвнутреннего продукта на 1 - 2 процента ежегодно и позволит повысить оплату труда специалистов \\nво всех отраслях экономики за счет увеличения объема выпуска продукции (товаров, работ, услуг) \\nи улучшения ее качества. Таким образом, использовани е искусственного интеллекта позволит \\nобеспечить переход Российской Федерации к типу организации экономических отношений, при'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 20}, page_content='благоприятных нормативно -правовых условий для разрабо тки, внедрения и использования \\nтехнологий искусственного интеллекта и решений, разработанных на их основе, с учетом \\nобеспечения защиты прав и свобод человека и безопасности Российской Федерации. Для \\nдостижения поставленной цели и стимулирования развития и использования технологий \\nискусственного интеллекта необходимо совершенствование нормативно -правового регулирования \\nв части, касающейся взаимодействия человека с искусственным интеллектом, устранение \\nизлишних нормативных барьеров и распространение соответст вующих этических норм, а также \\nиспользование лучших мировых практик нормативно -правового регулирования. При этом такое \\nрегулирование не должно замедлить темпы разработки и внедрения решений в области \\nискусственного интеллекта.  \\n51(10). Основными принципами нормативно -правового регулирования общественных \\nотношений, связанных с развитием и использованием технологий искусственного интеллекта,')],\n",
       " 'answer': 'Цель данного документа - предоставить информацию о стремительном развитии технологий искусственного интеллекта, их влиянии на различные сферы жизни, а также о необходимости нормативно-правового регулирования для обеспечения безопасности и этичности использования этих технологий.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conversational_rag_chain.invoke(\n",
    "    {\"input\": 'И какие тенденции?'},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'И какие тенденции?',\n",
       " 'chat_history': [HumanMessage(content='Цель этого документа', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Цель данного документа - предоставить информацию о стремительном развитии технологий искусственного интеллекта, их влиянии на различные сферы жизни, а также о необходимости нормативно-правового регулирования для обеспечения безопасности и этичности использования этих технологий.', additional_kwargs={}, response_metadata={})],\n",
       " 'context': [Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 15}, page_content='обеспечения опережающего развития искусственного интеллекта являются:  \\nа) реализация инновационных задач в области искусственного интеллекта, в том числе по \\nразработке и адаптации больших фундаментальных моделей для их применения в отраслях  \\nэкономики, по формированию условий для создания сильного искусственного интеллекта, \\nповышения доступности искусственного интеллекта в целях его использования в повседневной \\nжизни путем поддержки и создания новых исследовательских центров в области искусст венного \\nинтеллекта;  \\nб) стимулирование научных исследований и разработок в области искусственного интеллекта \\nза счет бюджетных ассигнований федерального бюджета, бюджетов субъектов Российской \\nФедерации и средств внебюджетных источников;  \\nв) увеличение по сра внению с 2023 годом объема финансирования междисциплинарных \\nисследовательских проектов в области искусственного интеллекта в различных отраслях'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 16}, page_content='искусственного интеллекта, направленных в том числе на анализ последствий широкомасштабного \\nвнедрения технологий искусственного интеллекта, оценку его влияния на когнитивные способности \\nчеловека, рисков замещения человеческого  труда искусственным интеллектом;  \\nк) формирование единого механизма взаимодействия научных групп по вопросам \\nисследований в области искусственного интеллекта.  \\n51(4). Качественное развитие российской науки в области искусственного интеллекта должно \\nосуществ ляться путем:  \\nа) закрепления показателей отнесения научных исследований в области искусственного \\nинтеллекта к высокому уровню (публикации на конференциях в области искусственного интеллекта \\nуровня А*) и среднему уровню (публикации на конференциях в области  искусственного интеллекта \\nуровня А и в научных журналах первого квартиля \"Белого списка\");  \\nб) установления возможности корректировать программы исследований по искусственному \\nинтеллекту на ежегодной основе.'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 5}, page_content='разработанных на основе искусственного интеллекта;  \\nб) высокая степень влияния технологических решений, разработанных на основе \\nискусственного интеллекта, на результативность деятельности организаций и человека, в том числе \\nсвязанной с принятием управленческих решений;  \\nв) высокая доступность инструментов (в том числе программ для ЭВМ с открытым кодом) для \\nразработки на основе искус ственного интеллекта технологических решений;  \\nг) потребность в обработке больших объемов данных, создаваемых как человеком, так и \\nтехническими устройствами, для повышения эффективности экономической и иной деятельности.  \\n12. Благодаря реализации настоящей С тратегии должны быть созданы условия для \\nэффективного взаимодействия государства, организаций, в том числе научных, и граждан в сфере \\nразвития искусственного интеллекта, что позволит российским технологиям искусственного \\nинтеллекта занять значительную долю  мирового рынка.'),\n",
       "  Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf', 'page': 11}, page_content='являются:  \\nа) повышение доступности инфраструктуры, необходимой для развития технологий \\nиску сственного интеллекта;  \\nб) поддержка организаций - разработчиков технологий искусственного интеллекта;  \\nв) поддержка научных исследований и разработок в целях обеспечения опережающего \\nразвития искусственного интеллекта;  \\nг) повышение уровня компетенций в обла сти искусственного интеллекта и уровня \\nинформированности граждан о технологиях искусственного интеллекта;')],\n",
       " 'answer': 'В документе представлены следующие тенденции: \\n\\n1. Развитие искусственного интеллекта происходит быстрыми темпами, что требует соответствующей поддержки и регулирования.\\n\\n2. Искусственный интеллект становится все более доступным и широко используется в различных отраслях экономики.\\n\\n3. Технологии искусственного интеллекта оказывают значительное влияние на результаты деятельности организаций и человека, включая принятие управленческих решений.\\n\\n4. Необходимость обработки больших объемов данных для повышения эффективности экономической и иной деятельности.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Work\\\\Rag\\\\papers\\\\нацстратегия.pdf'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# извлечение метадаты\n",
    "ans['context'][0].metadata['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты с Loaderами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\22456229\\AppData\\Local\\Temp\\ipykernel_9580\\3349939310.py:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  test_filepath = 'C:\\Work\\Rag\\papers\\paper2.pdf'\n"
     ]
    }
   ],
   "source": [
    "test_filepath = 'C:\\Work\\Rag\\papers\\paper2.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader_pypdf = PyPDFLoader(file_path=test_filepath)\n",
    "docs_pypdf = loader_pypdf.lazy_load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "splits_pypdfl = text_splitter.split_documents(docs_pypdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 0}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='Источники: PwC MENA , Подсчет затрат-расходов ИИ БВСА, IDC➢Наибольшие выгоды в регионе БВСА получают страны Персидского залива и Египет, а ОАЭ являются\\nкрупнейшими бенефициарамиАНАЛИЗ ЗАТРАТ И ВЫГОД ИИ\\n▪Распространение искусственного интеллекта в \\nразличных секторах и отраслях промышленности \\nтребует значительных первоначальных инвестиций в его \\nинфраструктуру. В регионе Ближнего Востока и Африки будет наблюдаться самый быстрый в мире \\nрост расходов в ИИ.\\n▪Ожидается, что в 2022-2026 годах объем расходов \\nувеличится в среднем на 30% в годовом исчислении и \\nсоставит 6,4 миллиарда долларов в 2026 году,  чему будут \\nспособствовать ОАЭ и Саудовская Аравия, наиболее \\nдинамично развивающиеся экономики региона.\\n▪По оценкам Международной корпорации обработки \\nданных (IDC), в обозримом будущем (  2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='способствовать ОАЭ и Саудовская Аравия, наиболее \\nдинамично развивающиеся экономики региона.\\n▪По оценкам Международной корпорации обработки \\nданных (IDC), в обозримом будущем (  2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться \\nчетырьмя ключевыми отраслями: банковским делом, \\nрозничной торговлей, услугами для органов \\nгосударственного управления и производственной \\nсферой.▪Выгоды, связанные с предполагаемыми затратами на ИИ-\\nинфраструктуру, значительно перевешивают\\nзатраченные средства.\\n▪Ожидается, что к 2030 году на регион БВСА будет\\nприходиться 2% от общего объема глобальных\\nпреимуществ искусственного интеллекта. Это\\nэквивалентно 320 миллиардам долларов США.ЗатратыВыгодыЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='ОАЭ В ИНДЕКСАХ ПО ИИ\\n➢ОАЭ - восходящая звезда на арене искусственного интеллекта, позиционирующая себя как\\nглобальный центр искусственного интеллекта\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих \\nстран в области ИИ. Оксфордский индекс готовности к искусственному интеллекту (Oxford AI Readiness Index) призван отразить возможности и готовность государственного сектора использовать потенциал искусственного интеллекта в своей деятельности и \\nпредоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.\\nОпрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='предоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.\\nОпрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.\\nИсточники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ , Gulf Today,ZawyaS▪91% компаний ОАЭ считают, что ИИ имеет решающее \\nзначение для будущего роста.\\n▪83% предприятий ОАЭ были готовы интегрировать\\nгенеративный ИИ в свою деятельность.\\n▪51% опрошенных предприятий сообщили о\\nмасштабном внедрении искусственного интеллекта,\\nохватывающем все функции.Главные выводы исследования\\nЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_pypdfl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.7 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loader_miner = PDFMinerLoader(file_path=test_filepath, extract_images=False, concatenate_pages=True)\n",
    "docs_miner = loader_miner.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "splits_miner = text_splitter.split_documents(docs_miner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf'}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ\\n\\n\\x0cЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ\\n\\nАНАЛИЗ ЗАТРАТ И ВЫГОД ИИ\\n\\n➢ Наибольшие выгоды в регионе БВСА получают страны Персидского залива и Египет, а ОАЭ являются\\n\\nкрупнейшими бенефициарами\\n\\nЗатраты\\n\\n▪\\n\\nРаспространение искусственного интеллекта в \\nразличных секторах и отраслях промышленности \\nтребует значительных первоначальных инвестиций в его \\nинфраструктуру. В регионе Ближнего Востока и \\nАфрики будет наблюдаться самый быстрый в мире \\nрост расходов в ИИ.\\n\\n▪ Ожидается, что в 2022-2026 годах объем расходов \\n\\nувеличится в среднем на 30% в годовом исчислении и \\nсоставит 6,4 миллиарда долларов в 2026 году,  чему будут \\nспособствовать ОАЭ и Саудовская Аравия, наиболее \\nдинамично развивающиеся экономики региона.\\n\\n▪ По оценкам Международной корпорации обработки'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf'}, page_content='увеличится в среднем на 30% в годовом исчислении и \\nсоставит 6,4 миллиарда долларов в 2026 году,  чему будут \\nспособствовать ОАЭ и Саудовская Аравия, наиболее \\nдинамично развивающиеся экономики региона.\\n\\n▪ По оценкам Международной корпорации обработки \\n\\nданных (IDC), в обозримом будущем (  2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться \\nчетырьмя ключевыми отраслями: банковским делом, \\nрозничной торговлей, услугами для органов \\nгосударственного управления и производственной \\nсферой.\\n\\nВыгоды\\n▪ Выгоды, связанные с предполагаемыми затратами на ИИ-\\n\\nинфраструктуру, значительно перевешивают\\nзатраченные средства.\\n\\n▪ Ожидается, что к 2030 году на регион БВСА будет\\nприходиться 2% от общего объема глобальных\\nпреимуществ искусственного интеллекта. Это\\nэквивалентно 320 миллиардам долларов США.\\n\\nИсточники: PwC MENA, Подсчет затрат-расходов ИИ БВСА, IDC'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf'}, page_content='▪ Ожидается, что к 2030 году на регион БВСА будет\\nприходиться 2% от общего объема глобальных\\nпреимуществ искусственного интеллекта. Это\\nэквивалентно 320 миллиардам долларов США.\\n\\nИсточники: PwC MENA, Подсчет затрат-расходов ИИ БВСА, IDC\\n\\n\\x0cЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ\\n\\nОАЭ В ИНДЕКСАХ ПО ИИ\\n\\n➢ ОАЭ - восходящая звезда на арене искусственного интеллекта, позиционирующая себя как\\n\\nглобальный центр искусственного интеллекта\\n\\nГлавные выводы исследования\\n\\n▪\\n\\n91% компаний ОАЭ считают, что ИИ имеет решающее \\nзначение для будущего роста.\\n\\n▪ 83% предприятий ОАЭ были готовы интегрировать\\n\\nгенеративный ИИ в свою деятельность.\\n\\n▪\\n\\n51% опрошенных предприятий сообщили о\\nмасштабном внедрении искусственного интеллекта,\\nохватывающем все функции.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf'}, page_content='▪\\n\\n91% компаний ОАЭ считают, что ИИ имеет решающее \\nзначение для будущего роста.\\n\\n▪ 83% предприятий ОАЭ были готовы интегрировать\\n\\nгенеративный ИИ в свою деятельность.\\n\\n▪\\n\\n51% опрошенных предприятий сообщили о\\nмасштабном внедрении искусственного интеллекта,\\nохватывающем все функции.\\n\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих \\nстран в области ИИ. Оксфордский индекс готовности к искусственному интеллекту (Oxford AI Readiness Index) призван отразить возможности и готовность государственного сектора использовать потенциал искусственного интеллекта в своей деятельности и \\nпредоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf'}, page_content='Опрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.\\nИсточники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ, Gulf Today,ZawyaS\\n\\n\\x0cЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ\\n\\nВЗГЛЯД НА ИИ ИЗ ОАЭ\\n\\n➢ ОАЭ разрабатывают всеобъемлющую стратегию в области искусственного интеллекта “Национальная\\n\\nстратегия ОАЭ в области искусственного интеллекта до 2031 года”.\\n\\n▪ По прогнозам PwC, к 2030 году вклад искусственного интеллекта \\n\\nв ВВП составит 335 млрд. дирх. ОАЭ, что эквивалентно \\nувеличению на 26%.\\n\\n▪ Совет ОАЭ по ИИ и блокчейну следит за реализацией ИИ \\n\\nстратегии.\\n\\n▪ Стратегия направлена на:\\n\\n❖ создание образа места назначения для тематики ИИ\\n❖ повышение конкурентоспособности ОАЭ в приоритетных \\n\\nсекторах за счет внедрения ИИ')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_miner[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# loader_miner_im = PDFMinerLoader(file_path=test_filepath, extract_images=True, concatenate_pages=True)\n",
    "# docs_miner_im = loader_miner_im.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits_miner_im = text_splitter.split_documents(docs_miner_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits_miner[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_pypdf_im = PyPDFLoader(file_path=test_filepath, extract_images=True)\n",
    "# docs_pypdf_im = loader_pypdf_im.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits_pypdfl_im = text_splitter.split_documents(docs_pypdf_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits_pypdfl_im[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://token:****@sberosc.sigma.sbrf.ru/repo/pypi/simple\n",
      "Requirement already satisfied: pymupdf in c:\\work\\rag\\.venv\\lib\\site-packages (1.24.10)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in c:\\work\\rag\\.venv\\lib\\site-packages (from pymupdf) (1.24.10)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pymy = PyMuPDFLoader(file_path = test_filepath, extract_images=False)\n",
    "docs_pymy = loader_pymy.lazy_load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "splits_pymy = text_splitter.split_documents(docs_pymy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Mohamed Walid Lotfy', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creationDate': \"D:20240206101828+04'00'\", 'modDate': \"D:20240731151649+03'00'\", 'trapped': ''}, page_content='AI SIGNIFICANCE IN THE UAE ECONOMY'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1, 'total_pages': 16, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Mohamed Walid Lotfy', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creationDate': \"D:20240206101828+04'00'\", 'modDate': \"D:20240731151649+03'00'\", 'trapped': ''}, page_content='IDC\\uf0d8Amongst the biggest gainers in the MENA region are concentrated in the Gulf region and Egypt \\nw ith the UAE being the largest beneficiary Exhibit 2: PwC estimates AI COST-\\nBENEFIT ANALYSIS\\uf0a7AI diffusion into the different sectors and industries require significant upfront investment in its infrastructure. The Middle East and Africa (MENA) region will see the world’s fastest spending growth in AI.\\uf0a7The spent is expected to surge at a compound annual rate of 30% over the 2022–2026 period to $6.4 billion in 2026, driven by the UAE and Saudi Arabia, the region’s most vibrant economies.\\uf0a7The International Data Corporation (IDC) estimates that in the foreseeable future (2023-2024) 44% of the MENA region’s expenditure on AI will be absorbed by four key industries: banking, retail, government service verticals and manufacturing'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1, 'total_pages': 16, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Mohamed Walid Lotfy', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creationDate': \"D:20240206101828+04'00'\", 'modDate': \"D:20240731151649+03'00'\", 'trapped': ''}, page_content='.\\uf0a7The benefits generated from the cost estimated to pour into the AI infrastructure outweighs heavily the amounts spent. \\uf0a7The MENA region is expected to accrue 2% of the total global benefits of AI in 2030. This is equivalent to US$320 \\nb\\nillion.\\nCost\\nBenefit\\nAI SIGNIFICANCE IN THE UAE ECONOMY'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2, 'total_pages': 16, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Mohamed Walid Lotfy', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creationDate': \"D:20240206101828+04'00'\", 'modDate': \"D:20240731151649+03'00'\", 'trapped': ''}, page_content='THE UAE IN AI INDICES\\uf0d8The UAE is a rising star on the AI arena, positioning itself as a \\ng lobal hub for AITortoise Global AI Index28th in 2022 out of 62 countriesSources: Tortoise Global AI Index, Oxford AI Readiness Index, Gulf Today,ZawyaS Note: Tortoise Global AI Index is a composite index that measures investment, innovation, and implementation of AI, along with a set of interrelated measures to assess the multidimensional concept of AI globally for 62 leading countries in AI space.Oxford AI Readiness Index aims to capture the capacity and readiness of the public sector to exploit the potential of AI and uti\\nlize it in their operations and delivery of government services. It provides a ranking for 160 countries around the world.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2, 'total_pages': 16, 'format': 'PDF 1.7', 'title': 'PowerPoint Presentation', 'author': 'Mohamed Walid Lotfy', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creationDate': \"D:20240206101828+04'00'\", 'modDate': \"D:20240731151649+03'00'\", 'trapped': ''}, page_content='lize it in their operations and delivery of government services. It provides a ranking for 160 countries around the world. \\nThe survey was conducted via Coursera to businesses residing in the UAE (N=<500) and published on Zawya in July 2023. Oxford AI Readiness Index19th in 2021out of 160 countries\\uf0a791% of UAE businesses consider AI crucial for future growth.\\uf0a783% of UAE businesses were ready to integrate generative AI into their operations.\\uf0a751% of the surveyed businesses revealed extensive \\ni\\nmplementation covering all functions.\\nSurvey findings\\nAI SIGNIFICANCE IN THE UAE ECONOMY')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_pymy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf',\n",
       " 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 16,\n",
       " 'format': 'PDF 1.7',\n",
       " 'title': 'PowerPoint Presentation',\n",
       " 'author': 'Mohamed Walid Lotfy',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Microsoft® PowerPoint® for Microsoft 365',\n",
       " 'producer': 'Microsoft® PowerPoint® for Microsoft 365',\n",
       " 'creationDate': \"D:20240206101828+04'00'\",\n",
       " 'modDate': \"D:20240731151649+03'00'\",\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_pymy[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_pymy[0].metadata['creationDate'][2:6] #например так можно достать год"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://token:****@sberosc.sigma.sbrf.ru/repo/pypi/simple\n",
      "Requirement already satisfied: pdfplumber in c:\\work\\rag\\.venv\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\work\\rag\\.venv\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\work\\rag\\.venv\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\work\\rag\\.venv\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\work\\rag\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.12)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\work\\rag\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\work\\rag\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\work\\rag\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_Plumber = PDFPlumberLoader(file_path=test_filepath)\n",
    "docs_Plumber = loader_Plumber.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "splits_Plumber = text_splitter.split_documents(docs_Plumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 0, 'total_pages': 16, 'Author': 'Mohamed Walid Lotfy', 'CreationDate': \"D:20240206101828+04'00'\", 'Creator': 'Microsoft® PowerPoint® for Microsoft 365', 'ModDate': \"D:20240731151649+03'00'\", 'Producer': 'Microsoft® PowerPoint® for Microsoft 365', 'Title': 'PowerPoint Presentation'}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1, 'total_pages': 16, 'Author': 'Mohamed Walid Lotfy', 'CreationDate': \"D:20240206101828+04'00'\", 'Creator': 'Microsoft® PowerPoint® for Microsoft 365', 'ModDate': \"D:20240731151649+03'00'\", 'Producer': 'Microsoft® PowerPoint® for Microsoft 365', 'Title': 'PowerPoint Presentation'}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ АНАЛИЗ ЗАТРАТ И ВЫГОД ИИ\\n➢ Наибольшие выгоды в регионе БВСА получают страны Персидского залива и Египет, а ОАЭ являются\\nкрупнейшими бенефициарами\\nВыгоды\\nЗатраты\\n▪ Распространение искусственного интеллекта в ▪ Выгоды, связанные с предполагаемыми затратами на ИИ-\\nразличных секторах и отраслях промышленности инфраструктуру, значительно перевешивают\\nтребует значительных первоначальных инвестиций в его затраченные средства.\\nинфраструктуру. В регионе Ближнего Востока и\\nАфрики будет наблюдаться самый быстрый в мире ▪ Ожидается, что к 2030 году на регион БВСА будет\\nприходиться 2% от общего объема глобальных\\nрост расходов в ИИ.\\nпреимуществ искусственного интеллекта. Это\\n▪ Ожидается, что в 2022-2026 годах объем расходов эквивалентно 320 миллиардам долларов США.\\nувеличится в среднем на 30% в годовом исчислении и\\nсоставит 6,4 миллиарда долларов в 2026 году, чему будут'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1, 'total_pages': 16, 'Author': 'Mohamed Walid Lotfy', 'CreationDate': \"D:20240206101828+04'00'\", 'Creator': 'Microsoft® PowerPoint® for Microsoft 365', 'ModDate': \"D:20240731151649+03'00'\", 'Producer': 'Microsoft® PowerPoint® for Microsoft 365', 'Title': 'PowerPoint Presentation'}, page_content='рост расходов в ИИ.\\nпреимуществ искусственного интеллекта. Это\\n▪ Ожидается, что в 2022-2026 годах объем расходов эквивалентно 320 миллиардам долларов США.\\nувеличится в среднем на 30% в годовом исчислении и\\nсоставит 6,4 миллиарда долларов в 2026 году, чему будут\\nспособствовать ОАЭ и Саудовская Аравия, наиболее\\nдинамично развивающиеся экономики региона.\\n▪ По оценкам Международной корпорации обработки\\nданных (IDC), в обозримом будущем ( 2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться\\nчетырьмя ключевыми отраслями: банковским делом,\\nрозничной торговлей, услугами для органов\\nгосударственного управления и производственной\\nсферой.\\nИсточники: PwC MENA, Подсчет затрат-расходов ИИ БВСА, IDC'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2, 'total_pages': 16, 'Author': 'Mohamed Walid Lotfy', 'CreationDate': \"D:20240206101828+04'00'\", 'Creator': 'Microsoft® PowerPoint® for Microsoft 365', 'ModDate': \"D:20240731151649+03'00'\", 'Producer': 'Microsoft® PowerPoint® for Microsoft 365', 'Title': 'PowerPoint Presentation'}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ ОАЭ В ИНДЕКСАХ ПО ИИ\\n➢ ОАЭ - восходящая звезда на арене искусственного интеллекта, позиционирующая себя как\\nглобальный центр искусственного интеллекта\\nГлавные выводы исследования\\n▪ 91% компаний ОАЭ считают, что ИИ имеет решающее\\nзначение для будущего роста.\\n▪ 83% предприятий ОАЭ были готовы интегрировать\\nгенеративный ИИ в свою деятельность.\\n▪ 51% опрошенных предприятий сообщили о\\nмасштабном внедрении искусственного интеллекта,\\nохватывающем все функции.\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'file_path': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2, 'total_pages': 16, 'Author': 'Mohamed Walid Lotfy', 'CreationDate': \"D:20240206101828+04'00'\", 'Creator': 'Microsoft® PowerPoint® for Microsoft 365', 'ModDate': \"D:20240731151649+03'00'\", 'Producer': 'Microsoft® PowerPoint® for Microsoft 365', 'Title': 'PowerPoint Presentation'}, page_content='охватывающем все функции.\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих\\nстран в области ИИ. Оксфордский индекс готовности к искусственному интеллекту (Oxford AI Readiness Index) призван отразить возможности и готовность государственного сектора использовать потенциал искусственного интеллекта в своей деятельности и\\nпредоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.\\nОпрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.\\nИсточники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ, Gulf Today,ZawyaS')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_Plumber[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 0}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='Источники: PwC MENA , Подсчет затрат-расходов ИИ БВСА, IDC➢Наибольшие выгоды в регионе БВСА получают страны Персидского залива и Египет, а ОАЭ являются\\nкрупнейшими бенефициарамиАНАЛИЗ ЗАТРАТ И ВЫГОД ИИ\\n▪Распространение искусственного интеллекта в \\nразличных секторах и отраслях промышленности \\nтребует значительных первоначальных инвестиций в его \\nинфраструктуру. В регионе Ближнего Востока и Африки будет наблюдаться самый быстрый в мире \\nрост расходов в ИИ.\\n▪Ожидается, что в 2022-2026 годах объем расходов \\nувеличится в среднем на 30% в годовом исчислении и \\nсоставит 6,4 миллиарда долларов в 2026 году,  чему будут \\nспособствовать ОАЭ и Саудовская Аравия, наиболее \\nдинамично развивающиеся экономики региона.\\n▪По оценкам Международной корпорации обработки \\nданных (IDC), в обозримом будущем (  2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться \\nчетырьмя ключевыми отраслями: банковским делом, \\nрозничной торговлей, услугами для органов'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='данных (IDC), в обозримом будущем (  2023-2024) 44%\\nмировых ИИ-расходов региона БВСА будут покрываться \\nчетырьмя ключевыми отраслями: банковским делом, \\nрозничной торговлей, услугами для органов \\nгосударственного управления и производственной \\nсферой.▪Выгоды, связанные с предполагаемыми затратами на ИИ-\\nинфраструктуру, значительно перевешивают\\nзатраченные средства.\\n▪Ожидается, что к 2030 году на регион БВСА будет\\nприходиться 2% от общего объема глобальных\\nпреимуществ искусственного интеллекта. Это\\nэквивалентно 320 миллиардам долларов США.ЗатратыВыгодыЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='ОАЭ В ИНДЕКСАХ ПО ИИ\\n➢ОАЭ - восходящая звезда на арене искусственного интеллекта, позиционирующая себя как\\nглобальный центр искусственного интеллекта\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих \\nстран в области ИИ. Оксфордский индекс готовности к искусственному интеллекту (Oxford AI Readiness Index) призван отразить возможности и готовность государственного сектора использовать потенциал искусственного интеллекта в своей деятельности и \\nпредоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.\\nОпрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='Опрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.\\nИсточники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ , Gulf Today,ZawyaS▪91% компаний ОАЭ считают, что ИИ имеет решающее \\nзначение для будущего роста.\\n▪83% предприятий ОАЭ были готовы интегрировать\\nгенеративный ИИ в свою деятельность.\\n▪51% опрошенных предприятий сообщили о\\nмасштабном внедрении искусственного интеллекта,\\nохватывающем все функции.Главные выводы исследования\\nЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_pypdfl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "loader_ium = PyPDFium2Loader(file_path=test_filepath)\n",
    "docs_ium = loader_ium.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits_ium = text_splitter.split_documents(docs_ium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 0}, page_content='ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='Источники: PwC MENA, Подсчет затрат-расходов ИИ БВСА, IDC\\r\\n➢ Наибольшие выгоды в регионе БВСА получают страны Персидского залива и Египет, а ОАЭ являются\\r\\nкрупнейшими бенефициарами\\r\\nАНАЛИЗ ЗАТРАТ И ВЫГОД ИИ\\r\\n▪ Распространение искусственного интеллекта в \\r\\nразличных секторах и отраслях промышленности \\r\\nтребует значительных первоначальных инвестиций в его \\r\\nинфраструктуру. В регионе Ближнего Востока и \\r\\nАфрики будет наблюдаться самый быстрый в мире \\r\\nрост расходов в ИИ.\\r\\n▪ Ожидается, что в 2022-2026 годах объем расходов\\r\\nувеличится в среднем на 30% в годовом исчислении и\\r\\nсоставит 6,4 миллиарда долларов в 2026 году, чему будут\\r\\nспособствовать ОАЭ и Саудовская Аравия, наиболее\\r\\nдинамично развивающиеся экономики региона.\\r\\n▪ По оценкам Международной корпорации обработки\\r\\nданных (IDC), в обозримом будущем ( 2023-2024) 44%\\r\\nмировых ИИ-расходов региона БВСА будут покрываться\\r\\nчетырьмя ключевыми отраслями: банковским делом,\\r\\nрозничной торговлей, услугами для органов'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 1}, page_content='данных (IDC), в обозримом будущем ( 2023-2024) 44%\\r\\nмировых ИИ-расходов региона БВСА будут покрываться\\r\\nчетырьмя ключевыми отраслями: банковским делом,\\r\\nрозничной торговлей, услугами для органов\\r\\nгосударственного управления и производственной \\r\\nсферой.\\r\\n▪ Выгоды, связанные с предполагаемыми затратами на ИИ\\x02инфраструктуру, значительно перевешивают\\r\\nзатраченные средства.\\r\\n▪ Ожидается, что к 2030 году на регион БВСА будет\\r\\nприходиться 2% от общего объема глобальных\\r\\nпреимуществ искусственного интеллекта. Это\\r\\nэквивалентно 320 миллиардам долларов США.\\r\\nЗатраты Выгоды\\r\\nЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='ОАЭ В ИНДЕКСАХ ПО ИИ\\r\\n➢ ОАЭ - восходящая звезда на арене искусственного интеллекта, позиционирующая себя как\\r\\nглобальный центр искусственного интеллекта\\r\\nПримечание: Глобальный индекс ИИ Tortoise - это обзорный индекс, который измеряет инвестиции, инновации и внедрение ИИ, а также набор взаимосвязанных показателей для оценки многомерной концепции ИИ в глобальном масштабе для 62 ведущих \\r\\nстран в области ИИ. Оксфордский индекс готовности к искусственному интеллекту (Oxford AI Readiness Index) призван отразить возможности и готовность государственного сектора использовать потенциал искусственного интеллекта в своей деятельности и \\r\\nпредоставлении государственных услуг. В нем представлен рейтинг для 160 стран по всему миру.\\r\\nОпрос был проведен с помощью Coursera среди компаний, проживающих в ОАЭ (N=<500), и опубликован на Zawya в июле 2023 г.\\r\\nИсточники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ, Gulf Today,ZawyaS'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\paper2.pdf', 'page': 2}, page_content='Источники: Глобальный индекс искусственного интеллекта Tortoise, Оксфордский индекс готовности к ИИ, Gulf Today,ZawyaS\\r\\n▪ 91% компаний ОАЭ считают, что ИИ имеет решающее \\r\\nзначение для будущего роста.\\r\\n▪ 83% предприятий ОАЭ были готовы интегрировать\\r\\nгенеративный ИИ в свою деятельность.\\r\\n▪ 51% опрошенных предприятий сообщили о\\r\\nмасштабном внедрении искусственного интеллекта,\\r\\nохватывающем все функции.\\r\\nГлавные выводы исследования\\r\\nЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_ium[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы: быстрее всего PyMuPDFLoader + переводит, PDFPlumberLoader для таблиц и вроде лучше других (имхо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\22456229\\AppData\\Local\\Temp\\ipykernel_12440\\2190664826.py:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  loader_iumg = PyPDFium2Loader(file_path='C:\\Work\\Rag\\papers\\skan.pdf', extract_images=True)\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\pypdfium2\\_helpers\\textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "loader_iumg = PyPDFium2Loader(file_path='C:\\Work\\Rag\\papers\\skan.pdf', extract_images=True)\n",
    "docs_iumg = loader_iumg.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits_iumg = text_splitter.split_documents(docs_iumg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\skan.pdf', 'page': 0}, page_content='YKA3\\nHPE3HIEHTA POCCHYCKOY ΦEIEPAHH\\nHarpaKIeHHM TocyIapcTBeHHbIMH HarpaIaMH\\nPoccuickoi Φelepaun\\n3a  6oJbmHe\\n3acJIyrH\\nB\\nykpeIJIeHuu\\nHHcTITyTa\\nceMbH\\nH BOcIHTaHHH IeTeH HpHcBoHTb 3BaHHe\\n\"MATb-IEPOHA\"\\nBAPTAIIEBHY JIuJHY BuJIbeBHe, Pecy6JHKa KapeJIHA\\nJIEMHEC OkcaHe BuTaIbeBHe, PecIy6JHKa KpbIM\\nKOFbIJIMHO TaTbXHe BJaIHMHpoBHe, ApxaHreJIbcKag 06JIacTh\\nJIYKbAHOBOH HaTaIbe BHKTOpoBHe, Pecy6J1HKa KpbIM\\nMAKEEBO TaMape HuKoJaeBHe, KypraHcKa 06J1acTb\\nMATBEEBOH Ombre HIeTpoBHe, HIpHMopcKui kpai\\nMEPKYJIOBON Mapuu BHKTopoBHe, BmaINMupcKa 06JacTb\\nHOBIOPOIOBOH MapHH AJ1eKcaHIpoBHe, PecHIy6JIHKa MopIoBHA\\nOHIAP AiHaa1 BJaIUMupoBHe, PecIy6JIHKa THIBa\\nHOJIPE3OBON MapHN IOpbeBHe, EBpeicKa8 aBTOHOMHa 06J1acTH\\nPOXHOBO TaJIHHe HuKOJIaeBHe, OpJIOBcKaA 06JIaCTH\\nTPOIHIMHON EJeHe AJeKcaHIpoBHe, ropon MocKBa\\nXJIIOIHO HaμexIe CepreeBHe, CBepIoBcKa o6JIacTb.\\n2 100071 945632'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\skan.pdf', 'page': 1}, page_content='2\\nHarpaIuTb:\\nOPIIEHOM\"POIMTEJIBCKAACJIABA\"\\nKHCTOJI HaTaJIBIO BJIaIUMHpOBHy, CBepIOBcKaA 06JaCTH\\nKHCTOJIA AJ1eKce BaJepbeBHua, CBepIJIOBcKaa 06JIacTb\\nIEBYEHKO EBreHH BuKTopoBHya, OMcKa 06JIacTb\\nIHEBYEHKO OJ1bry HuKOJIaeBHy, OMcKaA 06JIacTh.\\nMEIIAJIBIOOPLIEHA\"POIMTEJIbCKAACJIABA\"\\nBOBAJIIbIKOBA MHHrHAHa IOpbeBH4a, Pecy6JIHKa KaJIMbIKHA\\nBOBAJIIbIKOBY Bepy BJaJHMHpoBHy, PecIy6JIHKa KaJIMbIKHA\\nIPAHEBA AHaTOJHA H1LHYa, CBepIJIOBCKa 06JacTh\\nIOPIKHEBA YTHacyHa MHxaiJIOBHYa, Pecy6JIHKa KaJIMHIKHA\\nIOPIIKMEBY IeJIO BacHJIbeBHy, PecIy6JIHKa KaJIMHIKHA\\nKJIAIOBA OJera AJeKcaHIpoBHya, CBepIOBcKa 06JacTL\\nKJIA』OBY EKaTepHHy AJIeKcaHIpOBHy, CBepIIOBCKa 06JIacTh\\nKJIMMEHTOBA AJIeKcea CepreeBH4a, PocTOBcKag 06JIacTb\\nKJIWMEHTOBY CBeTJIaHy BnaJHMHpOBHy, PocTOBcKa O6JIacTH\\nHIETPOBA eHHca AJIeKceeBHua, TBepcKa o6IacTb\\nIIETPOBY CBeTIaHy MuxaiJIoBHy, TBepcKaA 06JIacTH\\nIIOTHMKOBA IyapIa IeHHaIbeBHya, CBepIIOBcKa 06JIacTb\\nHIJIOTHVKOBY MpHHy IOpbeBHy, CBepIJIOBcKa 06JIacTb'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\skan.pdf', 'page': 1}, page_content='HIETPOBA eHHca AJIeKceeBHua, TBepcKa o6IacTb\\nIIETPOBY CBeTIaHy MuxaiJIoBHy, TBepcKaA 06JIacTH\\nIIOTHMKOBA IyapIa IeHHaIbeBHya, CBepIIOBcKa 06JIacTb\\nHIJIOTHVKOBY MpHHy IOpbeBHy, CBepIJIOBcKa 06JIacTb\\nHOPTHAT HHA BayecJIaBa AHaToJIbeBHya, CBepIJIOBcKa o6JIacTb\\nHIOPTHMT HHY CBeTIaHy ИBaHOBHy, CBepIJIOBCKa 06JIaCTH\\nCYTAK EJeHy IOpbeBHy, MypMaHcKa 06JIacTb\\nTIOTEHbKOBA AHaTOJIHa HHKOJIaeBHYa, Pecy6JIMKa KpbIM\\nYEIIYPKO AHHy BJaIHMHpoBHy, CTaBpoOIbcKH Kpan\\nYEIIyPKO BJIaJHMHpa AJIeKcaHIpoBHua, CTaBpoIOIbcKHi Kpan\\nIHECTEPMHA Onera IOpbeBHua, BopoHeKCKa o6JacTh\\nIHIECTEPHHY TaTbAHy HnKOJaeBHy, BopoHeKcKaa 06nacTb.'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\skan.pdf', 'page': 2}, page_content='3\\n3a\\nMyKeCTBO\\nH\\ncaMOoTBepKeHHOcTb,\\nIpOABJIeHHbIe\\nIpH HcHOJIHeHUИ\\nycJoBHax, HarpaIHTb:\\nOPIEHOMMYKECTBA\\n     \\npeIepaIbHoro\\n06pa30BaTeJIbHOrO\\nTocyiapcTBeHHoro\\naBTOHOMHOIO\\n06pa30BaHNA\\n\"PoccHickHn\\nBHIcIllerO\\nHaIHOHAJINHEIN\\nHCcJeIOBaTeJbcKHY MeIHHHCKHY yHHBepcHTeT HMeHH H.H.IIuporOBa\",\\nropo MocKBa\\nKAIMEBA XH3Hpa LIaMHIbeBH4a - PyKOBOIHTeJIA cTpOHTeJIbCTBa\\n3acrpoiuka \"Top-CTpoi\", HeueHcKaa PecHy6JIMKa (1ocMepTHO).\\nMEIIAJIbIO\"3AXPABPOCTB\"\\nIICTEIIEHI\\nXACAEBA PyCTaM6eKa PycJaHOBHua - KJIaIOBHKa o61ueCTBa\\n\"Top-Crpoi\", HeyeHcKaa Pecy6JHKa (HocMepTHo)\\nLVTYPOBA IOHyca\\nXH3ep0BH4a\\nBOIHTEJX\\naBTOMOOHJIA\\n3acTpoiⅢHKa \"Top-CTpoi\", HeyeHcKag PecHy6JMKa (1IocMepTHo)\\nHEIWEBA BaIpyIuAparHeBHua\\nao6ecTBa\\nKJIaIOBIIHKa\\n\\'Top-Crpon\", He4eHcKaa PecHy6JHka (nocMeprHo).\\n3a\\naKTHBHYIO\\n06eCTBeHHYIO\\nH\\n6JIaroTBopHTeJIbHyIO\\nIeaTeJIbHocTb HarpaIHTb\\n3HAKOMOTJINMIA\"3AEJATOLIEAHWE\"\\nIIAIIKMHY  AHy\\nTeoprHeBHy\\nHIOMOIIHHKa\\nTeHepaJibHorO\\n061ecTBa\\nIupeKTopa\\nc\\nOrpaHHYeHHOH\\nOTBeTCTBeHHOCTbIO\\n ΦJOT\",\\nEJIarOTBOpHTeJIbHOrO\\ndOHa\\nIHpekTopa'),\n",
       " Document(metadata={'source': 'C:\\\\Work\\\\Rag\\\\papers\\\\skan.pdf', 'page': 2}, page_content='3HAKOMOTJINMIA\"3AEJATOLIEAHWE\"\\nIIAIIKMHY  AHy\\nTeoprHeBHy\\nHIOMOIIHHKa\\nTeHepaJibHorO\\n061ecTBa\\nIupeKTopa\\nc\\nOrpaHHYeHHOH\\nOTBeTCTBeHHOCTbIO\\n ΦJOT\",\\nEJIarOTBOpHTeJIbHOrO\\ndOHa\\nIHpekTopa\\n\"CrapopyccKaa 12\", ropon CaHKT-IIerep6ypr.')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_iumg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://token:****@sberosc.sigma.sbrf.ru/repo/pypi/simple\n",
      "Collecting markdown\n",
      "  Downloading https://sberosc.sigma.sbrf.ru/repo/pypi/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading https://sberosc.sigma.sbrf.ru/repo/pypi/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.7\n"
     ]
    }
   ],
   "source": [
    "! pip install markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нужна ли чистка?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\22456229\\AppData\\Local\\Temp\\ipykernel_9580\\2830737909.py:70: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  directory = 'C:\\Work\\Rag\\papers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: paper1.pdf\n",
      "Обработано: paper2.pdf\n",
      "Обработано: skantype.pdf\n",
      "Обработано: Нацстратегия.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import markdown\n",
    "from pdfminer.high_level import extract_text as extract_text_from_pdf\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Класс для очистки HTML-тегов из текста\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.text = StringIO()\n",
    " \n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    " \n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    " \n",
    "def strip_tags(html):\n",
    "    \"\"\"Удалить HTML-теги из строки.\"\"\"\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    " \n",
    "def clean_markdown(text):\n",
    "    \"\"\"Очистить синтаксис Markdown из текста.\"\"\"\n",
    "    # Удалить ссылки в формате Markdown\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    # Удалить маркеры жирного и курсивного текста\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
    "    text = re.sub(r'__([^_]+)__', r'\\1', text)\n",
    "    text = re.sub(r'_([^_]+)_', r'\\1', text)\n",
    "    # Удалить изображения и их ссылки\n",
    "    text = re.sub(r'!\\[[^\\]]*]\\([^)]*\\)', '', text)\n",
    "    # Удалить маркеры заголовков\n",
    "    text = re.sub(r'#+\\s?', '', text)\n",
    "    # Удалить другой синтаксис Markdown (например, таблицы, маркеры списка)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'-{2,}', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Удалить лишние пустые строки\n",
    "    return text\n",
    " \n",
    "# def extract_text_from_docx(docx_path):\n",
    "#     \"\"\"Извлечь и очистить текст из Markdown-файла.\"\"\"\n",
    "#     with open(docx_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         md_content = file.read()\n",
    "#         html = markdown.markdown(md_content)\n",
    "#         text = strip_tags(html)\n",
    "#         return clean_markdown(text)\n",
    " \n",
    "# def extract_text_from_file(file_path):\n",
    "#     \"\"\"Извлечь текст из файла на основе его расширения.\"\"\"\n",
    "#     if file_path.endswith('.pdf'):\n",
    "#         return extract_text_from_pdf(file_path)\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         return extract_text_from_docx(file_path)\n",
    "#     elif file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     else:\n",
    "#         return \"Неподдерживаемый формат файла.\"\n",
    " \n",
    "# Директория, содержащая документы для обработки\n",
    "directory = 'C:\\Work\\Rag\\papers'\n",
    " \n",
    "# Параметры для разбиения текста\n",
    "chunk_size = 900\n",
    "chunk_overlap = 300\n",
    " \n",
    "# Список для хранения всех частей документов\n",
    "all_docs = []\n",
    "allowed_extensions = ['.docx', '.pdf', '.txt']\n",
    " \n",
    "# Обработка каждого файла в директории\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        # Получить расширение файла\n",
    "        _, file_extension = os.path.splitext(filename)\n",
    "        if file_extension in allowed_extensions:\n",
    "            file_path = os.path.join(root, filename)  # Полный путь к файлу\n",
    " \n",
    "            # Удалить расширение \".docx\", \".pdf\" или \".txt\" из имени файла\n",
    "            file_name_without_extension = os.path.splitext(filename)[0]\n",
    "             \n",
    "            # Открыть и прочитать файл\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            file_content = loader.load()\n",
    "            \n",
    "             \n",
    "            # Разбить текст на части\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            docs = text_splitter.split_documents(file_content)\n",
    "            all_docs.append(docs)\n",
    "            # for i, chunk in enumerate(docs):\n",
    "            #     # Определить метаданные для каждой части (можно настроить по своему усмотрению)\n",
    "            #     metadata = {\n",
    "            #         \"File Name\": file_name_without_extension,\n",
    "            #         \"Chunk Number\": i + 1,\n",
    "            #     }\n",
    "                 \n",
    "            #     # Создать заголовок с метаданными и именем файла\n",
    "            #     header = f\"File Name: {file_name_without_extension}\\n\"\n",
    "            #     for key, value in metadata.items():\n",
    "            #         header += f\"{key}: {value}\\n\"\n",
    "                 \n",
    "            #     # Объединить заголовок, имя файла и содержимое части\n",
    "            #     chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
    "            #     all_docs.append(chunk_with_header)\n",
    "                     \n",
    "            print(f\"Обработано: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = file_content[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Документ предоставлен КонсультантПлюс  \\n \\n \\n10 октября  2019  года  N 490 \\n \\n \\nУКАЗ  \\n \\nПРЕЗИДЕНТА РОССИЙСКОЙ ФЕДЕРАЦИИ  \\n \\nО РАЗВИТИИ ИСКУССТВЕННОГО ИНТЕЛЛЕКТА В РОССИЙСКОЙ ФЕДЕРАЦИИ  \\n \\n  Список изменяющих документов  \\n(в ред. Указа  Президента РФ от 15.02.2024 N 124)   \\n \\nВ целях обеспечения ускоренного развития искусственного интеллекта в Российской \\nФедерации, проведения научных иссл едований в области искусственного интеллекта, повышения \\nдоступности информации и вычислительных ресурсов для пользователей, совершенствования \\nсистемы подготовки кадров в этой области постановляю:  \\n1. Утвердить прилагаемую Национальную стратегию  развития искусственного интеллекта на \\nпериод до 2030 года.  \\n2. Правительству Российской Федерации:  \\nа) до 15 декабря 2019 г. обеспечить внесение изменений в национальную программу  \\n\"Цифровая экономика Российской Федерации\", в том числе разработать и утвердить федеральный \\nпроект \"Искусственный интеллект\";  \\nа(1)) до 1 июля 2024 г. обеспечить включение федеральног о проекта  \"Искусственный \\nинтеллект\" в национальный проект по формированию экономики данных на период до 2030 года;  \\n(пп. \"а(1)\" введен Указом  Президента РФ от 15.02.2024 N 124)  \\nб) представлять Президенту Российской Федерации ежегодно доклад о ходе реализации \\nНациональной стратегии  развития искусственного и нтеллекта на период до 2030 года;  \\nв) предусматривать при формировании в 2020 - 2030 годах проектов федеральных бюджетов \\nна очередной финансовый год и на плановый период бюджетные ассигнования на реализацию \\nнастоящего Указа.  \\n2(1). Федеральным органам исполн ительной власти руководствоваться положениями \\nНациональной стратегии  развития искусственного интеллекта на период до 2030 года при \\nразработке и реализации отраслевых документов стратегического планирования Российской \\nФедерации, го сударственных программ (подпрограмм) Российской Федерации и иных документов \\nстратегического планирования.  \\n(п. 2(1) введен Указом  Президента РФ от 15.02.2024 N 124)  \\n2(2). Рекомендовать:  \\nа) органам государственной власти субъектов Российской Федерации и органам местного \\nсамоуправления руководствоваться положениями Национальной стратегии  развития \\nискусственного интеллекта на период до 2030 года п ри разработке и реализации стратегических \\nнаправлений цифровой трансформации отраслей экономики, социальной сферы и \\nгосударственного управления, государственных программ (подпрограмм) субъектов Российской \\nФедерации, муниципальных программ и иных документов ; '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Документ предоставлен КонсультантПлюс  \\n \\n \\n10 октября  2019  года  N 490 \\n \\n \\nУКАЗ  \\n \\nПРЕЗИДЕНТА РОССИЙСКОЙ ФЕДЕРАЦИИ  \\n \\nО РАЗВИТИИ ИСКУССТВЕННОГО ИНТЕЛЛЕКТА В РОССИЙСКОЙ ФЕДЕРАЦИИ  \\n \\n  Список изменяющих документов  \\n(в ред. Указа  Президента РФ от 15.02.2024 N 124)   \\n \\nВ целях обеспечения ускоренного развития искусственного интеллекта в Российской \\nФедерации, проведения научных иссл едований в области искусственного интеллекта, повышения \\nдоступности информации и вычислительных ресурсов для пользователей, совершенствования \\nсистемы подготовки кадров в этой области постановляю:  \\n1. Утвердить прилагаемую Национальную стратегию  развития искусственного интеллекта на \\nпериод до 2030 года.  \\n2. Правительству Российской Федерации:  \\nа) до 15 декабря 2019 г. обеспечить внесение изменений в национальную программу  \\n\"Цифровая экономика Российской Федерации\", в том числе разработать и утвердить федеральный \\nпроект \"Искусственный интеллект\";  \\nа(1)) до 1 июля 2024 г. обеспечить включение федеральног о проекта  \"Искусственный \\nинтеллект\" в национальный проект по формированию экономики данных на период до 2030 года;  \\n(пп. \"а(1)\" введен Указом  Президента РФ от 15.02.2024 N 124)  \\nб) представлять Президенту Российской Федерации ежегодно доклад о ходе реализации \\nНациональной стратегии  развития искусственного и нтеллекта на период до 2030 года;  \\nв) предусматривать при формировании в 2020 - 2030 годах проектов федеральных бюджетов \\nна очередной финансовый год и на плановый период бюджетные ассигнования на реализацию \\nнастоящего Указа.  \\n2(1). Федеральным органам исполн ительной власти руководствоваться положениями \\nНациональной стратегии  развития искусственного интеллекта на период до 2030 года при \\nразработке и реализации отраслевых документов стратегического планирования Российской \\nФедерации, го сударственных программ (подпрограмм) Российской Федерации и иных документов \\nстратегического планирования.  \\n(п. 2(1) введен Указом  Президента РФ от 15.02.2024 N 124)  \\n2(2). Рекомендовать:  \\nа) органам государственной власти субъектов Российской Федерации и органам местного \\nсамоуправления руководствоваться положениями Национальной стратегии  развития \\nискусственного интеллекта на период до 2030 года п ри разработке и реализации стратегических \\nнаправлений цифровой трансформации отраслей экономики, социальной сферы и \\nгосударственного управления, государственных программ (подпрограмм) субъектов Российской \\nФедерации, муниципальных программ и иных документов ; '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = strip_tags(x)\n",
    "clean_markdown(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2495"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2495"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЗНАЧЕНИЕ ИИ В ЭКОНОМИКЕ ОАЭ'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[1][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'export.arxiv.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "retriever = ArxivRetriever(\n",
    "    load_max_docs=2,\n",
    "    get_ful_documents=True,\n",
    ")\n",
    "docs_arxiv = retriever.invoke(\"RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.21059v1', 'Published': datetime.date(2024, 7, 26), 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang'}, page_content='Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2409.01666v1', 'Published': datetime.date(2024, 9, 3), 'Title': 'In Defense of RAG in the Era of Long-Context Language Models', 'Authors': 'Tan Yu, Anbang Xu, Rama Akkiraju'}, page_content='Overcoming the limited context limitations in early-generation LLMs,\\nretrieval-augmented generation (RAG) has been a reliable solution for\\ncontext-based answer generation in the past. Recently, the emergence of\\nlong-context LLMs allows the models to incorporate much longer text sequences,\\nmaking RAG less attractive. Recent studies show that long-context LLMs\\nsignificantly outperform RAG in long-context applications. Unlike the existing\\nworks favoring the long-context LLM over RAG, we argue that the extremely long\\ncontext in LLMs suffers from a diminished focus on relevant information and\\nleads to potential degradation in answer quality. This paper revisits the RAG\\nin long-context answer generation. We propose an order-preserve\\nretrieval-augmented generation (OP-RAG) mechanism, which significantly improves\\nthe performance of RAG for long-context question-answer applications. With\\nOP-RAG, as the number of retrieved chunks increases, the answer quality\\ninitially rises, and then declines, forming an inverted U-shaped curve. There\\nexist sweet points where OP-RAG could achieve higher answer quality with much\\nless tokens than long-context LLM taking the whole context as input. Extensive\\nexperiments on public benchmark demonstrate the superiority of our OP-RAG.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2401.15391v1', 'Published': datetime.date(2024, 1, 27), 'Title': 'MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries', 'Authors': 'Yixuan Tang, Yi Yang'}, page_content='Retrieval-augmented generation (RAG) augments large language models (LLM) by\\nretrieving relevant knowledge, showing promising potential in mitigating LLM\\nhallucinations and enhancing response quality, thereby facilitating the great\\nadoption of LLMs in practice. However, we find that existing RAG systems are\\ninadequate in answering multi-hop queries, which require retrieving and\\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-hop queries, their ground-truth\\nanswers, and the associated supporting evidence. We detail the procedure of\\nbuilding the dataset, utilizing an English news article dataset as the\\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\\nMultiHop-RAG in two experiments. The first experiment compares different\\nembedding models for retrieving evidence for multi-hop queries. In the second\\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\\nqueries given the evidence. Both experiments reveal that existing RAG methods\\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\\nMultiHop-RAG will be a valuable resource for the community in developing\\neffective RAG systems, thereby facilitating greater adoption of LLMs in\\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\\nhttps://github.com/yixuantt/MultiHop-RAG/.')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = docs_arxiv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-07-26'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = d.metadata['Published'].strftime(\"%Y-%m-%d\")\n",
    "# print(date.strftime(\"%Y-%m-%d\"))\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = d.metadata[\"Entry ID\"].replace('abs','pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "        )\n",
    "    #return HuggingFaceEmbeddings(\n",
    "    #     model_name=model_name,\n",
    "    #     model_kwargs=model_kwargs,\n",
    "    #     encode_kwargs=encode_kwargs\n",
    "    #     )\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(d.metadata[\"Entry ID\"].replace('abs','pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                tmp_file.write(resp.content)\n",
    "                tmp_file_path = tmp_file.name\n",
    "loader = PDFMinerLoader(tmp_file_path)\n",
    "# loader = PyPDFLoader(temp_file, extract_images=True) #если PDF в виде скана мб нужно боковой тогл добавить\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "splitted_data = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\22456229\\\\AppData\\\\Local\\\\Temp\\\\tmp9nb49unu.pdf'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_data[0].metadata[\"source\"] = pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in splitted_data:\n",
    "    i.metadata[\"source\"] = pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_from_arxiv(query):\n",
    "#     retriever = ArxivRetriever(\n",
    "#     load_max_docs=2,\n",
    "#     get_ful_documents=True,)\n",
    "#     docs_arxiv = retriever.invoke(query)\n",
    "#     docs = []\n",
    "#     for doc in docs_arxiv:\n",
    "#         pdf_path = doc.metadata[\"Entry ID\"].replace('abs','pdf')\n",
    "#         response_load = requests.get(pdf_path)\n",
    "#         with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "#                 tmp_file.write(response_load.content)\n",
    "#                 tmp_file_path = tmp_file.name\n",
    "\n",
    "#         loader = PDFMinerLoader(tmp_file_path, concatenate_pages=True)\n",
    "#         # loader = PyPDFLoader(temp_file, extract_images=True) #если PDF в виде скана мб нужно боковой тогл добавить\n",
    "#         documents = loader.load()\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "#         splitted_data = text_splitter.split_documents(documents)\n",
    "#         for i in splitted_data:\n",
    "#              i.metadata[\"source\"] = pdf_path\n",
    "#         docs.extend(splitted_data)\n",
    "#         os.remove(tmp_file_path)\n",
    "#     # os.unlink(tmp_file_path)\n",
    "    \n",
    "#     embeddings = get_embeddings()\n",
    "    \n",
    "#     vectorstore_arxiv = FAISS.from_documents(splitted_data, embeddings)\n",
    "#     retriever_arxiv = vectorstore_arxiv.as_retriever(search_kwargs={\"k\": 3}, search_type=\"mmr\")\n",
    "#     return retriever_arxiv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'RAG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'export.arxiv.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retriever = ArxivRetriever(\n",
    "load_max_docs=2,\n",
    "get_ful_documents=True,)\n",
    "docs_arxiv = retriever.invoke('RAG')\n",
    "docs = []\n",
    "meta = []\n",
    "for doc in docs_arxiv:\n",
    "    pdf_path = doc.metadata[\"Entry ID\"].replace('abs','pdf')\n",
    "    response_load = requests.get(pdf_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response_load.content)\n",
    "            tmp_file_path = tmp_file.name\n",
    "\n",
    "#     loader = PyPDFLoader(tmp_file_path)\n",
    "\n",
    "    # loader = PyPDFLoader(temp_file, extract_images=True) #если PDF в виде скана мб нужно боковой тогл добавить\n",
    "    loader = PDFMinerLoader(tmp_file_path, concatenate_pages= True)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=450)\n",
    "    splitted_data = text_splitter.split_documents(documents)\n",
    "    for i in splitted_data:\n",
    "            i.metadata[\"source\"] = pdf_path\n",
    "    docs.extend(splitted_data)\n",
    "    meta.extend((f\" ответ будет производиться по следующей статье {pdf_path}\",f\"опубликованной {doc.metadata['Published'].strftime(\"%Y-%m-%d\")}\"))\n",
    "    os.remove(tmp_file_path)\n",
    "# os.unlink(tmp_file_path)\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "vectorstore_arxiv = FAISS.from_documents(docs, embeddings)\n",
    "retriever_arxiv = vectorstore_arxiv.as_retriever(search_kwargs={\"k\": 3}, search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ответ будет производиться по следующей статье http://arxiv.org/pdf/2407.21059v1',\n",
       " 'опубликованной 2024-07-26',\n",
       " ' ответ будет производиться по следующей статье http://arxiv.org/pdf/2409.01666v1',\n",
       " 'опубликованной 2024-09-03',\n",
       " ' ответ будет производиться по следующей статье http://arxiv.org/pdf/2401.15391v1',\n",
       " 'опубликованной 2024-01-27']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Modular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\n\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\n\\n1\\n\\n4\\n2\\n0\\n2\\n\\nl\\nu\\nJ\\n\\n6\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n9\\n5\\n0\\n1\\n2\\n.\\n7\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n(RAG)\\n\\nAbstract—Retrieval-augmented Generation'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='has\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG,\\nleading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional\\nlinear architecture, embracing a'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='with many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional\\nlinear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\ntheir respective implementation nuances. Modular RAG\\nof\\nfor the conceptualization\\npresents\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='fusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\ntheir respective implementation nuances. Modular RAG\\nof\\nfor the conceptualization\\npresents\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical\\nfoundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='innovative opportunities\\n\\nIndex Terms—Retrieval-augmented generation, large language\\n\\nmodel, modular system, information retrieval\\n\\nI. INTRODUCTION\\n\\nremarkable capabilities, yet\\n\\nL ARGE Language Models (LLMs) have demonstrated\\n\\nthey still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='During the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\n\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\n\\nSystems, Tongji University, Shanghai, 201210, China.\\n\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\n\\nComputer Science, Fudan University, Shanghai, 200438, China.\\n\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='limitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1,\\nit predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nthereby\\nwith the LLM’s identification of key information,\\nincreasing the risk of generating erroneous and hallucinated'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='for retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nthereby\\nwith the LLM’s identification of key information,\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='To overcome the aforementioned limitations, Advanced\\nRAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical\\nstrategies involve pre-retrieval processing and post-retrieval\\nprocessing. For instance, query rewriting is used to make\\nthe queries more clear and specific, thereby increasing the\\naccuracy of retrieval [10], and the reranking of retrieval results\\nis employed to enhance the LLM’s ability to identify and\\nutilize key information [11].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Despite the improvements in the practicality of Advanced\\nRAG, there remains a gap between its capabilities and real-\\nworld application requirements. On one hand, as RAG tech-\\nnology advances, user expectations rise, demands continue to\\nevolve, and application settings become more complex. For\\ninstance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has\\nfurther propelled the evolution of RAG technology.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='As shown in Figure 2, to achieve more accurate and efficient\\ntask execution, modern RAG systems are progressively inte-\\ngrating more sophisticated function, such as organizing more\\nrefined index base in the form of knowledge graphs, integrat-\\ning structured data through query construction methods, and\\nemploying fine-tuning techniques to enable encoders to better\\nadapt to domain-specific documents.\\n\\nIn terms of process design, the current RAG system has\\nsurpassed the traditional linear retrieval-generation paradigm.\\nResearchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-\\nibility. This flexibility in the process significantly enhances\\n\\n \\n \\n \\n \\n \\n \\n\\x0c2'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='In terms of process design, the current RAG system has\\nsurpassed the traditional linear retrieval-generation paradigm.\\nResearchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-\\nibility. This flexibility in the process significantly enhances\\n\\n \\n \\n \\n \\n \\n \\n\\x0c2\\n\\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\\nquestions, both encounter limitations and struggle to provide satisfactory\\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\\nrelevant documents have not been used correctly.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2\\n\\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\\nquestions, both encounter limitations and struggle to provide satisfactory\\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\\nrelevant documents have not been used correctly.\\n\\nthe expressive power and adaptability of RAG systems, en-\\nabling them to better adapt to various application scenarios.\\nHowever, this also makes the orchestration and scheduling of\\nworkflows more complex, posing greater challenges to system\\ndesign. Specifically, RAG currently faces the following new\\nchallenges:'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='the expressive power and adaptability of RAG systems, en-\\nabling them to better adapt to various application scenarios.\\nHowever, this also makes the orchestration and scheduling of\\nworkflows more complex, posing greater challenges to system\\ndesign. Specifically, RAG currently faces the following new\\nchallenges:\\n\\nComplex data sources integration. RAG are no longer\\nconfined to a single type of unstructured text data source but\\nhave expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple\\nsources can provide the system with a richer knowledge\\nbackground, and more reliable knowledge verification capa-\\nbilities [16].\\n\\nNew demands for system interpretability, controllability,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='New demands for system interpretability, controllability,\\n\\nFig. 2. Case of current Modular RAG.The system integrates diverse data\\nand more functional components. The process is no longer confined to linear\\nbut is controlled by multiple control components for retrieval and generation,\\nmaking the entire system more flexible and complex.\\n\\n\\x0cand maintainability. With the increasing complexity of sys-\\ntems, system maintenance and debugging have become more\\nchallenging. Additionally, when issues arise, it is essential to\\nquickly pinpoint the specific components that require opti-\\nmization.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='and maintainability. With the increasing complexity of sys-\\ntems, system maintenance and debugging have become more\\nchallenging. Additionally, when issues arise, it is essential to\\nquickly pinpoint the specific components that require opti-\\nmization.\\n\\nComponent selection and optimization. More neural net-\\nworks are involved in the RAG system, necessitating the\\nselection of appropriate components to meet the needs of spe-\\ncific tasks and resource configurations. Moreover, additional\\ncomponents enhance the effectiveness of RAG but also bring\\nnew collaborative work requirements [17]. Ensuring that these\\nmodels perform as intended and work efficiently together to\\nenhance the overall system performance is crucial.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Workflow orchestration and scheduling. Components\\nmay need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is\\nessential for improving system efficiency and achieving the\\ndesired outcomes [18].\\n\\nTo address the design, management, and maintenance chal-\\nlenges posed by the increasing complexity of RAG systems,\\nand to meet the ever-growing and diverse demands and ex-\\npectations, this paper proposes Modular RAG architecture.\\nIn modern computing systems, modularization is becoming\\na trend. It can enhance the system’s scalability and maintain-\\nability and achieve efficient task execution through process\\ncontrol.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The Modular RAG system consists of multiple independent\\nyet tightly coordinated modules, each responsible for handling\\nspecific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.\\nThis level not only inherits the main processes from the\\nAdvanced RAG paradigm but also introduces an orchestration\\nmodule to control the coordination of RAG processes. The\\nmiddle level is composed of sub-modules within each module,\\nfurther refining and optimizing the functions. The bottom level\\nconsists of basic units of operation—operators. Within the\\nModular RAG framework, RAG systems can be represented\\nin the form of computational graphs, where nodes represent\\nspecific operators. The comparison of the three paradigms is\\nshown in the Figure 3. Modular RAG evolves based on the'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='middle level is composed of sub-modules within each module,\\nfurther refining and optimizing the functions. The bottom level\\nconsists of basic units of operation—operators. Within the\\nModular RAG framework, RAG systems can be represented\\nin the form of computational graphs, where nodes represent\\nspecific operators. The comparison of the three paradigms is\\nshown in the Figure 3. Modular RAG evolves based on the\\nprevious development of RAG. The relationships among these\\nthree paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The advantages of Modular RAG are significant, as it\\nenhances the flexibility and scalability of RAG systems. Users\\ncan flexibly combine different modules and operators accord-\\ning to the requirements of data sources and task scenarios. In\\nsummary, the contributions of this paper are as follows:\\n\\n• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\n\\n3\\n\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\n\\n3\\n\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='3\\n\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.\\n\\n• The Modular RAG framework offers exceptional flexi-\\nbility and extensibility. This paper delves into the new\\nopportunities brought by Modular RAG and provides a\\nthorough discussion on the adaptation and expansion of\\nnew methods in different application scenarios, offering\\nguidance for future research directions and practical ex-\\nploration.\\n\\nII. RELATED WORK'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The development of RAG technology can be summarized\\nin three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In\\nspecific implementations, Retro [21] optimized pre-trained\\nautoregressive models through retrieval augmentation, while\\nAtlas [22] utilized a retrieval-augmented few-shot fine-tuning\\nmethod, enabling language models to adapt to diverse tasks.\\nIRCOT [23] further enriched the reasoning process during\\nthe inference phase by combining chain-of-thought and multi-\\nstep retrieval processes. Entering the second stage, as the\\nlanguage processing capabilities of LLMs significantly im-\\nproved, retrieval-augmented techniques began to serve as a\\nmeans of supplementing additional knowledge and providing\\nreferences, aiming to reduce the hallucination. For instance,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='IRCOT [23] further enriched the reasoning process during\\nthe inference phase by combining chain-of-thought and multi-\\nstep retrieval processes. Entering the second stage, as the\\nlanguage processing capabilities of LLMs significantly im-\\nproved, retrieval-augmented techniques began to serve as a\\nmeans of supplementing additional knowledge and providing\\nreferences, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant\\ntokens in retrieved document chunks.\\nWith the continuous progress of RAG technology, research\\nhas become more refined and focused, while also achieving\\ninnovative integration with other technologies such as graph\\nneural networks [26] and fine-tuning techniques [27]. The\\noverall pipeline has also become more flexible, such as using\\nLLMs to proactively determine the timing of retrieval and'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='removed redundant\\ntokens in retrieved document chunks.\\nWith the continuous progress of RAG technology, research\\nhas become more refined and focused, while also achieving\\ninnovative integration with other technologies such as graph\\nneural networks [26] and fine-tuning techniques [27]. The\\noverall pipeline has also become more flexible, such as using\\nLLMs to proactively determine the timing of retrieval and\\ngeneration [14], [28].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The development of RAG technology has been acceler-\\nated by LLM technology and practical application needs.\\nResearchers are examining and organizing the RAG frame-\\nwork and development pathways from different perspectives.\\nBuilding upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,\\nrelevant works on RAG were organized from the perspectives\\nof retrieval, generation, and augmentation methods. Huang\\net al., [29] categorize RAG methods into four main classes:\\npre-retrieval, retrieval, post-retrieval, generation, and provide\\na detailed discussion of the methods and techniques within\\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\\nguage Models (RALMs) form three key components, including\\nlanguage models, augmentations, and how their\\nretrievers,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='of retrieval, generation, and augmentation methods. Huang\\net al., [29] categorize RAG methods into four main classes:\\npre-retrieval, retrieval, post-retrieval, generation, and provide\\na detailed discussion of the methods and techniques within\\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\\nguage Models (RALMs) form three key components, including\\nlanguage models, augmentations, and how their\\nretrievers,\\ninteractions lead to different model structures and applications.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='4\\n\\nFig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\\nsystems.\\n\\nThey emphasize the importance of considering robustness,\\naccuracy, and relevance when evaluating RALMs and pro-\\npose several evaluation methods. Ding et al., [31] provide a\\ncomprehensive review from the perspectives of architecture,\\ntraining strategies, and applications. They specifically discuss\\nfour training methods of RALMs: training-free methods, in-\\ndependent training methods, sequence training methods, and\\njoint\\ntraining methods, and compare their advantages and\\ndisadvantages. Zhao et al., [32]analyze the applications of\\nRAG technology in various fields such as text generation,\\ncode generation, image generation, and video generation from\\nthe perspective of augmented intelligence with generative\\ncapabilities.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with\\noptimizing the retrieval and generation stages. However, it has\\nnot turned its attention to the new characteristics that RAG\\nresearch is continuously evolving, namely the characteristics\\nof process scheduling and functional componentization. There\\nis currently a lack of comprehensive analysis of the overall\\nRAG system, which has led to research on paradigms lagging\\nbehind the development of RAG technology.\\n\\nIII. FRAMEWORK AND NOTATION\\n\\nFor query Q = {qi}, a typical RAG system mainly consists\\nof three key components. 1) Indexing. Given documents D =\\n{d1, d2, . . . , dn} , where di represents the document chunk.\\nIndexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\n\\nNotation'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='III. FRAMEWORK AND NOTATION\\n\\nFor query Q = {qi}, a typical RAG system mainly consists\\nof three key components. 1) Indexing. Given documents D =\\n{d1, d2, . . . , dn} , where di represents the document chunk.\\nIndexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\n\\nNotation\\n\\nq\\ny\\nD\\nR(q, D)\\nF\\nP\\nfqe\\nfqc\\nfcomp\\nfsel\\nfr\\nM\\nop\\n\\nDescription\\n\\nThe original query\\nThe output of LLM\\nA document retrieval repository composed of chunks di.\\nRetriever,find similar chunks from D based on q.\\nRAG Flow\\nRAG Flow pattern\\nQuery expansion function\\nQuery transform function\\nChunk compression function\\nChunk selection function\\nRouting function\\nModule in modular RAG\\nThe specific operators within the Module.\\n\\nTABLE I\\nIMPORTANT NOTATION'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Notation\\n\\nq\\ny\\nD\\nR(q, D)\\nF\\nP\\nfqe\\nfqc\\nfcomp\\nfsel\\nfr\\nM\\nop\\n\\nDescription\\n\\nThe original query\\nThe output of LLM\\nA document retrieval repository composed of chunks di.\\nRetriever,find similar chunks from D based on q.\\nRAG Flow\\nRAG Flow pattern\\nQuery expansion function\\nQuery transform function\\nChunk compression function\\nChunk selection function\\nRouting function\\nModule in modular RAG\\nThe specific operators within the Module.\\n\\nTABLE I\\nIMPORTANT NOTATION\\n\\n2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\n\\nR : topk\\ndi∈D\\n\\nSim(q, di) → Dq\\n\\n(2)\\n\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\n\\nSim(q, di) = eq · edi\\n\\nor\\n\\neq · edi\\n∥eq∥ · ∥edi∥\\n\\n(3)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\n\\nR : topk\\ndi∈D\\n\\nSim(q, di) → Dq\\n\\n(2)\\n\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\n\\nSim(q, di) = eq · edi\\n\\nor\\n\\neq · edi\\n∥eq∥ · ∥edi∥\\n\\n(3)\\n\\n3) Generation. After getting the relevant documents. The\\nquery q and the retrieved document Dq chunks are inputted\\ntogether to the LLM to generate the final answer, where [·, ·]\\nstands for concatenation.\\n\\nI = {e1, e2, . . . , en} and ei = fe(di) ∈ Rd\\n\\n(1)\\n\\ny = LLM([Dq, q])\\n\\n(4)\\n\\n\\x0c5\\n\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='I = {e1, e2, . . . , en} and ei = fe(di) ∈ Rd\\n\\n(1)\\n\\ny = LLM([Dq, q])\\n\\n(4)\\n\\n\\x0c5\\n\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:\\n\\nMetadata Attachment. Chunks can be enriched with meta-\\ndata like page number, file name, author, timestamp, sum-\\nmary, or relevant questions. This metadata allows for filtered\\nretrieval, narrowing the search scope.\\n\\nL1 Module (M = {Ms}). The core process in RAG\\n\\nsystem.\\n\\nL2 Sub-module (Ms = {Op}).The functional modules in\\n\\nmodule.\\n\\nL3 Operator (Op = {fθi}). The the specific functional\\nimplementation in a module or sub-module. As a result, a\\nModular RAG system can be represented as:\\n\\nG = {q, D, M, {Ms}, {Op}}\\n\\n(5)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='L1 Module (M = {Ms}). The core process in RAG\\n\\nsystem.\\n\\nL2 Sub-module (Ms = {Op}).The functional modules in\\n\\nmodule.\\n\\nL3 Operator (Op = {fθi}). The the specific functional\\nimplementation in a module or sub-module. As a result, a\\nModular RAG system can be represented as:\\n\\nG = {q, D, M, {Ms}, {Op}}\\n\\n(5)\\n\\nThe arrangement between modules and operators constitutes\\nthe RAG Flow F = (Mϕ1, . . . , Mϕn ) where ϕ stands for\\nthe set of module parameters. A modular rag flow can be\\ndecomposed into a graph of sub-functions. In the simplest\\ncase,the graph is a linear chain.\\n\\nN aiveRAG : q\\n\\nR(q,D)\\n−−−−−−−−−−−→\\nT ext−Embedding\\n\\nDq\\n\\nLLM ([q,Dq])\\n−−−−−−−−−−−→\\nOpenAI/GP T −4\\n\\ny\\n\\n(6)\\n\\nIV. MODULE AND OPERATOR'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='G = {q, D, M, {Ms}, {Op}}\\n\\n(5)\\n\\nThe arrangement between modules and operators constitutes\\nthe RAG Flow F = (Mϕ1, . . . , Mϕn ) where ϕ stands for\\nthe set of module parameters. A modular rag flow can be\\ndecomposed into a graph of sub-functions. In the simplest\\ncase,the graph is a linear chain.\\n\\nN aiveRAG : q\\n\\nR(q,D)\\n−−−−−−−−−−−→\\nT ext−Embedding\\n\\nDq\\n\\nLLM ([q,Dq])\\n−−−−−−−−−−−→\\nOpenAI/GP T −4\\n\\ny\\n\\n(6)\\n\\nIV. MODULE AND OPERATOR\\n\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the\\ncurrent stage of RAG development, we have established\\nsix main modules: Indexing, Pre-retrieval, Retrieval, Post-\\nretrieval, Generation, and Orchestration.\\n\\nA. Indexing'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='A. Indexing\\n\\nIndexing is the process of split document into manageable\\nchunks and it is a key step in organizing a system. Indexing\\nfaces three main challenges. 1) Incomplete content represen-\\ntation.The semantic information of chunks is influenced by the\\nsegmentation method, resulting in the loss or submergence of\\nimportant information within longer contexts. 2) Inaccurate\\nchunk similarity search. As data volume increases, noise in\\nretrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-\\ninate from any document, devoid of citation trails, potentially\\nresulting in the presence of chunks from multiple different\\ndocuments that, despite being semantically similar, contain\\ncontent on entirely different topics.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Chunk Optimization: The size of the chunks and the\\noverlap between the chunks play a crucial role in the overall\\neffectiveness of the RAG system. Given a chunk di, its chunk\\nsize is denoted as Li = |di|, and the overlap is denoted as\\nLo\\ni = |di ∩ di+1|. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\n\\nSliding Window using overlapping chunks in a sliding win-\\ndow enhances semantic transitions. However, it has limitations\\nsuch as imprecise context size control, potential truncation of\\nwords or sentences, and lacking semantic considerations.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Sliding Window using overlapping chunks in a sliding win-\\ndow enhances semantic transitions. However, it has limitations\\nsuch as imprecise context size control, potential truncation of\\nwords or sentences, and lacking semantic considerations.\\n\\nSmall-to-Big [33] separate the chunks used for retrieval\\nfrom those used for synthesis. Smaller chunks enhance re-\\ntrieval accuracy, while larger chunks provide more context.\\nOne approach is to retrieve smaller summarized chunks and\\nreference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-\\nhancing information retrieval\\nis to establish a hierarchical\\nstructure for the documents. By constructing chunks structure,\\nRAG system can expedite the retrieval and processing of\\npertinent data.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Hierarchical Index. In the hierarchical structure of docu-\\nments, nodes are arranged in parent-child relationships, with\\nchunks linked to them. Data summaries are stored at each\\nnode, aiding in the swift traversal of data and assisting the\\nRAG system in determining which chunks to extract. This\\napproach can also mitigate the illusion caused by chunk\\nextraction issues. The methods for constructing a structured\\nindex primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and\\nLatex. 3) Semantic awareness based on semantic recognition\\nand segmentation of text.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='KG Index [34]. Using Knowledge Graphs (KGs) to struc-\\nture documents helps maintain consistency by clarifying con-\\nnections between concepts and entities, reducing the risk of\\nmismatch errors. KGs also transform information retrieval\\ninto instructions intelligible to language models, improving re-\\ntrieval accuracy and enabling contextually coherent responses.\\nThis enhances the overall efficiency of the RAG system.\\nFor example, organizing a corpus in the format of graph\\nG = {V, E, X }, where node V = {vi}n\\ni=1 represent document\\nstructures (e.g.passage, pages, table) , edge E ⊂ V × V rep-\\nresent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.\\n\\nB. Pre-retrieval'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='B. Pre-retrieval\\n\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nThe primary challenges in this module include: 1) Poorly\\nworded queries. The question itself is complex, and the\\nlanguage is not well-organized. 2) Language complexity and\\nambiguity. Language models often struggle when dealing\\nwith specialized vocabulary or ambiguous abbreviations with\\nmultiple meanings. For instance, they may not discern whether\\nLLM refers to Large Language Model or a Master of Laws in\\na legal context.\\n\\n1) Query Expansion : Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Query Expansion : Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\n\\n\\x0cfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\n\\nfqe(q) = {q1, q2, . . . , qn} ∀qi ∈ {q1, q2, . . . , qn}, qi /∈ Q\\n\\n(7)\\nMulti-Query uses prompt engineering to expand queries\\nvia LLMs, allowing for parallel execution. These expansions\\nare meticulously designed to ensure diversity and coverage.\\nHowever, this approach can dilute the user’s original intent.\\nTo mitigate this, the model can be instructed to assign greater\\nweight to the original query.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='(7)\\nMulti-Query uses prompt engineering to expand queries\\nvia LLMs, allowing for parallel execution. These expansions\\nare meticulously designed to ensure diversity and coverage.\\nHowever, this approach can dilute the user’s original intent.\\nTo mitigate this, the model can be instructed to assign greater\\nweight to the original query.\\n\\nSub-Query. By decomposing and planning for complex\\nproblems, multiple sub-problems are generated. Specifically,\\nleast-to-most prompting [35] can be employed to decom-\\npose the complex problem into a series of simpler sub-\\nproblems. Depending on the structure of the original problem,\\nthe generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing hallu-\\ncinations.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Query Transformation: Retrieve and generate based on\\n\\na transformed query instead of the user’s original query.\\n\\nfqt(q) = q′\\n\\n(8)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='fqt(q) = q′\\n\\n(8)\\n\\nRewrite. Original queries often fall short for retrieval in\\nreal-world scenarios. To address this, LLMs can be prompted\\nto rewrite. Specialized smaller models can also be employed\\nfor this purpose [24]. The implementation of the query rewrite\\nmethod in Taobao has significantly improved recall effective-\\nness for long-tail queries, leading to an increase in GMV [10].\\nHyDE [37]. In order to bridge the semantic gap between\\nquestions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-\\nlarity from answer to answer rather than seeking embedding\\nsimilarity for the problem or query. In addition, it also in-\\ncludes reverse HyDE, which generate hypothetical query for\\neach chunks and focuses on retrieval from query to query.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Step-back Prompting [38]. The original query is abstracted\\ninto a high-level concept question (step-back question). In the\\nRAG system, both the step-back question and the original\\nquery are used for retrieval, and their results are combined\\nto generate the language model’s answer.\\n\\n3) Query Construction:\\n\\nIn addition to text data, an in-\\ncreasing amount of structured data, such as tables and graph\\ndata, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query\\nlanguage to access alternative data sources, with common\\nmethods including Text-to-SQL or Text-to-Cypher . In many\\nscenarios, structured query languages (e.g., SQL, Cypher)\\nare often used in conjunction with semantic information and\\nmetadata to construct more complex queries.\\n\\n6\\n\\nC. Retrieval'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='6\\n\\nC. Retrieval\\n\\nThe retrieval process is pivotal in RAG systems. By lever-\\naging powerful embedding models, queries and text can be\\nefficiently represented in latent spaces, which facilitates the\\nestablishment of semantic similarity between questions and\\ndocuments, thereby enhancing retrieval. Three main consider-\\nations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Retriever Selection: With the widespread adoption of\\nRAG technology, the development of embedding models has\\nbeen in full swing. In addition to traditional models based\\non statistics and pre-trained models based on the encoder\\nstructure, embedding models fine-tuned on LLMs have also\\ndemonstrated powerful capabilities [39]. However, they often\\nleading to weaker inference\\ncome with more parameters,\\nand retrieval efficiency. Therefore, it is crucial to select the\\nappropriate retriever based on different task scenarios.\\n\\nSparse Retriever uses statistical methods to convert queries\\nand documents into sparse vectors. Its advantage lies in its\\nefficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Sparse Retriever uses statistical methods to convert queries\\nand documents into sparse vectors. Its advantage lies in its\\nefficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.\\n\\nDense Retriever employs pre-trained language models\\n(PLMs) to provide dense representations of queries and doc-\\numents. Despite higher computational and storage costs, it\\noffers more complex semantic representations. Typical models\\ninclude BERT structure PLMs, like ColBERT, and multi-task\\nfine-tuned models like BGE [40] and GTE [41].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Dense Retriever employs pre-trained language models\\n(PLMs) to provide dense representations of queries and doc-\\numents. Despite higher computational and storage costs, it\\noffers more complex semantic representations. Typical models\\ninclude BERT structure PLMs, like ColBERT, and multi-task\\nfine-tuned models like BGE [40] and GTE [41].\\n\\nHybrid Retriever is to use both sparse and dense retrievers\\nsimultaneously. Two embedding techniques complement each\\nother to enhance retrieval effectiveness. Sparse retriever can\\nprovide initial screening results. Additionally, sparse models\\nenhance the zero-shot retrieval capabilities of dense models,\\nparticularly in handling queries with rare entities,\\nthereby\\nincreasing system robustness.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Hybrid Retriever is to use both sparse and dense retrievers\\nsimultaneously. Two embedding techniques complement each\\nother to enhance retrieval effectiveness. Sparse retriever can\\nprovide initial screening results. Additionally, sparse models\\nenhance the zero-shot retrieval capabilities of dense models,\\nparticularly in handling queries with rare entities,\\nthereby\\nincreasing system robustness.\\n\\n2) Retriever Fine-tuning: In cases where the context may\\ndiverge from pre-trained corpus, particularly in highly special-\\nized fields like healthcare, law, and other domains abundant in\\nproprietary terminology. While this adjustment demands addi-\\ntional effort, it can substantially enhance retrieval efficiency\\nand domain alignment.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Retriever Fine-tuning: In cases where the context may\\ndiverge from pre-trained corpus, particularly in highly special-\\nized fields like healthcare, law, and other domains abundant in\\nproprietary terminology. While this adjustment demands addi-\\ntional effort, it can substantially enhance retrieval efficiency\\nand domain alignment.\\n\\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval\\nmodel based on labeled domain data is typically done using\\ncontrastive learning. This involves reducing the distance be-\\ntween positive samples while increasing the distance between\\nnegative samples. The commonly used loss calculation is\\nshown in the following:\\n\\nL(DR) = −\\n\\n1\\nT\\n\\nT\\n(cid:88)\\n\\nlog\\n\\ni=1\\n\\ne(sim(qi,d+\\n\\ni ))\\n\\ne(sim(qi,d+\\n\\ni )) + (cid:80)N\\n\\nj=1 e(sim(qi,d−\\ni ))'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Supervised Fine-Tuning (SFT). Fine-tuning a retrieval\\nmodel based on labeled domain data is typically done using\\ncontrastive learning. This involves reducing the distance be-\\ntween positive samples while increasing the distance between\\nnegative samples. The commonly used loss calculation is\\nshown in the following:\\n\\nL(DR) = −\\n\\n1\\nT\\n\\nT\\n(cid:88)\\n\\nlog\\n\\ni=1\\n\\ne(sim(qi,d+\\n\\ni ))\\n\\ne(sim(qi,d+\\n\\ni )) + (cid:80)N\\n\\nj=1 e(sim(qi,d−\\ni ))\\n\\n(10)\\nwhere d+\\nis the positive sample document corresponding to\\ni\\nthe i-th query, d−\\nis several negative sample, T is the total\\ni\\nnumber of queries, N is the number of negative samples, and\\nDR is the fine-tuning dataset.\\n\\nfqc(q) = q∗, q∗ ∈ Q∗ = {SQL, Cypher, . . . }\\n\\n(9)\\n\\nLM-supervised Retriever (LSR). In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='i ))\\n\\ne(sim(qi,d+\\n\\ni )) + (cid:80)N\\n\\nj=1 e(sim(qi,d−\\ni ))\\n\\n(10)\\nwhere d+\\nis the positive sample document corresponding to\\ni\\nthe i-th query, d−\\nis several negative sample, T is the total\\ni\\nnumber of queries, N is the number of negative samples, and\\nDR is the fine-tuning dataset.\\n\\nfqc(q) = q∗, q∗ ∈ Q∗ = {SQL, Cypher, . . . }\\n\\n(9)\\n\\nLM-supervised Retriever (LSR). In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-\\n\\n\\x0clizes the LM-generated results as supervisory signals to fine-\\ntune the embedding model during the RAG process.\\n\\nPLSR(d|q, y) =\\n\\nePLM (y|d,q)/β\\nd′∈D ePLM (y|d,q)/β)\\n\\n(cid:80)\\n\\n(11)\\n\\nPLM (y|d, q) is LM probability of the ground truth output y\\ngiven the input context d and query q, and β is a hyper-\\nparamter.\\n\\nAdapter. At'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='(9)\\n\\nLM-supervised Retriever (LSR). In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-\\n\\n\\x0clizes the LM-generated results as supervisory signals to fine-\\ntune the embedding model during the RAG process.\\n\\nPLSR(d|q, y) =\\n\\nePLM (y|d,q)/β\\nd′∈D ePLM (y|d,q)/β)\\n\\n(cid:80)\\n\\n(11)\\n\\nPLM (y|d, q) is LM probability of the ground truth output y\\ngiven the input context d and query q, and β is a hyper-\\nparamter.\\n\\nAdapter. At\\n\\ntimes, fine-tuning a large retriever can be\\ncostly, especially when dealing with retrievers based on LLMs\\nlike gte-Qwen. In such cases, it can mitigate this by incorpo-\\nrating an adapter module and conducting fine-tuning. Another\\nbenefit of adding an adapter is the ability to achieve better\\nalignment with specific downstream tasks [42].\\n\\nD. Post-retrieval'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='D. Post-retrieval\\n\\nFeeding all retrieved chunks directly into the LLM is not an\\noptimal choice. Post-processing the chunks can aid in better\\nleveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle. Like humans, LLM tends\\nto remember only the beginning or the end of long texts,\\nwhile forgetting the middle portion [43]. 2) Noise/anti-fact\\nchunks. Retrieved noisy or factually contradictory documents\\ncan impact the final retrieval generation [44]. 3) Context\\nWindow. Despite retrieving a substantial amount of relevant\\ncontent, the limitation on the length of contextual information\\nin large models prevents the inclusion of all this content.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Rerank: Rerank the retrieved chunks without altering\\ntheir content or length, to enhance the visibility of the more\\ncrucial document chunks. Given the retrieved set Dq and a\\nre-ranking method frerank to obtain the re-ranked set:\\n\\nDq\\n\\nr = frerank(q, Dq) = {d′\\nwheref (d′\\n\\n1, d′\\n1) ≥ f (d′\\n\\n2, . . . , d′\\n\\nk}\\n\\n2) ≥ . . . ≥ f (d′\\n\\nk).\\n\\n(12)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Dq\\n\\nr = frerank(q, Dq) = {d′\\nwheref (d′\\n\\n1, d′\\n1) ≥ f (d′\\n\\n2, . . . , d′\\n\\nk}\\n\\n2) ≥ . . . ≥ f (d′\\n\\nk).\\n\\n(12)\\n\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,\\nrelevance and MRR (Maximal Marginal Relevance) [45]. The\\nidea is to reduce redundancy and increase result diversity.\\nMMR selects phrases for the final key phrase list based on a\\ncombined criterion of query relevance and information novelty.\\nModel-base rerank. Utilize a language model to reorder the\\ndocument chunks, commonly based on the relevance between\\nthe chunks and the query. Rerank models have become an\\nimportant component of RAG systems, and relevant model\\ntechnologies are also being iteratively upgraded. The scope\\nreordering has also been extended to multimodal data such as\\ntables and images [46].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy\\nretrieval prompt is beneficial. However, excessive context can\\nintroduce more noise, diminishing the LLM’s perception of\\nkey information. A common approach to address this is to\\ncompress and select the retrieved content.\\n\\nDq\\n\\nc = fcomp(q, Dq), where|dqc\\n\\ni | < |dq\\n\\ni | ∀dq\\n\\ni ∈ Dq\\n\\n(13)\\n\\n7'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Dq\\n\\nc = fcomp(q, Dq), where|dqc\\n\\ni | < |dq\\n\\ni | ∀dq\\n\\ni ∈ Dq\\n\\n(13)\\n\\n7\\n\\n(Long)LLMLingua [47]. By utilizing aligned and trained\\nsmall language models, such as GPT-2 Small or LLaMA-\\n7B, the detection and removal of unimportant tokens from\\nthe prompt is achieved, transforming it into a form that is\\nchallenging for humans to comprehend but well understood by\\nLLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.\\n\\n3) Selection: Unlike compressing the content of document\\n\\nchunks, Selection directly removes irrelevant chunks.\\n\\nDq\\n\\ns = fsel(Dq) = {di ∈ D | ¬P (di)}\\n\\n(14)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='3) Selection: Unlike compressing the content of document\\n\\nchunks, Selection directly removes irrelevant chunks.\\n\\nDq\\n\\ns = fsel(Dq) = {di ∈ D | ¬P (di)}\\n\\n(14)\\n\\nWhere fsel is the function for deletion operation and P (di) is\\na conditional predicate indicating that document (di) satisfies\\na certain condition. If document (di) satisfies (P (di)), it will\\nbe deleted. Conversely, documents for which (¬P (di)) is true\\nwill be retained.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Selective Context. By identifying and removing redundant\\ncontent in the input context, the input is refined, thus improv-\\ning the language model’s reasoning efficiency. In practice, se-\\nlective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this\\nmethod offers a more concise and efficient textual representa-\\ntion, without compromising their performance across diverse\\napplications. However, it overlooks the interdependence be-\\ntween compressed content and the alignment between the\\ntargeted language model and the small language model utilized\\nfor prompting compression [48].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='LLM-Critique. Another straightforward and effective ap-\\nproach involves having the LLM evaluate the retrieved content\\nbefore generating the final answer. This allows the LLM\\nto filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [49], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\n\\nE. Generation\\n\\nUtilize the LLM to generate answers based on the user’s\\nquery and the retrieved contextual\\ninformation. Select an\\nappropriate model based on the task requirements, considering\\nfactors such as the need for fine-tuning, inference efficiency,\\nand privacy protection.\\n\\n1) Generator Fine-tuning: In addition to direct LLM usage,\\ntargeted fine-tuning based on the scenario and data character-\\nistics can yield better results. This is also one of the greatest\\nadvantages of using an on-premise setup LLMs.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Generator Fine-tuning: In addition to direct LLM usage,\\ntargeted fine-tuning based on the scenario and data character-\\nistics can yield better results. This is also one of the greatest\\nadvantages of using an on-premise setup LLMs.\\n\\nInstruct-Tuning. When LLMs lack data in a specific do-\\nmain, additional knowledge can be provided to the LLM\\nthrough fine-tuning. General fine-tuning dataset can also be\\nused as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate\\nresponses in a particular style as instructed [50].\\n\\nReinforcement learning. Aligning LLM outputs with hu-\\nman or retriever preferences through reinforcement learning is'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Reinforcement learning. Aligning LLM outputs with hu-\\nman or retriever preferences through reinforcement learning is\\n\\n\\x0ca potential approach [51]. For instance, manually annotating\\nthe final generated answers and then providing feedback\\nthrough reinforcement learning. In addition to aligning with\\nhuman preferences,\\nis also possible to align with the\\npreferences of fine-tuned models and retrievers.\\n\\nit\\n\\nDual Fine-tuing Fine-tuning both generator and retriever\\nsimultaneously to align their preferences. A typical approach,\\nsuch as RA-DIT [27], aligns the scoring functions between\\nretriever and generator using KL divergence. Retrieval likeli-\\nhood of each retrieved document d is calculated as :\\n\\nPR(d|q) =\\n\\ne(sim(d,q))/γ\\nd∈Dq e(sim(d,q)/γ\\n\\n(cid:80)\\n\\n(15)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='it\\n\\nDual Fine-tuing Fine-tuning both generator and retriever\\nsimultaneously to align their preferences. A typical approach,\\nsuch as RA-DIT [27], aligns the scoring functions between\\nretriever and generator using KL divergence. Retrieval likeli-\\nhood of each retrieved document d is calculated as :\\n\\nPR(d|q) =\\n\\ne(sim(d,q))/γ\\nd∈Dq e(sim(d,q)/γ\\n\\n(cid:80)\\n\\n(15)\\n\\nPLM (y|d, q) is the LM probability of the ground truth output y\\ngiven the input context d, question q, and γ is a hyperparamter.\\nThe overall loss is calculated as:\\n\\nL =\\n\\n1\\n|T |\\n\\nT\\n(cid:88)\\n\\ni=1\\n\\nKL(PR(d|q)||PLSR(d|q, y|))\\n\\n(16)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='PR(d|q) =\\n\\ne(sim(d,q))/γ\\nd∈Dq e(sim(d,q)/γ\\n\\n(cid:80)\\n\\n(15)\\n\\nPLM (y|d, q) is the LM probability of the ground truth output y\\ngiven the input context d, question q, and γ is a hyperparamter.\\nThe overall loss is calculated as:\\n\\nL =\\n\\n1\\n|T |\\n\\nT\\n(cid:88)\\n\\ni=1\\n\\nKL(PR(d|q)||PLSR(d|q, y|))\\n\\n(16)\\n\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to\\nminimize the probability of hallucinations. Therefore, it can\\nfilter out responses that do not meet the required standards\\nthrough additional verification module. Common verification\\nmethods include knowledge-base and model-base .\\n\\nyk = fverif y(q, Dq, y)\\n\\n(17)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='KL(PR(d|q)||PLSR(d|q, y|))\\n\\n(16)\\n\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to\\nminimize the probability of hallucinations. Therefore, it can\\nfilter out responses that do not meet the required standards\\nthrough additional verification module. Common verification\\nmethods include knowledge-base and model-base .\\n\\nyk = fverif y(q, Dq, y)\\n\\n(17)\\n\\nKnowledge-base verification refers to directly validating the\\nresponses generated by LLMs through external knowledge.\\nGenerally, it extracts specific statements or triplets from re-\\nsponse first. Then, relevant evidence is retrieved from verified\\nknowledge base such as Wikipedia or specific knowledge\\ngraphs. Finally, each statement is incrementally compared with\\nthe evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Model-based verification refers to using a small language\\nmodel\\nto verify the responses generated by LLMs [53].\\nGiven the input question, the retrieved knowledge, and the\\ngenerated answer, a small language model is trained to de-\\ntermine whether the generated answer correctly reflects the\\nretrieved knowledge. This process is framed as a multiple-\\nchoice question, where the verifier needs to judge whether the\\nanswer reflects correct answer . If the generated answer does\\nnot correctly reflect the retrieved knowledge, the answer can\\nbe iteratively regenerated until the verifier confirms that the\\nanswer is correct.\\n\\nF. Orchestration'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='F. Orchestration\\n\\nOrchestration pertains to the control modules that govern the\\nRAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent\\nupon the previous outcomes. This adaptive and modular ca-\\npability is a hallmark of modular RAG, distinguishing it from\\nthe more simplistic Naive and Advance RAG paradigm.\\n\\n8'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='8\\n\\n1) Routing: In response to diverse queries, the RAG system\\nroutes to specific pipelines tailored for different scenario, a\\nfeature essential for a versatile RAG architecture designed\\nto handle a wide array of situations. A decision-making\\nmechanism is necessary to ascertain which modules will be\\nengaged, based on the input from the model or supplementary\\nmetadata. Different routes are employed for distinct prompts\\nor components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi\\nto\\neach module. These scores dictate the selection of the active\\nsubset of modules. Mathematically, the routing function is\\nrepresented as:\\n\\nfr : Q → F\\n\\n(18)\\n\\nwhere fr(·) maps the identified query to its corresponding\\nRAG flow.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='fr : Q → F\\n\\n(18)\\n\\nwhere fr(·) maps the identified query to its corresponding\\nRAG flow.\\n\\nMetadata routing involves extracting key terms, or entities,\\nfrom the query, applying a filtration process that uses these\\nkeywords and associated metadata within the chunks to refine\\nthe routing parameters. For a specific RAG flow, denoted as\\nFi, the pre-defined routing keywords are represented as the\\nset Ki = {ki1, ki2, . . . , kin}. The keyword identified within\\nthe query qi is designated as K ′\\ni. The matching process for\\nthe query q is quantified by the key score equation:\\n\\nscorekey(qi, Fj) =\\n\\n1\\n|K ′\\nj|\\n\\n|Ki ∩ K ′\\nj|\\n\\n(19)\\n\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the\\ncount of keywords in K ′\\nj. The final step is to determine the\\nmost relevant flow for the query q:'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='scorekey(qi, Fj) =\\n\\n1\\n|K ′\\nj|\\n\\n|Ki ∩ K ′\\nj|\\n\\n(19)\\n\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the\\ncount of keywords in K ′\\nj. The final step is to determine the\\nmost relevant flow for the query q:\\n\\nFi(q) = argmaxFj ∈F score(q, Fj)\\nSemantic routing routes to different modules based on the\\nsemantic information of the query. Given a pre-defined intent\\nΘ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\\nePLM (θ|q)\\nPΘ(θ|q) =\\nθ∈Θ ePLM (θ|q)) . Routing to specific RAG flow is\\ndetermined by the semantic score:\\n\\n(20)\\n\\n(cid:80)\\n\\nsocresemantic(q, Fj) = argmaxθj ∈ΘP (Θ)\\n\\n(21)\\n\\nThe function δ(·) serves as a mapping function that assigns\\nan intent to a distinct RAG flow Fi = δ(θi)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='(20)\\n\\n(cid:80)\\n\\nsocresemantic(q, Fj) = argmaxθj ∈ΘP (Θ)\\n\\n(21)\\n\\nThe function δ(·) serves as a mapping function that assigns\\nan intent to a distinct RAG flow Fi = δ(θi)\\n\\nHybrid Routing can be implemented to improve query\\nrouting by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\n\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj ∈Θsocresemantic(q, Fj)\\n(22)\\na is a weighting factor that balances the contribution of the\\nkey-based score and the semantic score.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Hybrid Routing can be implemented to improve query\\nrouting by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\n\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj ∈Θsocresemantic(q, Fj)\\n(22)\\na is a weighting factor that balances the contribution of the\\nkey-based score and the semantic score.\\n\\n2) Scheduling: The RAG system evolves in complexity\\nand adaptability, with the ability to manage processes through\\na sophisticated scheduling module. The scheduling module\\nplays a crucial role in the modular RAG , identifying critical\\njunctures that require external data retrieval, assessing the\\nadequacy of the responses, and deciding on the necessity for\\nfurther investigation. It is commonly utilized in scenarios that\\ninvolve recursive, iterative, and adaptive retrieval, ensuring'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='that the system makes informed decisions on when to cease\\ngeneration or initiate a new retrieval loop.\\n\\nRule judge. The subsequent steps are dictated by a set of\\nestablished rules. Typically, the system evaluates the quality of\\ngenerated answers through scoring mechanisms. The decision\\nto proceed or halt the process is contingent upon whether these\\nscores surpass certain predetermined thresholds, often related\\nto the confidence levels of individual tokens, which can be\\ndefined as follow:\\n\\n(cid:40)\\n\\nyt =\\n\\nif all tokens of ˆst have probs ≥ τ\\n\\nˆst\\nst = LM ([Dqt, x, y<t])\\n\\notherwise\\n\\nHere, ˆst represents the tentative answer, and st is the output\\nfrom the language model. The condition for accepting ˆst is that\\nall tokens within it must have associated probabilities greater\\nthan or equal to the threshold τ . If this condition is not met,\\nthe system reverts to generating a new answer.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='(cid:40)\\n\\nyt =\\n\\nif all tokens of ˆst have probs ≥ τ\\n\\nˆst\\nst = LM ([Dqt, x, y<t])\\n\\notherwise\\n\\nHere, ˆst represents the tentative answer, and st is the output\\nfrom the language model. The condition for accepting ˆst is that\\nall tokens within it must have associated probabilities greater\\nthan or equal to the threshold τ . If this condition is not met,\\nthe system reverts to generating a new answer.\\n\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate\\nthis capability. The first method leverages LLM ’s in-context\\nlearning capability, and make judgments through prompt\\nengineering. A significant advantage of this method is the\\nelimination of model fine-tuning. Nonetheless, the format of\\nthe judgment output is contingent upon the LLM’s adherence\\nto the provided instructions.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The second approach involves the LLM generating specific\\ntokens that initiate targeted actions through fine-tuning. This\\ntechnique, with roots in the Toolformer [50], has been inte-\\ngrated into frameworks like Self-RAG [28]. This allows for a\\nmore direct control mechanism over the LLM’s actions, en-\\nhancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.\\nKnowledge-guide scheduling. Beyond the confines of rule-\\nbased methods and the complete reliance on LLMs for process\\ncontrol, a more adaptable intermediate approach emerges with\\nknowledge-guided scheduling [26]. These methods harness\\nthe power of knowledge graphs, to steer the retrieval and\\ngeneration processes. Specifically, it involves extracting infor-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='large number of compliant instruction sets to fine-tune LLM.\\nKnowledge-guide scheduling. Beyond the confines of rule-\\nbased methods and the complete reliance on LLMs for process\\ncontrol, a more adaptable intermediate approach emerges with\\nknowledge-guided scheduling [26]. These methods harness\\nthe power of knowledge graphs, to steer the retrieval and\\ngeneration processes. Specifically, it involves extracting infor-\\nmation relevant to the question from a knowledge graph and\\nconstructing a reasoning chain. This reasoning chain consists\\nof a series of logically interconnected nodes, each containing\\ncritical information for the problem-solving process. Based\\non the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='mation relevant to the question from a knowledge graph and\\nconstructing a reasoning chain. This reasoning chain consists\\nof a series of logically interconnected nodes, each containing\\ncritical information for the problem-solving process. Based\\non the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only\\nthe efficacy and precision of problem-solving but also the\\nclarity of the explanations provided.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='3) Fusion: As RAG process has evolved beyond a linear\\npipeline,\\nit frequently necessitates broadening the retrieval\\nscope or enhancing diversity by exploring multiple pipelines.\\nConsequently, after the expansion into various branches, the\\nfusion module effectively integrates the information, ensuring\\na comprehensive and coherent response. The fusion module’s\\nreliance is not just for merging answers but also for ensuring\\nthat the final output is both rich in content and reflective of\\nthe multifaceted nature of the inquiry.\\n\\n9'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='9\\n\\nLLM fusion.One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from\\ndifferent branches. However, this approach also faces some\\nchallenges, particularly when dealing with long answers that\\nexceeds the LLM’s context window limitation. To mitigate this\\nissue, it is common practice to first summarize each branch’s\\nanswer, extracting the key information before inputting it into\\nthe LLM, thus ensuring that the most important content is\\nretained even within length constraints.\\n\\nWeighted ensemble\\n\\nis based on the weighted values of\\ndifferent tokens generated from multiple branches, leading to\\nthe comprehensive selection of the final output. This approach\\ncan be calculated as :\\n\\np(y|q, Dq) =\\n\\n(cid:88)\\n\\nd∈Dq\\n\\np(y|d, q) · λ(d, q)\\n\\n(23)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Weighted ensemble\\n\\nis based on the weighted values of\\ndifferent tokens generated from multiple branches, leading to\\nthe comprehensive selection of the final output. This approach\\ncan be calculated as :\\n\\np(y|q, Dq) =\\n\\n(cid:88)\\n\\nd∈Dq\\n\\np(y|d, q) · λ(d, q)\\n\\n(23)\\n\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is\\ncalculated using the softmax function, which ensures that the\\nweights are normalized and sum up to one.\\n\\nλ(d, q) =\\n\\nes(d,q)\\nd∈Dq es(d,q)\\n\\n(cid:80)\\n\\n(24)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='p(y|q, Dq) =\\n\\n(cid:88)\\n\\nd∈Dq\\n\\np(y|d, q) · λ(d, q)\\n\\n(23)\\n\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is\\ncalculated using the softmax function, which ensures that the\\nweights are normalized and sum up to one.\\n\\nλ(d, q) =\\n\\nes(d,q)\\nd∈Dq es(d,q)\\n\\n(cid:80)\\n\\n(24)\\n\\nRRF (Reciprocal Rank Fusion) is an ensemble technique\\nthat synthesizes multiple retrieval result rankings into a co-\\nhesive, unified list [54]. It employs a tailored weighted aver-\\naging approach to enhance collective predictive performance\\nand ranking precision. The method’s strength is its dynamic\\nweight assignment, which is informed by the interplay among\\nbranches. RRF is especially potent in scenarios characterized\\nby model or source heterogeneity, where it can markedly\\namplify the accuracy of predictions.\\n\\nV. RAG FLOW AND FLOW PATTERN'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='V. RAG FLOW AND FLOW PATTERN\\n\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F =\\n(Mϕ1, . . . , Mϕn ), where ϕ stands for the set of module param-\\neters. A modular rag flow can be decomposed into a graph of\\nsub-functions. Through control logic, the operators can execute\\nin a predetermined pipeline, while also performing conditional,\\nbranching or looping when necessary. In the simplest case. the\\ngraph is a linear chain.\\n\\nAfter conducting an in-depth analysis of current RAG meth-\\nods, we have identified a set of common RAG flow patterns,\\ndenoted as P. These patterns transcend various application\\ndomains and demonstrate a high level of consistency and\\nreusability, revealing the prevalent structures and behaviors in\\nprocess design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} → Mϕ2 : {Op2} → . . . → Mϕn : {Opn}}'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='After conducting an in-depth analysis of current RAG meth-\\nods, we have identified a set of common RAG flow patterns,\\ndenoted as P. These patterns transcend various application\\ndomains and demonstrate a high level of consistency and\\nreusability, revealing the prevalent structures and behaviors in\\nprocess design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} → Mϕ2 : {Op2} → . . . → Mϕn : {Opn}}\\n\\nA. Linear Pattern\\n\\nThe modules in the modular RAG system are organized in\\n\\na linear way, and can be described as Algorithm 1.\\n\\nPlinear = {M1 → M2 → . . . → Mn}\\n\\n(25)\\n\\n\\x0cFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\\nsequential order.\\n\\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\\nrewrite module before retrieval. This module employs reinforcement based on\\nthe output results of the LLM.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='A. Linear Pattern\\n\\nThe modules in the modular RAG system are organized in\\n\\na linear way, and can be described as Algorithm 1.\\n\\nPlinear = {M1 → M2 → . . . → Mn}\\n\\n(25)\\n\\n\\x0cFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\\nsequential order.\\n\\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\\nrewrite module before retrieval. This module employs reinforcement based on\\nthe output results of the LLM.\\n\\nThe linear flow pattern is the simplest and most com-\\nmonly used pattern. As shown in Figure 4, the full linear\\nRAG flow pattern mainly includes pre-retrieval processing,\\nretrieval, post-retrieval processing, and generation modules.\\nPlinearf ull = {Mindexing → Mpre-retrieval → Mretrieval →\\nMpost-retrieval → Mgenerate}. If there are no pre-retrieval and\\npost-retrieval modules, it follows the Naive RAG paradigm.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The linear flow pattern is the simplest and most com-\\nmonly used pattern. As shown in Figure 4, the full linear\\nRAG flow pattern mainly includes pre-retrieval processing,\\nretrieval, post-retrieval processing, and generation modules.\\nPlinearf ull = {Mindexing → Mpre-retrieval → Mretrieval →\\nMpost-retrieval → Mgenerate}. If there are no pre-retrieval and\\npost-retrieval modules, it follows the Naive RAG paradigm.\\n\\nAlgorithm 1 Linear RAG Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM , pre-processing function fpre, post-\\nprocessing function fpost\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n\\n← R(q′, D) // Retrieve documents related to the pre-\\n\\nprocessed query\\n\\n4: ˆDq′\\n\\n← fpost(q′, Dq′\\n\\n) // Post-process the retrieved docu-\\n\\nments\\n\\n5: ˆy ← LLM ([q, ˆDq′'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Algorithm 1 Linear RAG Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM , pre-processing function fpre, post-\\nprocessing function fpost\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n\\n← R(q′, D) // Retrieve documents related to the pre-\\n\\nprocessed query\\n\\n4: ˆDq′\\n\\n← fpost(q′, Dq′\\n\\n) // Post-process the retrieved docu-\\n\\nments\\n\\n5: ˆy ← LLM ([q, ˆDq′\\n\\n]) // Generate output using the lan-\\nguage model with the original query and post-processed\\ndocuments\\n\\n6: return ˆy // Return the final output'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='processed query\\n\\n4: ˆDq′\\n\\n← fpost(q′, Dq′\\n\\n) // Post-process the retrieved docu-\\n\\nments\\n\\n5: ˆy ← LLM ([q, ˆDq′\\n\\n]) // Generate output using the lan-\\nguage model with the original query and post-processed\\ndocuments\\n\\n6: return ˆy // Return the final output\\n\\nCommon linear RAG flow involves a query transform\\nmodule (such as rewrite or HyDE operators) at the pre-retrieval\\nstage and utilize rerank at the post-retrieval stage. Rewrite-\\nRetrieve-Read (RRR) [24] is a typical linear structure. As\\nillustrated in Figure 5, the query rewrite module frewrite is a\\nsmaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with\\nthe final output of the LLM serving as the reward. The retriever\\nutilizes a sparse encoding model, BM25.\\n\\nB. Conditional Pattern'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='B. Conditional Pattern\\n\\nThe RAG flow with conditional structure involves select-\\ning different RAG pipeline based on different conditions,\\nas illustrated in Figure 6. A detailed definition is shown in\\nAlgorithm 2. Typically, pipleline selection is accomplished\\n\\n10\\n\\nFig. 6. The conditional flow pattern. There is a routing module that controls\\nwhich RAG flow the query is directed to. Typically, different flows are used for\\nvarious configurations to meet the general requirements of the RAG system.\\n\\nFig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\\nand generation separately, and then they are aggregated at the end.\\n\\nthrough a routing module that determines the next module\\nin the flow.\\n\\nPconditional = {Mi\\n\\nfr−→ Mj ∨ Mk}\\n\\n(26)\\n\\nfr−→ represents that based on routing function fr(·), the\\n\\nWhere\\nflow can go to module Mj or Mk.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Fig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\\nand generation separately, and then they are aggregated at the end.\\n\\nthrough a routing module that determines the next module\\nin the flow.\\n\\nPconditional = {Mi\\n\\nfr−→ Mj ∨ Mk}\\n\\n(26)\\n\\nfr−→ represents that based on routing function fr(·), the\\n\\nWhere\\nflow can go to module Mj or Mk.\\n\\nAlgorithm 2 Conditional RAG Flow Pattern\\nRequire: original query q, documents D, language model\\n\\nLM , retriever R, routing function fr\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: q′ ← QueryTransform(q) // Pre-process the initial query\\n\\nif needed\\n\\n3: D′ ← R(q′, D) // Retrieve or update documents related\\n\\nto the query\\n\\n4: Mnext ← fr(q′, D′) // Determine the next module using\\n\\nˆy ← Mj(q′, D′) // Execute module Mj\\n\\nthe routing function\\n5: if Mnext = Mj then\\n6:\\n7: else if Mnext = Mk then\\n8:\\n9: end if\\n10: return ˆy\\n\\nˆy ← Mk(q′, D′) Mk'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='LM , retriever R, routing function fr\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: q′ ← QueryTransform(q) // Pre-process the initial query\\n\\nif needed\\n\\n3: D′ ← R(q′, D) // Retrieve or update documents related\\n\\nto the query\\n\\n4: Mnext ← fr(q′, D′) // Determine the next module using\\n\\nˆy ← Mj(q′, D′) // Execute module Mj\\n\\nthe routing function\\n5: if Mnext = Mj then\\n6:\\n7: else if Mnext = Mk then\\n8:\\n9: end if\\n10: return ˆy\\n\\nˆy ← Mk(q′, D′) Mk\\n\\nPipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies\\nacross questions related to serious issues, political matters,\\nor entertainment topics. These routing flow often diverge in\\nterms of retrieval sources, retrieval processes, configurations,\\nmodels, and prompts.\\n\\n\\x0c11'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='ˆy ← Mk(q′, D′) Mk\\n\\nPipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies\\nacross questions related to serious issues, political matters,\\nor entertainment topics. These routing flow often diverge in\\nterms of retrieval sources, retrieval processes, configurations,\\nmodels, and prompts.\\n\\n\\x0c11\\n\\nbranches, as opposed to selecting one branch from multiple\\noptions in the conditional approach. Structurally, it can be\\ncategorized into two types, which are depicted in Figure 7\\nand Figure 8.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='11\\n\\nbranches, as opposed to selecting one branch from multiple\\noptions in the conditional approach. Structurally, it can be\\ncategorized into two types, which are depicted in Figure 7\\nand Figure 8.\\n\\nPre-Retrieval Branching (Multi-Query, Parallel Retrieval).\\nAs shown in Algorithm 3, the process involves initially taking\\na query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′\\ni. These document sets, along with the corresponding\\nsub-queries, are fed into a generation module Mgenerate to\\nproduce a set of answers Gi. Ultimately, all these generated\\nanswers are combined using a merging module Mmerge to\\nform the final result y. This entire flow can be mathematically\\nrepresented as:\\n\\nPbranchpre =Mmerge(q′\\n\\ni∈Mexpand(q){Mgenerate(q′\\n\\ni, d′\\n\\nij) |'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Pbranchpre =Mmerge(q′\\n\\ni∈Mexpand(q){Mgenerate(q′\\n\\ni, d′\\n\\nij) |\\n\\nij ∈ Mretrieve(q′\\nd′\\n\\ni)})\\n\\n(28)\\n\\nPost-Retrieval Branching (Single Query, Parallel Genera-\\ntion). As shown in Algorithm 4, in the post-retrieval branching\\npattern, the process starts with a single query q which is\\nused to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by\\na generation module Mgenerate to produce a set of generated\\nresults G. These results are subsequently merged using a\\nmerge module Mmerge to form the final result y. The process\\ncan be succinctly represented as y = Mmerge(Oi), where Oi is\\nthe collection of all generated results from each document dq\\ni\\nin Dq. Therefore, the entire process can be represented as:\\n\\nPbranchpost = Mmerge({Mgenerate(dq\\n\\ni ) | dq\\n\\ni ∈ Mretrieve(q)})\\n\\n(29)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Pbranchpost = Mmerge({Mgenerate(dq\\n\\ni ) | dq\\n\\ni ∈ Mretrieve(q)})\\n\\n(29)\\n\\nAlgorithm 4 Post-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\n\\nguage model LLM , merge module Mmerge\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n\\n← R(q′, D) // Retrieve a set of documents based on\\n\\nthe pre-processed query\\n\\n4: G ← ∅ // Initialize an empty set to store generated results\\n\\n5: for all di ∈ Dq′\\n6:\\n\\ndo\\n\\nyi ← LLM ([q, di]) // Generate results independently\\nfor each document chunk using the language model\\n7: Oi ← Oi ∪ {yi} // Add the generated result to the set\\n\\nof results\\n\\n8: end for\\n9: ˆy ← Mmerge(Oi) // Merge all generated results using the\\n\\nmerge function\\n\\n10: return ˆy'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='← R(q′, D) // Retrieve a set of documents based on\\n\\nthe pre-processed query\\n\\n4: G ← ∅ // Initialize an empty set to store generated results\\n\\n5: for all di ∈ Dq′\\n6:\\n\\ndo\\n\\nyi ← LLM ([q, di]) // Generate results independently\\nfor each document chunk using the language model\\n7: Oi ← Oi ∪ {yi} // Add the generated result to the set\\n\\nof results\\n\\n8: end for\\n9: ˆy ← Mmerge(Oi) // Merge all generated results using the\\n\\nmerge function\\n\\n10: return ˆy\\n\\nREPLUG [55] embodies a classic post-retrieval branching\\nstructure, wherein the probability of each token is predicted\\nfor each branch. Through weighted possibility ensemble, the\\ndifferent branches are aggregated, and the final generation\\n\\nFig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\\nthen generation is carried out separately for each retrieved document chunks,\\nfollowed by aggregation.\\n\\nC. Branching'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='In many cases, the RAG flow system may have multiple\\nparallel running branches , usually to increase the diver-\\nsity of generated results. Assuming multiple branches bi are\\ngenerated in module B = Msplit(·) = {b1, b2, . . . , bm}.\\nFor each branch bi ∈ B, the same or different RAG pro-\\ncesses can be executed, passing through multiple processing\\nmodules {M1, M2, . . . , Mk} to obtain branch output result\\npi = Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\\nbranches are aggregated using an aggregation function to\\nobtain intermediate output results. ˆO = Mmerge({pi\\n| bi ∈\\nB}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\\nmultiple model responses, they can continue through a val-\\nidation module. Therefore, the entire branch flow pattern can'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='branches are aggregated using an aggregation function to\\nobtain intermediate output results. ˆO = Mmerge({pi\\n| bi ∈\\nB}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\\nmultiple model responses, they can continue through a val-\\nidation module. Therefore, the entire branch flow pattern can\\nbe represented as:'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Pbranch =Mjn(. . . Mj1(Mmerge({Mik\\n\\n(. . . Mi1(bi) . . .) | bi ∈ Msplit(q)})) . . .)\\n\\n(27)\\n\\nAlgorithm 3 Pre-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, query expand mod-\\nlanguage model LLM ,\\n\\nule Mexpand, retriever Mretrieve,\\nmerge module Mmerge\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: Q′ ← Mexpand(q) // Expand the original query to multiple\\n\\nsub-queries\\n\\n3: for all q′\\n4: D′\\n\\ni ∈ Q′ do\\ni ← Mretrieve(q′\\n\\ni, D) // Retrieve documents for each\\n\\nsub-query\\n\\n5: Gi ← ∅ // Initialize an empty set for generated results\\n\\n6:\\n7:\\n\\n8:\\n9:\\n10:\\n\\nof the sub-query\\nij ∈ D′\\nfor all d′\\ni do\\nyij ← LLM ([q′\\ndocument of the sub-query\\nOi ← Oi ∪ {yij} // Add generated results to the set\\n\\nij]) // Generate results for each\\n\\ni, d′\\n\\nend for\\nˆy ← Mmerge(Oi) // Merge generated results of the sub-\\nquery into the final result\\n\\n11: end for\\n12: return ˆy'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='i, D) // Retrieve documents for each\\n\\nsub-query\\n\\n5: Gi ← ∅ // Initialize an empty set for generated results\\n\\n6:\\n7:\\n\\n8:\\n9:\\n10:\\n\\nof the sub-query\\nij ∈ D′\\nfor all d′\\ni do\\nyij ← LLM ([q′\\ndocument of the sub-query\\nOi ← Oi ∪ {yij} // Add generated results to the set\\n\\nij]) // Generate results for each\\n\\ni, d′\\n\\nend for\\nˆy ← Mmerge(Oi) // Merge generated results of the sub-\\nquery into the final result\\n\\n11: end for\\n12: return ˆy\\n\\nThe RAG flow with a branching structure differs from\\nthe conditional approach in that it involves multiple parallel\\n\\n\\x0c12\\n\\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\\nof retrieval and generation. It can be categorized into three forms: iterative,\\nrecursive, and adaptive.\\n\\nAlgorithm 5 Iterative RAG Flow Pattern\\nRequire: original query q, documents D, maximum iterative\\ntimes T , language model LLM , retriever R, initial output\\ny<1 = ∅'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='12\\n\\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\\nof retrieval and generation. It can be categorized into three forms: iterative,\\nrecursive, and adaptive.\\n\\nAlgorithm 5 Iterative RAG Flow Pattern\\nRequire: original query q, documents D, maximum iterative\\ntimes T , language model LLM , retriever R, initial output\\ny<1 = ∅\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: qt ← q // Initialize query for the first iteration\\n3: y<1 ← ∅ // Initialize previous outputs as empty\\n4: t ← 1 // Initialize iteration step\\n5: while t ≤ T do\\n6:\\n\\nqt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\n\\nrelated to the current query\\nyt ← LLM ([y<t−1, qt, Dt]) // Generate output using\\nthe language model\\ny<t ← [y<t−1, yt] // Update the list of previous outputs\\n\\n8:\\n\\n9:\\n\\nbreak'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\n\\nrelated to the current query\\nyt ← LLM ([y<t−1, qt, Dt]) // Generate output using\\nthe language model\\ny<t ← [y<t−1, yt] // Update the list of previous outputs\\n\\n8:\\n\\n9:\\n\\nbreak\\n\\nif Judge(yt, q) = false then\\n\\n10:\\n11:\\n12:\\n13:\\n14: end while\\n15: yf inal = synthesizeOutput(y≤t) // Synthesize final output\\n\\nend if\\nt ← t + 1 // Increment iteration step\\n\\nfrom the list of outputs\\n\\n16: return ˆy\\n\\nFig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\\nbranching pattern. Each retrieved chunks undergoes parallel generation, and\\nthen they are aggregated using a weighted probability ensemble.\\n\\nresult is used to fine-tune the retriever, known as Contriever,\\nthrough feedback.\\n\\nD. Loop Pattern'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='The RAG flow with a loop structure, as an important char-\\nacteristic of Modular RAG, involves interdependent retrieval\\nand generation steps. It typically includes a scheduling module\\nfor flow control. The modular RAG system can be abstracted\\nas a directed graph G = (V, E), where V is the set of vertices\\nrepresenting the various modules Mi in the system, and E is\\nthe set of edges representing the control flow or data flow be-\\ntween modules. If there is a vertex sequence Mi1, Mi2, ..., Min\\nsuch that Min can reach Mi1 (i.e., Min → Mi1), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous\\nmodule Mk through a Judge module, it can be represented\\nJudge\\nas: Mi\\n−−−→ Mk where Mk is the\\npredecessor module of Mj. If Mi return to Mj, it can be\\nrepresented as: ∃Judge(Mi, Mj)\\n(Mi, Mj) ∈ E and'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='tween modules. If there is a vertex sequence Mi1, Mi2, ..., Min\\nsuch that Min can reach Mi1 (i.e., Min → Mi1), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous\\nmodule Mk through a Judge module, it can be represented\\nJudge\\nas: Mi\\n−−−→ Mk where Mk is the\\npredecessor module of Mj. If Mi return to Mj, it can be\\nrepresented as: ∃Judge(Mi, Mj)\\n(Mi, Mj) ∈ E and\\nJudge(Mi, Mj) = true. If the Judge module not to return\\nto any previous module, it can be represented as: ∀Mi ∈\\nJudge(Mi, Mj) = false for all Mj that are predecessors\\nV,\\nof Mi. Loop pattern can be further categorized into iterative,\\nrecursive, and adaptive (active) retrieval approaches.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Judge\\n−−−→ Mj\\n\\nor Mi\\n\\ns.t.\\n\\nIterative retrieval At times, a single retrieval and genera-\\ntion may not effectively address complex questions requiring\\nextensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query\\nqt and the previous output sequence y<t = [y0, . . . , yt−1] ,\\niterations proceed under the condition that t is less than the\\nmaximum allowed iterations T . In each loop, it retrieves a\\ndocument chunks Dt−1 using the last output yt−1 and the\\ncurrent query qt. Subsequently, a new output yt is generated.\\nThe continuation of the iteration is determined by a Judge\\nmodule, which makes its decision based on the yt, y<t, qt,\\nand the Dt−1.\\n\\nis\\n\\niterative\\n\\nretrieval\\n\\nAn exemplary case of'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='is\\n\\niterative\\n\\nretrieval\\n\\nAn exemplary case of\\n\\nITER-\\nRETGEN [56] (Figure 11), which iterates retrieval-augmented\\ngeneration and generation-augmented retrieval. Retrieval-\\naugmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a\\nspecific context to help retrieve more relevant knowledge.\\n\\n\\x0c13\\n\\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\\nthat each recursive retrieval uses the new query generated from the previous\\nstep, thereby progressively deepening analysis of the original complex query.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='13\\n\\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\\nthat each recursive retrieval uses the new query generated from the previous\\nstep, thereby progressively deepening analysis of the original complex query.\\n\\nbiguous Question (DQ). The exploration of the tree concludes\\nupon reaching the maximum number of valid nodes or the\\nmaximum depth. Once the clarification tree is constructed,\\nToC gathers all valid nodes and generates a comprehensive\\nlong-text answer to address AQ.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='biguous Question (DQ). The exploration of the tree concludes\\nupon reaching the maximum number of valid nodes or the\\nmaximum depth. Once the clarification tree is constructed,\\nToC gathers all valid nodes and generates a comprehensive\\nlong-text answer to address AQ.\\n\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the\\nemergence of adaptive retrieval (see Algorithm 7) , also known\\nas active retrieval, which is partly attributed to the powerful\\ncapabilities of LLM. This shares a core concept with LLM\\nAgent [57]. RAG systems can actively determine the timing\\nof retrieval and decide when to conclude the entire process and\\nproduce the final result. Based on the criteria for judgment,\\nthis can be further categorized into Prompt-base and Tuning-\\nbase approaches.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Algorithm 7 Active RAG Flow Pattern\\nRequire: original query Q, documents D, maximum iterative\\n\\ntimes T , language model LLM , retriever R\\n\\nEnsure: final output ˆy\\n\\n1: Initialize:\\n2: t ← 1 // Initialize loop step\\n3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅ // Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new\\n\\nquery from previous output and query\\nif Evaluate(Qt, y<t−1) then\\n\\nDt ← R(qt, D) // Retrieve documents based on the\\nnew query\\nyt ← LLM ([qt, Dt]) // Generate output using the\\nlanguage model\\n\\nelse\\n\\nyt ← ∅ // Set output as empty if query evaluation is\\nfalse\\nend if\\ny<t ← [y<t−1, yt] // Update the list of previous outputs\\n\\nif isOutputAcceptable(yt, y<t, qt) = false then\\nbreak // Break if the output is not acceptable\\n\\n7:\\n\\n8:\\n\\n9:\\n\\n10:\\n11:\\n\\n12:\\n13:\\n\\n14:\\n15:\\n\\nend if\\nt ← t + 1 // Increment iteration step'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Dt ← R(qt, D) // Retrieve documents based on the\\nnew query\\nyt ← LLM ([qt, Dt]) // Generate output using the\\nlanguage model\\n\\nelse\\n\\nyt ← ∅ // Set output as empty if query evaluation is\\nfalse\\nend if\\ny<t ← [y<t−1, yt] // Update the list of previous outputs\\n\\nif isOutputAcceptable(yt, y<t, qt) = false then\\nbreak // Break if the output is not acceptable\\n\\n7:\\n\\n8:\\n\\n9:\\n\\n10:\\n11:\\n\\n12:\\n13:\\n\\n14:\\n15:\\n\\nend if\\nt ← t + 1 // Increment iteration step\\n\\n16:\\n17:\\n18: end while\\n19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\\n\\nthe list of outputs\\n\\n20: return ˆy\\n\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A\\n\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='16:\\n17:\\n18: end while\\n19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\\n\\nthe list of outputs\\n\\n20: return ˆy\\n\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A\\n\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\n\\nTermination of the loop is determined by a predefined number\\nof iterations.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='the list of outputs\\n\\n20: return ˆy\\n\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A\\n\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\n\\nTermination of the loop is determined by a predefined number\\nof iterations.\\n\\nRecursive retrieval The characteristic feature of recursive\\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\\nits clear dependency on the previous step and its continuous\\ndeepening of retrieval. Typically, it follows a tree-like structure\\nand there is a clear termination mechanism as an exit condition\\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\\nally involves query transform, relying on the newly rewritten\\nquery for each retrieval.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Algorithm 6 Recursive RAG Flow Pattern\\nRequire: initial query q, document D, retriever R, language\\n\\nmodel LM , maximum recursive depth Kmax\\n\\nEnsure: final output ˆy\\n\\nk ← 0 // Initialize recursion depth\\n\\n1: Initialize:\\n2: Q ← {q}\\n3:\\n4: while Q ̸= ∅ and k < Kmax do\\n5: Q′ ← ∅ // To store queries for the next recursion level\\n6:\\n7:\\n\\nfor all q ∈ Q do\\n\\nDq ← R(q, D) // Retrieve or update documents\\nrelated to the current query\\nY ← LM ([q, Dq]) // Generate outputs using the\\nlanguage model\\nQ′′ ← deriveNewQueries(q, Dq, Y ) // Derive new\\nqueries from generated outputs\\nfor all q′ ∈ Q′′ do\\n\\n// Update the set of queries for the next\\n\\nif q′ /∈ Q′ and q′ /∈ Q then\\n\\nQ′ ← Q′ ∪ {q′}\\n\\nend for\\n\\nend if\\nend for\\n\\n13:\\n14:\\n15:\\n16: Q ← Q′\\nrecursion\\nk ← k + 1 // Increment recursion depth\\n\\n17:\\n18: end while\\n19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\\n\\ngenerated outputs\\n\\n20: return ˆy'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='// Update the set of queries for the next\\n\\nif q′ /∈ Q′ and q′ /∈ Q then\\n\\nQ′ ← Q′ ∪ {q′}\\n\\nend for\\n\\nend if\\nend for\\n\\n13:\\n14:\\n15:\\n16: Q ← Q′\\nrecursion\\nk ← k + 1 // Increment recursion depth\\n\\n17:\\n18: end while\\n19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\\n\\ngenerated outputs\\n\\n20: return ˆy\\n\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-\\nnodes into the clarification tree from the initial ambiguous\\nquestion (AQ). At each expansion step, paragraph re-ranking\\nis performed based on the current query to generate a disam-\\n\\n8:\\n\\n9:\\n\\n10:\\n11:\\n12:\\n\\n\\x0c14\\n\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='8:\\n\\n9:\\n\\n10:\\n11:\\n12:\\n\\n\\x0c14\\n\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n\\n1) Retriever FT: In the RAG flow, common methods for\\nfine-tuning the retriever is shown in Figure 15 ,which include:\\n• Direct supervised fine-tuning of the retriever. Construct-\\ning a specialized dataset for retrieval and fine-tuning the\\ndense retriever. For example, using open-source retrieval\\ndatasets or constructing one based on domain-specific\\ndata.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Retriever FT: In the RAG flow, common methods for\\nfine-tuning the retriever is shown in Figure 15 ,which include:\\n• Direct supervised fine-tuning of the retriever. Construct-\\ning a specialized dataset for retrieval and fine-tuning the\\ndense retriever. For example, using open-source retrieval\\ndatasets or constructing one based on domain-specific\\ndata.\\n\\n• Adding trainable adapter modules. Sometimes, direct\\nfine-tuning of the API-base embedding model (e.g., Ope-\\nnAI Ada-002 and Cohere) is not feasible. Incorporating\\nan adapter module can enhance the representation of\\nyour data. Additionally, the adapter module facilitates\\nbetter alignment with downstream tasks, whether for task-\\nspecific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\n\\nbased on the results generated by LLM.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\n\\nbased on the results generated by LLM.\\n\\n• LLM Reward RL. Still using the LLM output results as\\nthe supervisory signal. Employing reinforcement learning\\nto align the retriever with the generator. The whole re-\\ntrieval process is disassembled in the form of a generative\\nMarkov chain.\\n\\n2) Generator FT: The primary methods for fine-tuning a\\ngenerator in RAG flow is shown in Figure 16, which include:\\n• Direct supervised fine-tuning. Fine-tuning through an\\nexternal dataset can supplement the generator with ad-\\nis the ability to\\nditional knowledge. Another benefit\\ncustomize input and output formats. By setting the Q&A\\nformat, LLM can understand specific data formats and\\noutput according to instructions.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Generator FT: The primary methods for fine-tuning a\\ngenerator in RAG flow is shown in Figure 16, which include:\\n• Direct supervised fine-tuning. Fine-tuning through an\\nexternal dataset can supplement the generator with ad-\\nis the ability to\\nditional knowledge. Another benefit\\ncustomize input and output formats. By setting the Q&A\\nformat, LLM can understand specific data formats and\\noutput according to instructions.\\n\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning\\ndata to enhance the capabilities of the open-source model.\\n• RL from LLM/human feedback. Reinforcement learning\\nbased on feedback from the final generated answers. In\\naddition to using human evaluations, powerful LLMs can\\nalso serve as an evaluative judge.\\n\\n3) Dual FT:'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning\\ndata to enhance the capabilities of the open-source model.\\n• RL from LLM/human feedback. Reinforcement learning\\nbased on feedback from the final generated answers. In\\naddition to using human evaluations, powerful LLMs can\\nalso serve as an evaluative judge.\\n\\n3) Dual FT:\\n\\nIn the RAG system, fine-tuning both the\\nretriever and the generator simultaneously is a unique feature\\nof the RAG system. It is important to note that the emphasis\\nof system fine-tuning is on the coordination between the\\nretriever and the generator. An exemplary implementation is\\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\\nThe LM-ft component updates the LLM to maximize the'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='3) Dual FT:\\n\\nIn the RAG system, fine-tuning both the\\nretriever and the generator simultaneously is a unique feature\\nof the RAG system. It is important to note that the emphasis\\nof system fine-tuning is on the coordination between the\\nretriever and the generator. An exemplary implementation is\\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\\nThe LM-ft component updates the LLM to maximize the\\n\\nFig. 13. RAG flow of FLARE [14]. The generated provisional answer will\\nundergo confidence assessment. If it does not meet the required confidence\\nlevel, the process will return to the retrieval stage and generate anew. The\\nassessment criteria are implemented through prompt'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Fig. 13. RAG flow of FLARE [14]. The generated provisional answer will\\nundergo confidence assessment. If it does not meet the required confidence\\nlevel, the process will return to the retrieval stage and generate anew. The\\nassessment criteria are implemented through prompt\\n\\nFig. 14. RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\\na suitable instruct fine-tuning dataset to fine-tune the deployed open-source\\nLLM. This allows the model to output four specific tokens during generation,\\nwhich are used to control the RAG process.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='typical\\nimplementation example is FLARE [14]. Its core\\nconcept is that LLMs should only retrieve when essential\\nknowledge is lacking, to avoid unnecessary or inappropriate\\nretrieval in an enhanced LM. FLARE iteratively generates the\\nnext provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-\\nbased approach involves fine-tuning LLM to generate special\\ntokens, thereby triggering retrieval or generation. This concept\\ncan be traced back to Toolformer [50], where the generation of\\nspecific content assists in invoking tools. In RAG systems, this\\napproach is used to control both retrieval and generation steps.\\nA typical case is Self-RAG [28](see Figure 14). Given an\\ninput prompt and the preceding generation result, first predict'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='based approach involves fine-tuning LLM to generate special\\ntokens, thereby triggering retrieval or generation. This concept\\ncan be traced back to Toolformer [50], where the generation of\\nspecific content assists in invoking tools. In RAG systems, this\\napproach is used to control both retrieval and generation steps.\\nA typical case is Self-RAG [28](see Figure 14). Given an\\ninput prompt and the preceding generation result, first predict\\nwhether the special token Retrieve is helpful for enhancing\\nthe continued generation through retrieval. Then, if retrieval\\nis needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved\\npassage. Finally, a critique token evaluates the overall utility of\\nthe response and selects the optimal result as the final output.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='E. Tuning Pattern\\n\\nRAG is continuously integrating with more LLM-related\\ntechnologies. In Modular RAG, many components are com-\\nposed of trainable language models. Through fine-tuning, the\\nperformance of the components and the compatibility with\\nthe overall flow can be further optimized. This section will\\nintroduce three main patterns of fine-tuning stages, namely\\nretriever fine-tuning, generator fine-tuning, and dual fine-\\ntuning.\\n\\n\\x0c15\\n\\nof this, we list three typical scalability cases, which clearly\\nshows that Modular RAG paradigm provides robust support\\nand flexibility for the innovation and development of RAG\\ntechnology.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='1) Recombination of the current modules: In this scenario,\\nno new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\\nand classifier selection mechanism, incorporating a branching\\nretrieval structure. In the first stage, retrieving chunks relevant\\nto the query. In the second stage, the query is combined\\nindividually with each chunk retrieved in the first stage, and a\\nparallel secondary retrieval is conducted. The retrieved content\\nis then input into a classifier to filter out the most relevant\\ndynamic documents. This ensures that the retrieved documents\\nare highly relevant to the query while reducing redundant\\ninformation. DR-RAG improved retrieval method significantly\\nenhances the accuracy and efficiency of answers, bolstering'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='individually with each chunk retrieved in the first stage, and a\\nparallel secondary retrieval is conducted. The retrieved content\\nis then input into a classifier to filter out the most relevant\\ndynamic documents. This ensures that the retrieved documents\\nare highly relevant to the query while reducing redundant\\ninformation. DR-RAG improved retrieval method significantly\\nenhances the accuracy and efficiency of answers, bolstering\\nRAG’s performance in multi-hop question-answering scenar-\\nios.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to\\naddress more complex scenarios without proposing new mod-\\nules. The core idea of PlanRAG [18] lies in its introduction of\\na preliminary planning stage, a crucial step that occurs before\\nretrieval and generation. Initially, the system employs a judge\\nmodule to assess whether the current context necessitates the\\nformulation of a new plan or adjustments to an existing one.\\nWhen encountering a problem for the first time, the system\\ninitiates the planning process, while in subsequent interactions,\\nit decides whether to execute re-planning based on previous\\nplans and retrieved data.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Next, the system devises an execution plan tailored to the\\nquery,\\ntreating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-\\nsion module to extend and refine the query. For each derived\\nsub-query, the system conducts targeted retrieval. Following\\nretrieval, another judge module evaluates the current results to\\ndecide whether further retrieval is required or if it should return\\nto the planning stage for re-planning. Through this strategy,\\nPlanRAG is able to handle complex decision-making problems\\nthat require multi-step data analysis more efficiently.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='3) New flow derived from new operators.: New operators\\noften introduce novel flow design, exemplified by Multi-Head\\nRAG [60]. Existing RAG solutions do not focus on queries that\\nmay require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far\\napart in the embedding space. Multi-Head RAG addresses this\\nby designing a new retriever that uses the activations of the\\nmulti-head attention layers of the Transformer, rather than the\\ndecoder layers, as keys for retrieving multifaceted documents.\\nDifferent attention heads can learn to capture different aspects\\nof the data. By using the corresponding activation results,\\nembeddings that represent different aspects of the data items\\nand the query can be generated, thereby enhancing the retrieval\\naccuracy for complex queries.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Fig. 16. Generator fine-tuning pattern, The main methods include SFT,\\ndistillation and RL from LLM/human feedback.\\n\\nFig. 17. Dual fine-tuning pattern. In this mode, both the retriever and\\ngenerator participate in fine-tuning, and their preferences will be aligned.\\n\\nlikelihood of the correct answer given the retrieval-augmented\\ninstructions while the R-ft component updates the retriever\\nto minimize the KL-Divergence between the retriever score\\ndistribution and the LLM preference.\\n\\nVI. DISCUSSION\\n\\nIn this chapter, we explore the innovative horizons opened\\nby the modular RAG paradigm. We examine its compatibility\\nwith cutting-edge methodologies in the progression of RAG\\ntechnology, emphasizing its scalability. It not only fosters a\\nfertile ground for model innovation but also paves the way for\\nseamless adaptation to the dynamic requirements of various\\napplications.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='VI. DISCUSSION\\n\\nIn this chapter, we explore the innovative horizons opened\\nby the modular RAG paradigm. We examine its compatibility\\nwith cutting-edge methodologies in the progression of RAG\\ntechnology, emphasizing its scalability. It not only fosters a\\nfertile ground for model innovation but also paves the way for\\nseamless adaptation to the dynamic requirements of various\\napplications.\\n\\nA. Opportunities in Modular RAG\\n\\nThe benefits of Modular RAG are evident, providing a\\nfresh and comprehensive perspective on existing RAG-related\\nwork. Through modular organization, relevant technologies\\nand methods are clearly summarized.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='A. Opportunities in Modular RAG\\n\\nThe benefits of Modular RAG are evident, providing a\\nfresh and comprehensive perspective on existing RAG-related\\nwork. Through modular organization, relevant technologies\\nand methods are clearly summarized.\\n\\nFrom a research perspective. Modular RAG is highly\\nscalable, it empowers researchers to introduce innovative mod-\\nules and operators, leveraging a deep understanding of RAG’s\\nevolving landscape. This flexibility enables the exploration of\\nnew theoretical and practical dimensions in the field.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='From a research perspective. Modular RAG is highly\\nscalable, it empowers researchers to introduce innovative mod-\\nules and operators, leveraging a deep understanding of RAG’s\\nevolving landscape. This flexibility enables the exploration of\\nnew theoretical and practical dimensions in the field.\\n\\nFrom an application perspective. The modularity of RAG\\nsystems simplifies their design and implementation. Users can\\ntailor RAG flows to fit their specific data, use cases, and\\ndownstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and\\npatterns that are tailored to various application contexts and\\ndomains. This approach not only streamlines the development\\nprocess but also enriches the functionality and versatility of\\nRAG applications.\\n\\nB. Compatibility with new methods'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='B. Compatibility with new methods\\n\\nModular RAG paradigm demonstrates exceptional compati-\\nbility with new developments. To gain a deeper understanding\\n\\n\\x0cVII. CONCLUSION'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='RAG is emerging as a pivotal technology for LLM applica-\\ntions. As technological landscapes evolve and the intricacies of\\napplication requirements escalate, RAG systems are being en-\\nhanced by integrating a diverse suite of technologies, thereby\\nachieving a higher level of complexity and functionality. This\\npaper introduces the innovative paradigm of Modular RAG.\\nThis approach systematically disassembles the complex archi-\\ntecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='Modular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.\\nIt encourages the innovation of novel functional modules and\\nthe crafting of innovative workflows, thereby driving forward\\nthe frontiers of RAG systems.\\n\\nREFERENCES\\n\\n[1] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='REFERENCES\\n\\n[1] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n\\n[2] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997, 2023.\\n\\n[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\\nand Z. Li, “Retrieval-augmented generation with knowledge graphs\\nfor customer service question answering,” in Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2024, pp. 2905–2909.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\\nand Z. Li, “Retrieval-augmented generation with knowledge graphs\\nfor customer service question answering,” in Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2024, pp. 2905–2909.\\n\\n[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y. Gao, Y. Hu, and E. Chen,\\n“Notellm: A retrievable large language model for note recommendation,”\\nin Companion Proceedings of the ACM on Web Conference 2024, 2024,\\npp. 170–179.\\n\\n[5] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\\n2023.\\n\\n[6] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[5] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\\n2023.\\n\\n[6] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524, 2023.\\n\\n[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\\n\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n\\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\\nmodels on text analytics in deep learning environment: a review,”\\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\\n\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n\\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\\nmodels on text analytics in deep learning environment: a review,”\\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\\n\\n[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n\\n[10] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\\n“Large language model based long-tail query rewriting in taobao search,”\\narXiv preprint arXiv:2311.03758, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n\\n[10] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\\n“Large language model based long-tail query rewriting in taobao search,”\\narXiv preprint arXiv:2311.03758, 2023.\\n\\n[11] Y. Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\\nY. Yu, “A bird’s-eye view of reranking: from list level to page level,”\\nin Proceedings of the Sixteenth ACM International Conference on Web\\nSearch and Data Mining, 2023, pp. 1075–1083.\\n\\n16\\n\\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[11] Y. Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\\nY. Yu, “A bird’s-eye view of reranking: from list level to page level,”\\nin Proceedings of the Sixteenth ACM International Conference on Web\\nSearch and Data Mining, 2023, pp. 1075–1083.\\n\\n16\\n\\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n\\n[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696, 2023.\\n\\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696, 2023.\\n\\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983, 2023.\\n\\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\\nand J. Larson, “From local to global: A graph rag approach to query-\\nfocused summarization,” arXiv preprint arXiv:2404.16130, 2024.\\n\\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices\\n\\nfor\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\\nand J. Larson, “From local to global: A graph rag approach to query-\\nfocused summarization,” arXiv preprint arXiv:2404.16130, 2024.\\n\\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices\\n\\nfor\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n\\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y. Wu, Z. Xu, T. Shi, Z. Wang,\\nS. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219, 2024.\\n\\n[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\\nmented generation for generative large language models as decision\\nmakers,” arXiv preprint arXiv:2406.12430, 2024.\\n\\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\ninformation re-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\\nmented generation for generative large language models as decision\\nmakers,” arXiv preprint arXiv:2406.12430, 2024.\\n\\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\ninformation re-\\n\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot\\ntrieval,” arXiv preprint arXiv:2310.20158, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\ninformation re-\\n\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot\\ntrieval,” arXiv preprint arXiv:2310.20158, 2023.\\n\\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,” in\\nInternational conference on machine learning. PMLR, 2022, pp. 2206–\\n2240.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A.\\nJoulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n\\n[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509, 2022.\\n\\n[24] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n\\nthe 15th Biennial Conference of'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509, 2022.\\n\\n[24] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n\\nthe 15th Biennial Conference of\\n\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof\\nthe Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track and\\nGovernment Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\\nand A. Yanishevsky, Eds. Orlando, USA: Association for Machine\\nTranslation in the Americas, Sep. 2022, pp. 202–209.\\n[Online].\\nAvailable: https://aclanthology.org/2022.amta-upg.14'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[26] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\\nful and interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n\\n[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\\nJ. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352, 2023.\\n\\n[28] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection,” arXiv preprint\\narXiv:2310.11511, 2023.\\n\\n[29] Y. Huang and J. Huang, “A survey on retrieval-augmented text gen-\\neration for large language models,” arXiv preprint arXiv:2404.10981,\\n2024.\\n\\n[30] Y. Hu and Y. Lu, “Rag and rau: A survey on retrieval-augmented\\nlanguage processing,” arXiv preprint\\n\\nin natural\\n\\nlanguage model\\narXiv:2404.19543, 2024.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[29] Y. Huang and J. Huang, “A survey on retrieval-augmented text gen-\\neration for large language models,” arXiv preprint arXiv:2404.10981,\\n2024.\\n\\n[30] Y. Hu and Y. Lu, “Rag and rau: A survey on retrieval-augmented\\nlanguage processing,” arXiv preprint\\n\\nin natural\\n\\nlanguage model\\narXiv:2404.19543, 2024.\\n\\n[31] Y. Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\\nQ. Li, “A survey on rag meets llms: Towards retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2405.06211, 2024.\\n\\n[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:\\nA survey,” arXiv preprint arXiv:2402.19473, 2024.\\n\\nYang,\\n\\n[33] S.\\nbig\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n\\nrag\\nSmall-to-\\nhttps://towardsdatascience.com/\\n\\n“Advanced\\n\\nretrieval,”\\n\\n01:\\n\\n\\x0c17'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:\\nA survey,” arXiv preprint arXiv:2402.19473, 2024.\\n\\nYang,\\n\\n[33] S.\\nbig\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n\\nrag\\nSmall-to-\\nhttps://towardsdatascience.com/\\n\\n“Advanced\\n\\nretrieval,”\\n\\n01:\\n\\n\\x0c17\\n\\n[57] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\\nmulti-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\\n2023.\\n\\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='“Advanced\\n\\nretrieval,”\\n\\n01:\\n\\n\\x0c17\\n\\n[57] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\\nmulti-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\\n2023.\\n\\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\\n“Dr-rag: Applying dynamic document relevance to retrieval-augmented\\ngeneration for question-answering,” arXiv preprint arXiv:2406.07348,\\n2024.\\n\\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M¨uller et al., “Multi-\\nhead rag: Solving multi-aspect problems with llms,” arXiv preprint\\narXiv:2406.05085, 2024.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\\n“Dr-rag: Applying dynamic document relevance to retrieval-augmented\\ngeneration for question-answering,” arXiv preprint arXiv:2406.07348,\\n2024.\\n\\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M¨uller et al., “Multi-\\nhead rag: Solving multi-aspect problems with llms,” arXiv preprint\\narXiv:2406.05085, 2024.\\n\\n[34] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730, 2023.\\n\\n[35] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[34] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730, 2023.\\n\\n[35] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n\\n[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495, 2023.\\n\\n[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\\n[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117, 2023.\\n[39] H. Cao, “Recent advances in text embedding: A comprehensive review\\nof top-performing methods on the mteb benchmark,” arXiv preprint\\narXiv:2406.01607, 2024.\\n\\n[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\\n\\n2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\\n\\n2023.\\n\\n[41] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang, “Towards\\ngeneral text embeddings with multi-stage contrastive learning,” arXiv\\npreprint arXiv:2308.03281, 2023.\\n\\n[42] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\\n“Prca: Fitting black-box large language models for retrieval question an-\\nswering via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n\\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[42] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\\n“Prca: Fitting black-box large language models for retrieval question an-\\nswering via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n\\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172, 2023.\\n\\n[44] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172, 2023.\\n\\n[44] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n\\n[45] L. Xia, J. Xu, Y. Lan, J. Guo, and X. Cheng, “Learning maximal\\nmarginal relevance model via directly optimizing diversity evaluation\\nmeasures,” in Proceedings of the 38th international ACM SIGIR con-\\nference on research and development in information retrieval, 2015, pp.\\n113–122.\\n\\n[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\\n\\nhere,” https://txt.cohere.com/rerank/, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[45] L. Xia, J. Xu, Y. Lan, J. Guo, and X. Cheng, “Learning maximal\\nmarginal relevance model via directly optimizing diversity evaluation\\nmeasures,” in Proceedings of the 38th international ACM SIGIR con-\\nference on research and development in information retrieval, 2015, pp.\\n113–122.\\n\\n[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\\n\\nhere,” https://txt.cohere.com/rerank/, 2023.\\n\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-\\nnarios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\\n[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\\nmatha, “Scatter: selective context attentional scene text recognizer,” in\\nproceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, 2020, pp. 11 962–11 972.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[49] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092, 2023.\\n\\n[50] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\\n2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='[50] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\\n2023.\\n\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in neural\\ninformation processing systems, vol. 35, pp. 27 730–27 744, 2022.\\n[52] S. J. Semnani, V. Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\\nStopping the hallucination of large language model chatbots by few-\\nshot grounding on wikipedia,” arXiv preprint arXiv:2305.14292, 2023.\\nJ. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n\\nJ. C. Park, and S.\\n\\nJeong, M. Kang,\\n\\n[53] J. Baek, S.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='J. C. Park, and S.\\n\\nJeong, M. Kang,\\n\\n[53] J. Baek, S.\\n\\n[54] G. V. Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\\nfusion outperforms condorcet and individual rank learning methods,”\\nin Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval, 2009, pp. 758–759.\\n[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\\nmodels,” arXiv preprint arXiv:2301.12652, 2023.\\n\\n[56] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='In Defense of RAG in the Era of Long-Context Language Models\\n\\nTan Yu\\nNVIDIA\\nSanta Clara, California\\nUnited States\\ntayu@nvidia.com\\n\\nAnbang Xu\\nNVIDIA\\nSanta Clara, California\\nUnited States\\nanbangx@nvidia.com\\n\\nRama Akkiraju\\nNVIDIA\\nSanta Clara, California\\nUnited States\\nrakkiraju@nvidia.com\\n\\nAbstract'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Overcoming the limited context limitations in\\nearly-generation LLMs, retrieval-augmented\\ngeneration (RAG) has been a reliable solution\\nfor context-based answer generation in the past.\\nRecently, the emergence of long-context LLMs\\nallows the models to incorporate much longer\\ntext sequences, making RAG less attractive.\\nRecent studies show that long-context LLMs\\nsignificantly outperform RAG in long-context\\napplications. Unlike the existing works favor-\\ning the long-context LLM over RAG, we ar-\\ngue that the extremely long context in LLMs\\nsuffers from a diminished focus on relevant in-\\nformation and leads to potential degradation in\\nanswer quality. This paper revisits the RAG in\\nlong-context answer generation. We propose\\nan order-preserve retrieval-augmented genera-\\ntion (OP-RAG) mechanism, which significantly\\nimproves the performance of RAG for long-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='ing the long-context LLM over RAG, we ar-\\ngue that the extremely long context in LLMs\\nsuffers from a diminished focus on relevant in-\\nformation and leads to potential degradation in\\nanswer quality. This paper revisits the RAG in\\nlong-context answer generation. We propose\\nan order-preserve retrieval-augmented genera-\\ntion (OP-RAG) mechanism, which significantly\\nimproves the performance of RAG for long-\\ncontext question-answer applications. With\\nOP-RAG, as the number of retrieved chunks\\nincreases, the answer quality initially rises, and\\nthen declines, forming an inverted U-shaped\\ncurve. There exist sweet points where OP-RAG\\ncould achieve higher answer quality with much\\nless tokens than long-context LLM taking the\\nwhole context as input. Extensive experiments\\non public benchmark demonstrate the superior-\\nity of our OP-RAG.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='(a) F1 score.\\n\\n(b) Input token count.\\n\\nFigure 1: Comparisons between the proposed order-\\npreserve retrieval-augmented generation (OP-RAG) and\\napproaches using long-context LLMs without RAG\\non En.QA dataset of ∞Bench. Our OP-RAG uses\\nLlama3.1-70B as generator, which significantly outper-\\nforms its counterpart using Llama3.1-70B without RAG.\\n\\n1\\n\\nIntroduction'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Figure 1: Comparisons between the proposed order-\\npreserve retrieval-augmented generation (OP-RAG) and\\napproaches using long-context LLMs without RAG\\non En.QA dataset of ∞Bench. Our OP-RAG uses\\nLlama3.1-70B as generator, which significantly outper-\\nforms its counterpart using Llama3.1-70B without RAG.\\n\\n1\\n\\nIntroduction\\n\\nDue to the limited context window length\\n(eg, 4096) of early-generation large language\\nmodels (LLMs), retrieval augmented generation\\n(RAG) (Guu et al., 2020; Lewis et al., 2020) is an\\nindispensable choice to handle a large-scale context\\ncorpus. Since the answer quality is heavily depen-\\ndent on the performance of the retrieval model, a\\nlot of efforts are devoted to improving the retrieval\\nrecall/precision when designing the RAG system.\\nRecently, the state-of-art LLMs support much\\nlonger context windows.\\nFor example, GPT-\\n4O (OpenAI, 2023), Claudi-3.5 (Anthropic, 2024),'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Llama3.1 (Meta, 2024b), Phi-3 (Abdin et al., 2024),\\nand Mistral-Large2 (AI, 2024) all support 128-K\\ncontext. Gemini-1.5-pro even supports a 1M con-\\ntext window. The recent emergence of long-context\\nLLMs naturally leads to the question: is RAG nec-\\nessary in the age of long-context LLMs? Li et al.\\n(2024) recently systematically compares RAG with\\nlong-context (LC) LLMs (w/o RAG) and demon-\\nstrates that LC LLMs consistently outperform RAG\\nin terms of answer quality.\\n\\nIn this work, we re-examine the effectiveness\\nof RAG in long-context answer generation. We\\nobserve that the order of retrieved chunks in the\\n\\n4\\n2\\n0\\n2\\n\\np\\ne\\nS\\n3\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n6\\n6\\n6\\n1\\n0\\n.\\n9\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='In this work, we re-examine the effectiveness\\nof RAG in long-context answer generation. We\\nobserve that the order of retrieved chunks in the\\n\\n4\\n2\\n0\\n2\\n\\np\\ne\\nS\\n3\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n6\\n6\\n6\\n1\\n0\\n.\\n9\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n \\n \\n \\n \\n \\n \\n\\x0ccontext of LLM is vital for the answer quality. Dif-\\nferent from traditional RAG which places the re-\\ntrieved chunks in a relevance-descending order, we\\npropose to preserve the order of retrieved chunks\\nin the original text. Our experiments show that the\\nproposed order-preserving mechanism significantly\\nimproves the answer quality of RAG.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Meanwhile, using the proposed order-preserve\\nRAG, as the number of retrieved chunks increases,\\nthe answer quality initially rises and then declines.\\nThis is because, with more retrieved chunks, the\\nmodel has access to more potentially relevant in-\\nformation, which improves the chances of retriev-\\ning the correct context needed to generate a high-\\nquality answer. However, as more chunks are re-\\ntrieved, the likelihood of introducing irrelevant or\\ndistracting information also increases. This excess\\ninformation can confuse the model, leading to a\\ndecline in answer quality. The trade-off, therefore,\\nis between improving recall by retrieving more\\ncontext and maintaining precision by limiting dis-\\ntractions. The optimal point is where the balance\\nbetween relevant and irrelevant information maxi-\\nmizes the quality of the answer. Beyond this point,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='distracting information also increases. This excess\\ninformation can confuse the model, leading to a\\ndecline in answer quality. The trade-off, therefore,\\nis between improving recall by retrieving more\\ncontext and maintaining precision by limiting dis-\\ntractions. The optimal point is where the balance\\nbetween relevant and irrelevant information maxi-\\nmizes the quality of the answer. Beyond this point,\\nthe introduction of too much irrelevant information\\ndegrades the model’s performance. It explains the\\ninferior performance of the approach taking the\\nwhole long context as the input of LLM.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Different from the conclusion from Li et al.\\n(2024), with the proposed order-preserving mech-\\nanism, RAG achieves higher answer quality com-\\npared with its counterparts that rely solely on Long-\\nContext LLMs. As shown in Figure 4a, On En.QA\\ndataset of ∞Bench (Zhang et al., 2024), using only\\n16K retrieved tokens, we achieve 44.43 F1 score\\nwith Llama3.1-70B. In contrast, without RAG,\\nLlama3.1-70B making full use of 128K context\\nonly achieves 34.32 F1 score, GPT-4O achieves\\nonly 32.36 F1 score and Gemini-1.5-Pro obtains\\nonly 43.08 F1 score as evaluated by Li et al. (2024).\\nThat is, RAG could achieve a higher F1 score even\\nwith a significant reduction on input length.\\n\\n2 Related Work'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='2 Related Work\\n\\nRetrieval-augmented generation. By incorporat-\\ning the external knowledge as context, retrieval-\\naugmented generation (RAG) (Guu et al., 2020;\\nLewis et al., 2020; Mialon et al., 2023) allows lan-\\nguage model to access up-to-date and specific in-\\nformation, reducing hallucinations and improving\\nfactual accuracy. Before the era of long-context\\n\\nFigure 2: Vanilla RAG versus the proposed order-\\npreserve the RAG. As shown in the example, a long\\ndocument is cropped into 13 chunks, {ci}13\\ni=1. The sim-\\nilarity score is appended to each chunk. We retrieve\\ntop 4 chunks with the highest similarity scores. Vanilla\\nRAG places the chunks in a score-descending order,\\nwhereas the proposed order-preserve RAG places the\\nchunks based on the order in the original document.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='LLMs, RAG is a promising solution to overcoming\\nthe limitation of short context window.\\nLong-context LLM. To support the long sequence\\nof language models, many efforts have been de-\\nvoted to improving the computing efficiency of\\nself-attention (Choromanski et al., 2020; Zaheer\\net al., 2020; Tay et al., 2020; Dao et al., 2022; Dao,\\n2024) and boosting extensibility of positional en-\\ncoding (Press et al., 2021; Sun et al., 2022; Chen\\net al., 2023). Recently, the flagship LLMs such as\\nGPT-4O (OpenAI, 2023), Gemini-1.5-Pro (Reid\\net al., 2024), Claudi-3.5 (Anthropic, 2024), Grok-\\n2 (xAI, 2024), and Llama3.1 (Meta, 2024a) have\\nsupported extremely large context. With the ex-\\nistence of long-context LLMs, RAG is no longer\\na indispensable module for long-context question-\\nanswering task. Recently, Li et al. (2024) con-\\ncludes that using long-context without RAG could'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='et al., 2023). Recently, the flagship LLMs such as\\nGPT-4O (OpenAI, 2023), Gemini-1.5-Pro (Reid\\net al., 2024), Claudi-3.5 (Anthropic, 2024), Grok-\\n2 (xAI, 2024), and Llama3.1 (Meta, 2024a) have\\nsupported extremely large context. With the ex-\\nistence of long-context LLMs, RAG is no longer\\na indispensable module for long-context question-\\nanswering task. Recently, Li et al. (2024) con-\\ncludes that using long-context without RAG could\\nsignificantly outperforms RAG. Different from the\\nconclusion from (Li et al., 2024), in this work,\\nwe demonstrate the proposed order-preserve RAG\\ncould beat the long-context LLMs without RAG.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='3 Order-Preserve RAG\\n\\nLet us denote the long textual context, e.g., a long\\ndocument, by d. We split d into N chunks sequen-\\ntially and uniformly, {ci}N\\ni=1. The index i implies\\nthe sequential order of the chunk ci in d. That is,\\nci−1 denotes the chunk before ci whereas ci+1 de-\\nnotes the chunk right after ci. Given a query q, we\\nobtain the relevance score of the chunk ci by com-\\nputing cosine similarity between the embedding of\\nq and that of ci:\\n\\nsi = cos(emb(q), emb(ci)),\\n\\n(1)\\n\\n\\x0c(a) EN.QA\\n\\n(b) EN.MC\\n\\nFigure 3: The influence of context length on the performance of RAG. The evaluations are conducted on En.QA and\\nEN.MC datasets of ∞Bench.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='si = cos(emb(q), emb(ci)),\\n\\n(1)\\n\\n\\x0c(a) EN.QA\\n\\n(b) EN.MC\\n\\nFigure 3: The influence of context length on the performance of RAG. The evaluations are conducted on En.QA and\\nEN.MC datasets of ∞Bench.\\n\\nwhere cos(·, ·) denotes the cosine similarity func-\\ntion and emb(·) denotes the embedding function.\\nWe retrieve the top k chunks with the highest\\nsimilarity scores with the query and denote the in-\\ndices of top k chunks by J = {ji}k\\ni=1. We preserve\\nthe order of chunks in the original long context d,\\nthat is, we constrain\\n\\njl > jm ⇐⇒ l > m.\\n\\n(2)\\n\\nFigure 2 visualizes the difference between the\\nvanilla RAG and the proposed order-preserve RAG.\\nDifferent from vanilla RAG placing the chunks in\\nthe order of similarity descending, the proposed\\norder-preserve RAG keep the order of chunks in\\nthe original document.\\n\\n4 Experiments\\n\\n4.1 Datasets.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='4 Experiments\\n\\n4.1 Datasets.\\n\\nWe conduct experiments on EN.QA and EN.MC\\ndatasets of ∞Bench (Zhang et al., 2024) bench-\\nmark, specially designed for long-context QA eval-\\nuation. To be specific, En.QA consists of 351\\nhuman-annotated question-answer pairs. On av-\\nerage, the long context in En.QA contains 150,374\\nwords. We use F1-score as metric for evaluation on\\nEn.QA. EN.MC consists of 224 question-answer\\npairs, which are annotated similarly to En.QA, but\\neach question is provided with four answer choices.\\nOn average, the long context in En.MC contains\\n142,622 words. We use accuracy as metric for eval-\\nuation on En.QA. We notice there is another bench-\\nmark termed LongBench (Bai et al., 2023). Never-\\n\\ntheless, the average context length of LongBench\\nis below 20K words, which is not long enough to\\nevaluate the recent long-context LLMs supporting\\n128K-token window size.\\n\\n4.2'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='theless, the average context length of LongBench\\nis below 20K words, which is not long enough to\\nevaluate the recent long-context LLMs supporting\\n128K-token window size.\\n\\n4.2\\n\\nImplementation details.\\n\\nWe set the chunk size as 128 tokens on all datasets.\\nChunks are non-overlapped. We use BGE-large-en-\\nv1.5 (Xiao et al., 2023) to extract the embedding\\nof queries and chunks, by default.\\n\\n4.3 Ablation Study'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='The influence of context length. We evaluate\\nthe influence of the context length on the perfor-\\nmance of the proposed order-preserve RAG. Since\\neach chunk contains 128 tokens, the context length\\nis 128m, where m is the number of the retrieved\\nchunks as the context for generating the answer. As\\nshown in Figure 3, as the context length increases,\\nthe performance initially increases. This is because\\nmore context might have a greater chance of cover-\\ning the relevant chunk. Nevertheless, as the context\\nlength further increases, the answer quality drops\\nsince more irrelevant chunks are used as distrac-\\ntions. To be specific, Llama3.1-8B model achieves\\nthe performance peak when the context length is\\n16K on both EN.QA dataset and EN.MC dataset,\\nwhereas the best performance of Llama3.1-70B\\nmodel is achieved at 48K on EN.QA and 32K on\\nEN.MC. The fact that the peak point of Llama3.1-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='ing the relevant chunk. Nevertheless, as the context\\nlength further increases, the answer quality drops\\nsince more irrelevant chunks are used as distrac-\\ntions. To be specific, Llama3.1-8B model achieves\\nthe performance peak when the context length is\\n16K on both EN.QA dataset and EN.MC dataset,\\nwhereas the best performance of Llama3.1-70B\\nmodel is achieved at 48K on EN.QA and 32K on\\nEN.MC. The fact that the peak point of Llama3.1-\\n70B comes later than Llama3.1-8B model might\\nbe because the larger-scale model has a stronger\\ncapability to distinguish the relevant chunks from'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='(a) EN.QA\\n\\n(b) EN.MC\\n\\nFigure 4: Comparisons between the proposed order-preserve RAG and vanilla RAG. The evaluations are conducted\\non En.QA and EN.MC datasets of ∞Bench, using Llama3.1-70B model.\\n\\nirrelevant distractions.\\nOrder-preserve RAG versus vanilla RAG. As\\nshown in Figure 4, when the number of retrieved\\nchunks are small (e.g, 8), the advantage of the pro-\\nposed order-preserve RAG over vanilla RAG is not\\nconsiderably. In contrast, when the number of re-\\ntrieved chunks is large, our order-preserve RAG\\nsignificantly outperforms vanilla RAG. To be spe-\\ncific, on EN.QA dataset, when the number of re-\\ntrieved chunk is 128, vanilla RAG only achieves\\n38.40 F1-score whereas our order-preserve RAG\\nachieves 44.43 F1-score. On EN.MC dataset, re-\\ntrieving 192 chunks, vanialla RAG only achieves\\n81.22 accuracy whereas our order-preserve RAG\\nobtains 88.65 accuracy.\\n\\n4.4 Main Results'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='4.4 Main Results\\n\\nWe compare the proposed order-preserve RAG with\\ntwo types of baselines. The first category of ap-\\nproaches uses the long-context LLM without RAG.\\nAs shown in Table 1, without RAG, LLM takes a\\nhuge number of tokens as input, which is inefficient\\nand costly. In contrast, the proposed order-preserve\\nRAG not only significantly reduces the number of\\ntokens, but also significantly improves the answer\\nquality. For instance, using Llama3.1-70B model,\\nthe approach without RAG only achieves a 34.26\\nF1 score on EN.QA with an average of 117K to-\\nkens as input. In contrast, our OP-RAG with 48K\\ntokens as input attains a 47.25 F1 score. The sec-\\nond category of baselines takes the SELF-ROUTE\\nmechanism (Li et al., 2024), which routes queries\\nto RAG or long-context LLM based on the model\\nself-reflection. As shown in Table 1, ours signifi-\\n\\nMethod\\n\\nEN.QA\\n\\nEN.MC\\n\\nF1 Score\\n\\nTokens Acc.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Method\\n\\nEN.QA\\n\\nEN.MC\\n\\nF1 Score\\n\\nTokens Acc.\\n\\nTokens\\n\\nLong-context LLM w/o RAG\\n\\nLlama3.1-70B\\nGPT-4O\\nGemini-1.5-Pro\\n\\n34.26\\n32.36\\n43.08\\n\\n117K\\n117K\\n196K\\n\\n71.62\\n78.42\\n85.57\\n\\n117K\\n117K\\n188K\\n\\nSELF-ROUTE (Li et al., 2024)\\n\\nGPT-4O\\nGemini-1.5-Pro\\n\\n34.95\\n37.51\\n\\n85K\\n83K\\n\\n77.29\\n76.86\\n\\nLlama3.1-70B order-preserve RAG (ours)\\n\\nOP-RAG-16K\\nOP-RAG-24K\\nOP-RAG-48K\\n\\n44.43\\n45.45\\n47.25\\n\\n16K\\n24K\\n48K\\n\\n84.72\\n88.65\\n85.59\\n\\n62K\\n62K\\n\\n16K\\n24K\\n48K\\n\\nTable 1: Comparisons among the long-context LLM\\nwithout RAG, SELF-ROUTE mechanism (Li et al.,\\n2024) and the proposed order-preserve (OP) RAG.\\n\\ncantly outperforms than using much fewer tokens\\nin the input of LLMs.\\n\\n5 Conclusion'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='cantly outperforms than using much fewer tokens\\nin the input of LLMs.\\n\\n5 Conclusion\\n\\nIn this paper, we have revisited the role of retrieval-\\naugmented generation (RAG) in the era of long-\\ncontext language models (LLMs). While recent\\ntrends have favored long-context LLMs over RAG\\nfor their ability to incorporate extensive text se-\\nquences, our research challenges this perspective.\\nWe argue that extremely long contexts in LLMs\\ncan lead to a diminished focus on relevant infor-\\nmation, potentially degrading answer quality in\\nquestion-answering tasks. To address this issue, we\\nproposed the order-preserve retrieval-augmented\\ngeneration (OP-RAG) mechanism. Our extensive\\nexperiments on public benchmarks have demon-\\nstrated that OP-RAG significantly improves the\\n\\n\\x0cMeta. 2024a. Introducing llama 3.1: Our most capable\\n\\nmodels to date.\\n\\nMeta. 2024b. Llama 3.1 models.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Meta. 2024a. Introducing llama 3.1: Our most capable\\n\\nmodels to date.\\n\\nMeta. 2024b. Llama 3.1 models.\\n\\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\\nAsli Celikyilmaz, et al. 2023. Augmented language\\nmodels: a survey. arXiv preprint arXiv:2302.07842.\\n\\nOpenAI. 2023. GPT-4 technical report.\\n\\nArXiv,\\n\\n2303:08774.\\n\\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\\nTrain short, test long: Attention with linear biases\\nenables input length extrapolation. arXiv preprint\\narXiv:2108.12409.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='OpenAI. 2023. GPT-4 technical report.\\n\\nArXiv,\\n\\n2303:08774.\\n\\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\\nTrain short, test long: Attention with linear biases\\nenables input length extrapolation. arXiv preprint\\narXiv:2108.12409.\\n\\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\\nDmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\\nrat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-\\nlocking multimodal understanding across millions of\\ntokens of context. arXiv preprint arXiv:2403.05530.\\n\\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\\nSong, and Furu Wei. 2022. A length-extrapolatable\\ntransformer. arXiv preprint arXiv:2212.10554.\\n\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\\nzler. 2020. Efficient transformers: A survey.(2020).\\narXiv preprint cs.LG/2009.06732.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\\nSong, and Furu Wei. 2022. A length-extrapolatable\\ntransformer. arXiv preprint arXiv:2212.10554.\\n\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\\nzler. 2020. Efficient transformers: A survey.(2020).\\narXiv preprint cs.LG/2009.06732.\\n\\nxAI. 2024. Grok-2 beta release.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding. Preprint,\\narXiv:2309.07597.\\n\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\\nLi Yang, et al. 2020. Big bird: Transformers for\\nlonger sequences. Advances in neural information\\nprocessing systems, 33:17283–17297.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding. Preprint,\\narXiv:2309.07597.\\n\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\\nLi Yang, et al. 2020. Big bird: Transformers for\\nlonger sequences. Advances in neural information\\nprocessing systems, 33:17283–17297.\\n\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zi-\\nhang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\\nZhen Leng Thai, Shuo Wang, Zhiyuan Liu, and\\nMaosong Sun. 2024. ∞bench: Extending long\\ncontext evaluation beyond 100k tokens. Preprint,\\narXiv:2402.13718.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Xinrong Zhang, Yingfa Chen, Shengding Hu, Zi-\\nhang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\\nZhen Leng Thai, Shuo Wang, Zhiyuan Liu, and\\nMaosong Sun. 2024. ∞bench: Extending long\\ncontext evaluation beyond 100k tokens. Preprint,\\narXiv:2402.13718.\\n\\nperformance of RAG for long-context question-\\nanswer applications. OP-RAG’s superior perfor-\\nmance suggests that efficient retrieval and focused\\ncontext utilization can outperform the brute-force\\napproach of processing extremely long contexts.\\n\\nReferences\\n\\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\\nrat Behl, et al. 2024. Phi-3 technical report: A highly\\ncapable language model locally on your phone. arXiv\\npreprint arXiv:2404.14219.\\n\\nMistral AI. 2024. Mistral large 2.\\n\\nAnthropic. 2024. Claude 3.5 sonnet.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='References\\n\\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\\nrat Behl, et al. 2024. Phi-3 technical report: A highly\\ncapable language model locally on your phone. arXiv\\npreprint arXiv:2404.14219.\\n\\nMistral AI. 2024. Mistral large 2.\\n\\nAnthropic. 2024. Claude 3.5 sonnet.\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\\nand Juanzi Li. 2023. Longbench: A bilingual, mul-\\ntitask benchmark for long context understanding.\\narXiv preprint arXiv:2308.14508.\\n\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\\nYuandong Tian. 2023. Extending context window of\\nlarge language models via positional interpolation.\\narXiv preprint arXiv:2306.15595.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Shouyuan Chen, Sherman Wong, Liangjian Chen, and\\nYuandong Tian. 2023. Extending context window of\\nlarge language models via positional interpolation.\\narXiv preprint arXiv:2306.15595.\\n\\nKrzysztof Choromanski, Valerii Likhosherstov, David\\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with\\nperformers. arXiv preprint arXiv:2009.14794.\\n\\nTri Dao. 2024. FlashAttention-2: Faster attention with\\nbetter parallelism and work partitioning. In Inter-\\nnational Conference on Learning Representations\\n(ICLR).\\n\\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness.\\nIn Advances in Neural Information Processing Sys-\\ntems (NeurIPS).'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Tri Dao. 2024. FlashAttention-2: Faster attention with\\nbetter parallelism and work partitioning. In Inter-\\nnational Conference on Learning Representations\\n(ICLR).\\n\\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness.\\nIn Advances in Neural Information Processing Sys-\\ntems (NeurIPS).\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In International confer-\\nence on machine learning, pages 3929–3938. PMLR.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459–9474.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459–9474.\\n\\nZhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu\\nMei, and Michael Bendersky. 2024. Retrieval aug-\\nmented generation or long-context llms? a compre-\\nhensive study and hybrid approach. arXiv preprint\\narXiv:2407.16833.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\\nMulti-Hop Queries\\n\\nYixuan Tang and Yi Yang\\nHong Kong University of Science and Technology\\n{yixuantang,imyiyang}@ust.hk\\n\\n4\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n7\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n1\\n9\\n3\\n5\\n1\\n.\\n1\\n0\\n4\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Retrieval-augmented generation (RAG) aug-\\nments large language models (LLM) by re-\\ntrieving relevant knowledge, showing promis-\\ning potential in mitigating LLM hallucinations\\nand enhancing response quality, thereby facil-\\nitating the great adoption of LLMs in prac-\\ntice. However, we find that existing RAG sys-\\ntems are inadequate in answering multi-hop\\nqueries, which require retrieving and reasoning\\nover multiple pieces of supporting evidence.\\nFurthermore, to our knowledge, no existing\\nRAG benchmarking dataset focuses on multi-\\nhop queries. In this paper, we develop a novel\\ndataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-\\nhop queries, their ground-truth answers, and\\nthe associated supporting evidence. We detail\\nthe procedure of building the dataset, utiliz-\\ning an English news article dataset as the un-\\nderlying RAG knowledge base. We demon-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Furthermore, to our knowledge, no existing\\nRAG benchmarking dataset focuses on multi-\\nhop queries. In this paper, we develop a novel\\ndataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-\\nhop queries, their ground-truth answers, and\\nthe associated supporting evidence. We detail\\nthe procedure of building the dataset, utiliz-\\ning an English news article dataset as the un-\\nderlying RAG knowledge base. We demon-\\nstrate the benchmarking utility of MultiHop-\\nRAG in two experiments. The first experiment\\ncompares different embedding models for re-\\ntrieving evidence for multi-hop queries. In the\\nsecond experiment, we examine the capabili-\\nties of various state-of-the-art LLMs, includ-\\ning GPT-4, PaLM, and Llama2-70B, in rea-\\nsoning and answering multi-hop queries given\\nthe evidence. Both experiments reveal that ex-\\nisting RAG methods perform unsatisfactorily'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='strate the benchmarking utility of MultiHop-\\nRAG in two experiments. The first experiment\\ncompares different embedding models for re-\\ntrieving evidence for multi-hop queries. In the\\nsecond experiment, we examine the capabili-\\nties of various state-of-the-art LLMs, includ-\\ning GPT-4, PaLM, and Llama2-70B, in rea-\\nsoning and answering multi-hop queries given\\nthe evidence. Both experiments reveal that ex-\\nisting RAG methods perform unsatisfactorily\\nin retrieving and answering multi-hop queries.\\nWe hope MultiHop-RAG will be a valuable re-\\nsource for the community in developing effec-\\ntive RAG systems, thereby facilitating greater\\nadoption of LLMs in practice. The MultiHop-\\nRAG and implemented RAG system is publicly\\navailable at https://github.com/yixuantt/\\nMultiHop-RAG/.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='1\\n\\nIntroduction\\n\\nThe emergence of large language models (LLMs),\\nsuch as ChatGPT, has fostered a wide range of inno-\\nvations, powering intelligent chatbots and other nat-\\nural language processing (NLP) applications (Ope-\\n\\nFigure 1: RAG with multi-hop query.\\n\\nnAI, 2023). One promising use case is Retrieval-\\nAugmented Generation (RAG) (Asai et al., 2023),\\nwhich optimizes the output of a large language\\nmodel by referencing an external knowledge base\\noutside of the LLM training data sources before\\ngenerating a response. RAG improves LLM’s re-\\nsponse (Borgeaud et al., 2022) and also mitigates\\nthe occurrence of hallucinations, thereby enhancing\\nthe models’ credibility (Gao et al., 2023). LLM-\\nbased frameworks, such as LlamaIndex (Liu, 2022)\\nand LangChain (Chase, 2022), specialize in sup-\\nporting RAG pipelines.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='In real-world Retrieval-Augmented Generation\\n(RAG) applications, a user’s query often necessi-\\ntates retrieving and reasoning over evidence from\\nmultiple documents, a process known as multi-hop\\nquery. For instance, consider financial analysis us-\\ning a database of financial reports. A financial ana-\\nlyst might query, Which company among Google,\\nApple, and Nvidia reported the largest profit mar-\\ngins in their third-quarter reports for 2023? or\\ninquire about a specific company’s performance\\nover time, such as How does Apple’s sales trend\\nlook over the past three years? These queries re-\\nquire evidence from multiple documents to formu-\\nlate an answer. Due to the multifaceted nature of\\nsuch queries, involving information from various\\nsources, traditional similarity matching methods\\nlike cosine similarity between query and financial\\n\\n \\n \\n \\n \\n \\n \\n\\x0cNews source\\nEvidence\\n\\nClaim'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='News source\\nEvidence\\n\\nClaim\\n\\nBridge-Topic\\nBridge-Entity\\nQuery\\n\\nAnswer\\n\\nFortune Magazine\\nBack then, just like today, home prices had boomed\\nfor years before Fed officials were ultimately forced\\nto hike interest rates aggressively in an attempt to\\nfight inflation.\\nFederal Reserve officials were forced to aggressively\\nhike interest rates to combat inflation after years of\\nbooming home prices.\\nInterest rate hikes to combat inflation\\nFederal Reserve\\nDoes the article from Fortune suggest that the Federal Reserve’s interest rate hikes are a response to past\\nconditions, such as booming home prices, while The Sydney Morning Herald article indicates that the\\nFederal Reserve’s future interest rate decisions will be based on incoming economic data?\\nYes'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='The Sydney Morning Herald\\nPostponements of such reports could complicate\\nthings for the Fed, which has insisted it will make\\nupcoming decisions on interest rates based on what\\nincoming data say about the economy.\\nThe Federal Reserve has insisted that it will base its\\nupcoming decisions on interest rates on the incoming\\neconomic data.\\nInterest rate decisions based on economic data\\nFederal Reserve\\n\\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased\\nclaim, the bridge-topic and bridge-entity, and the corresponding answer.\\n\\nreport chunk embeddings might not yield optimal\\nresults. We demonstrate this multi-hop retrieval\\nprocess in Figure 1.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Table 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased\\nclaim, the bridge-topic and bridge-entity, and the corresponding answer.\\n\\nreport chunk embeddings might not yield optimal\\nresults. We demonstrate this multi-hop retrieval\\nprocess in Figure 1.\\n\\nHowever, existing RAG benchmarks, such as\\nRGB (Chen et al., 2023) and RECALL (Liu et al.,\\n2023), mainly evaluate a simple case where the an-\\nswer of a query can be retrieved and solved using\\none single piece of evidence. None of these bench-\\nmarks assess the retrieval and reasoning capability\\nof LLMs for complex multi-hop queries. To ad-\\ndress this gap and make RAG benchmarking more\\nclosely resemble real-world scenarios, in this paper,\\nwe introduce MultiHop-RAG. To our knowledge,\\nMultiHop-RAG is one of the first RAG datasets\\nfocusing specifically on multi-hop queries.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Based on the RAG queries commonly encoun-\\ntered in real-world scenarios, we first categorize\\nmulti-hop queries into four types: Inference query,\\nComparison query, Temporal query, and Null\\nquery. The first three types — Inference, Com-\\nparison, and Temporal — require the retrieval and\\nanalysis of evidence from multiple sources, encom-\\npassing tasks like inferring relationships, compar-\\ning data points, and sequencing events over time.\\nThe Null query represents a scenario where the\\nquery cannot be derived from the knowledge base.\\nThis category is crucial for assessing whether an\\nLLM might hallucinate an answer to a multi-hop\\nquery when the retrieved text lacks relevance.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='We construct our RAG knowledge base using a\\ncollection of news articles. Using GPT-4 as a data\\ngenerator, we then take an extensive procedure to\\nconstruct a diverse set of multi-hop queries, each\\nrequiring the retrieval and reasoning over multiple\\ndocuments. An example of query construction is\\nshown in Table 1. First, we begin by extracting'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='factual sentences from each news article as evi-\\ndence. For example, an extracted piece of evidence\\nfrom an article may state: “Back then, just like\\ntoday, home prices had boomed for years before\\nFed officials were ultimately forced to hike interest\\nrates aggressively in an attempt to fight inflation.”\\nSecond, we input each evidence piece into GPT-4,\\nprompting it to rephrase the evidence into a claim.\\nThis claim is clarified with a disambiguated topic\\nand entity. For instance, GPT-4 might rephrase the\\naforementioned evidence into: “Federal Reserve\\nofficials were forced to aggressively hike interest\\nrates to combat inflation after years of booming\\nhome prices”, identifying “Interest rate hikes to\\ncombat inflation” as the topic and “Federal Re-\\nserve” as the entity. These topics and entities act as\\nbridges for constructing multi-hop queries, known'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='and entity. For instance, GPT-4 might rephrase the\\naforementioned evidence into: “Federal Reserve\\nofficials were forced to aggressively hike interest\\nrates to combat inflation after years of booming\\nhome prices”, identifying “Interest rate hikes to\\ncombat inflation” as the topic and “Federal Re-\\nserve” as the entity. These topics and entities act as\\nbridges for constructing multi-hop queries, known\\nas bridge-topic or bridge-entity. Next, we use GPT-\\n4 to generate specific multi-hop queries related to\\nthe same bridge-topic or bridge-entity, accompa-\\nnied by the correct answers. Lastly, we undertake\\na validation step to ensure the data quality.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='We demonstrate the benchmarking capabilities\\nof MultiHop-RAG using two experiments, utilizing\\na RAG system implemented with LlamaIndex (Liu,\\n2022). The first experiment involves a comparison\\nof different embedding models for retrieving rele-\\nvant evidence for multi-hop queries. In the second\\nexperiment, we assess the reasoning and answering\\nabilities of various state-of-the-art LLMs, including\\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\\nand Mixtral-8x7B, for multi-hop queries when re-\\ntrieved text is provided. The results from both ex-\\nperiments indicate that the current RAG implemen-\\ntations are inadequate for effectively retrieving and\\nanswering multi-hop queries. We publicly release'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='this challenging MultiHop-RAG dataset and hope it\\nwill be a valuable resource for the community in de-\\nveloping and benchmarking RAG systems, thereby\\nunleashing the great potential of generative AI in\\npractice.\\n\\n2 RAG with multi-Hop queries\\n\\n2.1 Retrieval-augmented Generation (RAG)'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='2 RAG with multi-Hop queries\\n\\n2.1 Retrieval-augmented Generation (RAG)\\n\\nIn an RAG application, we utilize an external cor-\\npus, denoted as D, which comprises multiple docu-\\nments and serves as the knowledge base. Each doc-\\nument within this corpus, represented as di ∈ D, is\\nsegmented into a set of chunks.These chunks are\\nthen transformed into vector representations using\\nan embedding model and stored in an embedding\\ndatabase. Given a user query q, the system typi-\\ncally retrieves the top-K chunks that best match the\\nquery. These chunks constitute the retrieval set\\nfor query q, represented as Rq = {r1, r2, ..., rK}.\\nThe retrieved chunks, combined with the query\\nand an optional prompt, are then fed into an LLM\\nto generate a final answer, following the format:\\nLLM(q, Rq, prompt) → answer.\\n\\n2.2 Multi-Hop Query'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='2.2 Multi-Hop Query\\n\\nWe define a multi-hop query as one that requires\\nretrieving and reasoning over multiple pieces of\\nsupporting evidence to provide an answer. In other\\nwords, for a multi-hop query q, the chunks in the\\nretrieval set Rq collectively provide an answer\\nto q. For example, the query \"Which company\\namong Google, Apple, and Nvidia reported the\\nlargest profit margins in their third-quarter reports\\nfor 2023?\" requires 1) retrieving relevant pieces of\\nevidence related to profit margins from the reports\\nof the three companies; 2) generating an answer by\\ncomparing and reasoning from the multiple pieces\\nof retrieved evidence. This differs from a single-\\nhop query such as \"What is Google’s profit margin\\nin the third-quarter reports for 2023,\" where the\\nanswer can be directly derived from a single piece\\nof evidence.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Based on the queries commonly used in real-\\nworld RAG systems, we identify four types of\\nmulti-hop queries. For each type, we present a\\nhypothetical query within the context of a financial\\nRAG system, where the knowledge base consists\\nof a collection of annual reports.\\nInference query: For such a query q, the answer\\nis deduced through reasoning from the retrieval\\nset Rq. An example of an inference query might'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='be: Which report discusses the supply chain risk of\\nApple, the 2019 annual report or the 2020 annual\\nreport?\\nComparison query: For such a query q, the an-\\nswer requires a comparison of evidence within the\\nretrieval set Rq. For instance, a comparison query\\nmight ask: Did Netflix or Google report higher\\nrevenue for the year 2023?\"\\nTemporal query: For such a query q, the answer\\nrequires an analysis of the temporal information\\nof the retrieved chunks. For example, a temporal\\nquery may ask: Did Apple introduce the AirTag\\ntracking device before or after the launch of the 5th\\ngeneration iPad Pro?\\nNull query: For such as query q, the answer cannot\\nbe derived from the retrieved set Rq. We include\\nthe null query to assess the generation quality, es-\\npecially regarding the issue of hallucination. For a\\nnull query, even though a retrieved set is provided,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='of the retrieved chunks. For example, a temporal\\nquery may ask: Did Apple introduce the AirTag\\ntracking device before or after the launch of the 5th\\ngeneration iPad Pro?\\nNull query: For such as query q, the answer cannot\\nbe derived from the retrieved set Rq. We include\\nthe null query to assess the generation quality, es-\\npecially regarding the issue of hallucination. For a\\nnull query, even though a retrieved set is provided,\\nan LLM should produce a null response instead\\nof hallucinating an answer. For example, assum-\\ning ABCD is a non-existent company, a null query\\nmight ask: What are the sales of company ABCD\\nas reported in its 2022 and 2023 annual reports?'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='2.3 Evaluation Metrics\\n\\nAn RAG system handling multi-hop queries can be\\nassessed from two key aspects: retrieval evaluation\\nand generation evaluation.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Retrieval Evaluation: Evidently, the quality of\\nthe retrieval set Rq determines the final genera-\\ntion quality. We compare the retrieved set with\\nthe ground truth evidence associated with each\\nquery, except for the null queries, as they have\\nno evidence to derive from. Assuming the top-\\nK chunks are retrieved, i.e., |Rq| = K, we use\\nretrieval evaluation metrics including Mean Aver-\\nage Precision at K (MAP@K), Mean Reciprocal\\nRank at K (MRR@K), and Hit Rate at K (Hit@K).\\nMAP@K measures the average top-K retrieval pre-\\ncision across all queries. MRR@K calculates the\\naverage of the reciprocal ranks of the first relevant\\nchunk for each query, considering the top-K re-\\ntrieved set. Hit@K metric measures the fraction of\\nevidence that appears in the top-K retrieved set.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Response Evaluation: Since the multi-hop\\nquery requires reasoning over multiple pieces of\\nretrieved chunks, we can also evaluate the reason-\\ning capability of the LLM by comparing the LLM\\nresponse with the ground truth answer of the query.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='news article is paired with metadata, including the\\ntitle, publish date, author, category, URL, and news\\nsource.\\nStep 2: Evidence Extraction. For each article, we\\nextract factual or opinion sentences using a trained\\nlanguage model 2. These factual sentences are later\\nused as evidence for answering multi-hop queries.\\nWe retain only those news articles containing ev-\\nidence that may have overlapping keywords with\\nother news articles. This allows us to later create\\nmulti-hop queries where the answer’s evidences\\nare drawn from multiple sources.\\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen-\\neration. Our goal is to use GPT-4 to automatically\\ngenerate high-quality multi-hop queries using the\\nevidence set. However, the raw evidence obtained\\nfrom Step 2 is not ideal for query generation due\\nto inconsistency in linguistic structure. For exam-\\nple, some pieces of evidence use pronouns to refer'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='multi-hop queries where the answer’s evidences\\nare drawn from multiple sources.\\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen-\\neration. Our goal is to use GPT-4 to automatically\\ngenerate high-quality multi-hop queries using the\\nevidence set. However, the raw evidence obtained\\nfrom Step 2 is not ideal for query generation due\\nto inconsistency in linguistic structure. For exam-\\nple, some pieces of evidence use pronouns to refer\\nto subjects and lack the actual entity in the text.\\nTo address this, we employ GPT-4 to paraphrase\\nthe evidence, which we refer to as claims, given\\nthe original evidence and its context. To ensure\\nconsistency between the generated claim and the\\nevidence, we further perform fact-checking using\\nthe UniEval (Zhong et al., 2022) framework to ver-\\nify the alignment between the evidence and claim.\\nAppendix A presents the prompt used for GPT-4\\nfor claim generation.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='To address this, we employ GPT-4 to paraphrase\\nthe evidence, which we refer to as claims, given\\nthe original evidence and its context. To ensure\\nconsistency between the generated claim and the\\nevidence, we further perform fact-checking using\\nthe UniEval (Zhong et al., 2022) framework to ver-\\nify the alignment between the evidence and claim.\\nAppendix A presents the prompt used for GPT-4\\nfor claim generation.\\nBridge-Entity and Bridge-Topic: The shared en-\\ntity or topic across pieces of evidence is referred to\\nas the bridge-entity or bridge-topic. These bridge-\\nentities or bridge-topics can be used to link dif-\\nferent pieces of evidence from which a multi-hop\\nquery’s answer is derived. For example, in a claim\\nsuch as “Google reports its third-quarter results for\\n2023, showcasing a detailed overview of its finan-\\ncial performance, including revenue growth, profit'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='tity or topic across pieces of evidence is referred to\\nas the bridge-entity or bridge-topic. These bridge-\\nentities or bridge-topics can be used to link dif-\\nferent pieces of evidence from which a multi-hop\\nquery’s answer is derived. For example, in a claim\\nsuch as “Google reports its third-quarter results for\\n2023, showcasing a detailed overview of its finan-\\ncial performance, including revenue growth, profit\\nmargins”, the term profit margin can be viewed as\\na bridge-topic and the term Google can be viewed\\nas a bridge-entity that links the different pieces of\\nevidence. We prompt GPT-4 to identify the bridge-\\nentity and bridge-topic for each claim. Appendix A\\nalso presents the prompt used for GPT-4 for bridge\\ngeneration.\\nStep 4: Query and Answer Generation. In this\\nstep, we leverage the bridge-entity or bridge-topic\\nto generate multi-hop queries. Specifically, we first'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='a bridge-topic and the term Google can be viewed\\nas a bridge-entity that links the different pieces of\\nevidence. We prompt GPT-4 to identify the bridge-\\nentity and bridge-topic for each claim. Appendix A\\nalso presents the prompt used for GPT-4 for bridge\\ngeneration.\\nStep 4: Query and Answer Generation. In this\\nstep, we leverage the bridge-entity or bridge-topic\\nto generate multi-hop queries. Specifically, we first\\ngroup the claims having the same bridge-entity or'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='2https://huggingface.co/lighteternal/fact-or-opinion-xlmr-\\n\\nFigure 2: MultiHop-RAG Construction Pipeline.\\n\\n3 A Benchmarking Dataset:\\n\\nMultiHop-RAG\\n\\nIn this section, we provide detailed information\\non the construction of the MultiHop-RAG dataset.\\nSpecifically, we describe the process of creating a\\nset of multi-hop queries, along with the correspond-\\ning ground truth evidence sets and answers derived\\nfrom a collection of news articles.\\n\\n3.1 MultiHop-RAG Construction'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Step 1: Dataset Collection. We download a news\\ndataset using the mediastack API 1, a REST API in-\\nterface delivering worldwide news data. The news\\ndata source comprises various English-language\\nwebsites covering a range of news categories: en-\\ntertainment, business, sports, technology, health,\\nand science. To mimic real-world RAG scenarios,\\nwhere the knowledge base data, such as an enter-\\nprise’s internal data, may differ from the LLMs’\\ntraining data, we select news articles published\\nfrom September 26, 2023, to December 26, 2023.\\nThis timeframe extends beyond the knowledge cut-\\noff of some widely-used LLMs, including Chat-\\nGPT and LLaMA, as of the time of writing. This\\nselection also helps in teasing out the possibility\\nof the underlying LLM having been exposed to\\nthese news articles. We only keep articles with a\\ntoken length greater than or equal to 1,024. Every'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='1https://mediastack.com/\\n\\nel\\n\\n\\x0cbridge-topic into a claim set. We restrict the claim\\nset to have at least two claims but no more than four\\nclaims. For each type of query, we feed the claim\\nset to GPT-4 and prompt it with an instruction to\\ngenerate a query with information from each claim.\\nBelow, we explain the specifications for different\\nmulti-hop query types. In the construction of each\\nquery, we also include the source of the news article\\nwhere the supporting evidence is associated with\\nto mimic real-world RAG scenarios. Appendix\\nA presents the prompts used for GPT-4 for query\\ngeneration.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Inference Query: These queries are formulated\\nby synthesizing the various characterizations of the\\nbridge-entity across multiple claims, with the final\\nanswer being the identification of the entity itself.\\nComparison Query: These queries are struc-\\ntured to compare the similarities and differences\\nrelated to the bridge entity or topic. The resultant\\nanswer to such queries is typically a definitive “yes”\\nor “no”, based on the comparison.\\n\\nTemporal Query: These queries explore the\\ntemporal ordering of events across different points\\nin time. The answer to such queries is typically a\\n“yes” or “no” or a single temporal indicator word\\nlike “before” or “after”.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Null Query: Null query is a query whose an-\\nswer cannot be derived from the retrieved set. To\\ncreate null queries, we generate multi-hop queries\\nusing entities that do not exist in the existing bridge-\\nentities. To add complexity, we also include fic-\\ntional news source metadata when formulating\\nthese questions, ensuring that the questions do not\\nreference any contextually relevant content from\\nthe knowledge base. The answer to the null query\\nshould be “insufficient information” or similar.\\nStep 5: Quality Assurance. Finally, we use two\\napproaches to reassure the dataset quality. First, we\\nmanually review a subset sample of the generated\\nmulti-hop queries, their corresponding evidence\\nsets, and the final answers. The results of the man-\\nual examination indicate a high degree of accuracy\\nand data quality. Second, we utilize GPT-4 to as-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='the knowledge base. The answer to the null query\\nshould be “insufficient information” or similar.\\nStep 5: Quality Assurance. Finally, we use two\\napproaches to reassure the dataset quality. First, we\\nmanually review a subset sample of the generated\\nmulti-hop queries, their corresponding evidence\\nsets, and the final answers. The results of the man-\\nual examination indicate a high degree of accuracy\\nand data quality. Second, we utilize GPT-4 to as-\\nsess each example in the dataset against the follow-\\ning criteria: 1) The generated query must utilize\\nall provided evidence in formulating the response;\\n2) The query should be answerable solely based\\non the provided evidence; 3) The response to the\\ngenerated query should be either a single word or\\na specific entity; 4) The query must conform to its\\ndesignated query type.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Category\\ntechnology\\nentertainment\\nsports\\nscience\\nbusiness\\nhealth\\ntotal\\n\\nAvg. Tokens Entry Count\\n\\n2262.3\\n2084.3\\n2030.6\\n1745.5\\n1723.8\\n1481.1\\n2046.5\\n\\n172\\n114\\n211\\n21\\n81\\n10\\n609\\n\\nTable 2: Descriptive statistics of the news article knowl-\\nedge base in MultiHop-RAG.\\n\\nQuery Category\\nInference Query\\nComparison Query\\nTemporal Query\\nNull Query\\nTotal\\n\\nEntry Count Percentage\\n\\n816\\n856\\n583\\n301\\n2,556\\n\\n31.92%\\n33.49%\\n22.81%\\n11.78%\\n100.00 %\\n\\nTable 3: The distribution of query types in MultiHop-\\nRAG.\\n\\n3.2 Descriptive Statistics'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='The MultiHop-RAG dataset contains six different\\ntypes of news articles, covering 609 distinct news,\\nwith an average of 2,046 tokens. The distribution of\\nthe news categories is shown in Table 2. MultiHop-\\nRAG contains four types of multi-hop queries and\\nthe distribution of these queries is shown in Table\\n3. In total, about 88% of queries in the dataset are\\nnon-null queries where answers can be retrieved\\nand reasoned from the knowledge base. In addition,\\nthe form of queries exhibits considerable diversity.\\nApproximately 27% of interrogative queries start\\nwith \"does,\" around 15% initiate with \"what,\" a\\nsimilar proportion start \"which,\" and 14% begin\\nwith \"who,\" with the remainder incorporating a\\nsmall percentage of other interrogative words such\\nas \"when.\" Moreover, the number of evidence re-\\nquired to answer a multi-hop query varies. Table\\n4 shows the distribution of evidence numbers for'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='the form of queries exhibits considerable diversity.\\nApproximately 27% of interrogative queries start\\nwith \"does,\" around 15% initiate with \"what,\" a\\nsimilar proportion start \"which,\" and 14% begin\\nwith \"who,\" with the remainder incorporating a\\nsmall percentage of other interrogative words such\\nas \"when.\" Moreover, the number of evidence re-\\nquired to answer a multi-hop query varies. Table\\n4 shows the distribution of evidence numbers for\\neach query in the dataset. Around 42% of queries\\ncan be answered using two pieces of evidence,\\nwhile approximately 30% and 15% of queries can\\nbe answered using three or four pieces of evidence,\\nrespectively.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='4 Benchmarking RAG system using\\n\\nMultiHop-RAG\\n\\nMultiHop-RAG can be used as a benchmark for var-\\nious RAG-related tasks. Broadly speaking, RAG-\\n\\n\\x0cNum. of Evidence Needed Count Percentage\\n\\n0 (Null Query)\\n2\\n3\\n4\\nTotal\\n\\n301\\n1078\\n779\\n398\\n2,556\\n\\n11.78%\\n42.18%\\n30.48%\\n15.56%\\n100.00 %\\n\\nTable 4: The distribution of the number of evidence\\nrequired to answer multi-hop queries in MultiHop-RAG.\\n\\nrelated tasks can be categorized as retrieval-related\\ntasks and generation-related tasks. A retrieval-\\nrelated task focuses on retrieving relevant text from\\nthe knowledge base, while a generation-related task\\nfocuses on generating high-quality responses given\\nthe retrieved text. In this section, we showcase two\\nuse cases for each task where MultiHop-RAG can\\nbe employed.\\n\\n4.1 Retrieval-related Task'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='An important design choice in an RAG system is\\nthe selection of the embedding model. An embed-\\nding model converts data into numerical vectors\\nand subsequently stores these vectors in embedding\\ndatabases. In this experiment, we evaluate differ-\\nent embedding models by examining their retrieval\\nquality.\\nExperiment Setup: We implement an RAG sys-\\ntem using the LlamaIndex framework (Liu, 2022).\\nWe partition the documents in the MultiHop-RAG\\nknowledge base into chunks, each consisting of 256\\ntokens. We then convert the chunks using an em-\\nbedding model and save the embeddings into a vec-\\ntor database. Similarly, in the retrieval step, we con-\\nvert a query using the same embedding model and\\nretrieve the top-K most relevant chunks that have\\nthe highest cosine similarity with the query embed-\\nding. In this experiment, we test a variety set of em-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='knowledge base into chunks, each consisting of 256\\ntokens. We then convert the chunks using an em-\\nbedding model and save the embeddings into a vec-\\ntor database. Similarly, in the retrieval step, we con-\\nvert a query using the same embedding model and\\nretrieve the top-K most relevant chunks that have\\nthe highest cosine similarity with the query embed-\\nding. In this experiment, we test a variety set of em-\\nbedding models, including the ada-embeddings by\\nOpenAI (text-embedding-ada-002, text-search-ada-\\nquery-001), voyage-02 3, llm-embedder (Zhang\\net al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),\\njina-embeddings-v2-base-en (Günther et al., 2023),\\ne5-base-v2 (Wang et al., 2022), and instructor-large\\n(Su et al., 2023). NULL queries are excluded in\\nthis experiment because there is no matching evi-\\ndence to the query. Additionally, we also include'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='bedding models, including the ada-embeddings by\\nOpenAI (text-embedding-ada-002, text-search-ada-\\nquery-001), voyage-02 3, llm-embedder (Zhang\\net al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),\\njina-embeddings-v2-base-en (Günther et al., 2023),\\ne5-base-v2 (Wang et al., 2022), and instructor-large\\n(Su et al., 2023). NULL queries are excluded in\\nthis experiment because there is no matching evi-\\ndence to the query. Additionally, we also include\\na Reranker module to examine the retrieval perfor-\\nmance, using bge-reranker-large (Xiao et al., 2023).\\nAfter retrieving 20 related chunks using the em-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='3https://www.voyageai.com/\\n\\nbedding model, we further select the top-K chunks\\nusing the Reranker.\\nExperiment Result: Table 5 shows the retrieval\\nresult of using different embedding models.\\nIt\\nshows that there is still a significant gap in retriev-\\ning relevant evidence for the multi-hop queries.\\nWhile Rerank can effectively improve retrieval rel-\\nevance, the highest Hits@10 is only 0.7467 when\\nthe Reranker technique is used. Moreover, the drop\\nin the highest Hits@4 to 0.6625 is worrisome. In\\npractical RAG systems, the underlying LLM of-\\nten has a context window limit. As a result, the\\nnumber of retrieved chunks is usually restricted to\\na small number. The low values of the retrieval\\nmetrics highlight the challenges in retrieving rele-\\nvant pieces of evidence for multi-hop queries when\\nusing direct similarity matching between the multi-\\nhop query and text chunks.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='4.2 Generation-related Task'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='The underlying LLMs play a crucial role in gen-\\nerating responses in an RAG system. In this ex-\\nperiment, we evaluate the quality of generated re-\\nsponses under two different settings. In the first\\nsetting, we employ the best-performing retrieval\\nmodel, namely voyage-02 with bge-reranker-large,\\nas indicated in Table 5, to retrieve the top-K texts\\nand then feed them into the LLM. In the second\\nsetting, we use the ground-truth evidence associ-\\nated with each query as the retrieved text for the\\nLLM. This setting represents a ceiling performance\\nfor testing the LLM’s response capabilities, as it\\nutilizes the actual evidences.\\nExperiment Setup: In the first experiment, we\\nretrieve top-6 chunks so that the total length of the\\nretrieved text does not exceed 2,048. All queries\\nin MultiHop-RAG are tested in the experiment.\\nIn the second experiment, since the null queries'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='ated with each query as the retrieved text for the\\nLLM. This setting represents a ceiling performance\\nfor testing the LLM’s response capabilities, as it\\nutilizes the actual evidences.\\nExperiment Setup: In the first experiment, we\\nretrieve top-6 chunks so that the total length of the\\nretrieved text does not exceed 2,048. All queries\\nin MultiHop-RAG are tested in the experiment.\\nIn the second experiment, since the null queries\\ndo not have associated evidence, we exclude this\\ntype of query in the experiment. For the LLMs\\nused in the experiment, we consider state-of-the-\\nart commercial models, including GPT-4 (OpenAI,\\n2023), GPT-3.5, Claude-2 (Anthropic, 2023), and\\nGoogle-PaLM (Google, 2023). We obtain answers\\nusing the provided API of the respective models.\\nWe also assess some open-source models, includ-\\ning Mixtral-8x7b-instruct (Jiang et al., 2024) and'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='do not have associated evidence, we exclude this\\ntype of query in the experiment. For the LLMs\\nused in the experiment, we consider state-of-the-\\nart commercial models, including GPT-4 (OpenAI,\\n2023), GPT-3.5, Claude-2 (Anthropic, 2023), and\\nGoogle-PaLM (Google, 2023). We obtain answers\\nusing the provided API of the respective models.\\nWe also assess some open-source models, includ-\\ning Mixtral-8x7b-instruct (Jiang et al., 2024) and\\nLlama-2-70b-chat-hf (Touvron et al., 2023).\\nExperiment Results: Table 6 shows the response\\naccuracy of different LLMs. First, we can see\\nthat the response accuracy rate using the retrieved'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Embedding\\n\\nWithout Reranker\\n\\nWith bge-reranker-large\\n\\nMRR@10 MAP@10 Hits@10 Hits@4 MRR@10 MAP@10 Hits@10 Hits@4\\n\\ntext-embedding-ada-002\\ntext-search-ada-query-001\\nllm-embedder\\nbge-large-en-v1.5\\njina-embeddings-v2-base-en\\nintfloat/e5-base-v2\\nvoyage-02\\nhkunlp/instructor-large\\n\\n0.4203\\n0.4203\\n0.2558\\n0.4298\\n0.0621\\n0.1843\\n0.3934\\n0.3458\\n\\n0.3431\\n0.3431\\n0.1725\\n0.3423\\n0.031\\n0.1161\\n0.3143\\n0.265\\n\\n0.6381\\n0.6399\\n0.4499\\n0.6718\\n0.1479\\n0.3556\\n0.6506\\n0.5717\\n\\n0.504\\n0.5031\\n0.3189\\n0.5221\\n0.0802\\n0.2334\\n0.4619\\n0.4229\\n\\n0.5477\\n0.5483\\n0.425\\n0.563\\n0.1412\\n0.3237\\n0.586\\n0.5115\\n\\n0.4625\\n0.4625\\n0.3059\\n0.4759\\n0.0772\\n0.2165\\n0.4795\\n0.4118\\n\\n0.7059\\n0.7064\\n0.5478\\n0.7183\\n0.1909\\n0.4176\\n0.7467\\n0.659\\n\\n0.6169\\n0.6174\\n0.4756\\n0.6364\\n0.1639\\n0.3716\\n0.6625\\n0.5775\\n\\nTable 5: Retrieval performance of different embedding models.\\n\\nModels\\n\\nGPT-4\\nChatGPT\\nLlama-2-70b-chat-hf\\nMixtral-8x7B-Instruct\\nClaude-2.1\\nGoogle-PaLM\\n\\nAccuracy'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='0.504\\n0.5031\\n0.3189\\n0.5221\\n0.0802\\n0.2334\\n0.4619\\n0.4229\\n\\n0.5477\\n0.5483\\n0.425\\n0.563\\n0.1412\\n0.3237\\n0.586\\n0.5115\\n\\n0.4625\\n0.4625\\n0.3059\\n0.4759\\n0.0772\\n0.2165\\n0.4795\\n0.4118\\n\\n0.7059\\n0.7064\\n0.5478\\n0.7183\\n0.1909\\n0.4176\\n0.7467\\n0.659\\n\\n0.6169\\n0.6174\\n0.4756\\n0.6364\\n0.1639\\n0.3716\\n0.6625\\n0.5775\\n\\nTable 5: Retrieval performance of different embedding models.\\n\\nModels\\n\\nGPT-4\\nChatGPT\\nLlama-2-70b-chat-hf\\nMixtral-8x7B-Instruct\\nClaude-2.1\\nGoogle-PaLM\\n\\nAccuracy\\n\\nRetrieved Chunk Ground-truth Chunk\\n\\n0.56\\n0.44\\n0.28\\n0.32\\n0.52\\n0.47\\n\\n0.89\\n0.57\\n0.32\\n0.36\\n0.56\\n0.74\\n\\nTable 6: Generation accuracy of LLMs.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Models\\n\\nGPT-4\\nChatGPT\\nLlama-2-70b-chat-hf\\nMixtral-8x7B-Instruct\\nClaude-2.1\\nGoogle-PaLM\\n\\nAccuracy\\n\\nRetrieved Chunk Ground-truth Chunk\\n\\n0.56\\n0.44\\n0.28\\n0.32\\n0.52\\n0.47\\n\\n0.89\\n0.57\\n0.32\\n0.36\\n0.56\\n0.74\\n\\nTable 6: Generation accuracy of LLMs.\\n\\nchunks is not satisfactory, with the state-of-the-\\nart GPT-4 model achieving only 0.56 accuracy.\\nThis is expected, because the retrieval component\\nfalls short in retrieving relevant evidences from the\\nknowledge base. Second, even when we provide\\nthe LLM with the ground-truth evidences, we can\\nsee that the response accuracy is far from being per-\\nfect. Open source LLM such as Llama02-70B and\\nMixtral-8x7B only achieve an accuracy of 0.32 and\\n0.36 respectively. GPT-4 achieves strong reason-\\ning capability with an accuracy of 0.89, followed\\nby the second-based LLM Google-PaLM with an\\naccuracy of 0.74.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Figure 3 shows the detailed results of different\\nquery types for GPT-4 and Mixtral-8x7B-instruct.\\nBoth models show relatively high robustness on\\nnull queries, meaning they are generally good at\\ndetermining when a query cannot be answered\\nbased on the retrieved text. This is encouraging be-\\ncause one benefit of RAG is to mitigating the LLM\\nhallucination issue by augmenting LLM with re-\\ntrieval knowledge. However, Mixtral-8x7B model\\nperforms significantly worse than the GPT-4 in\\ncomparison and temporal queries. Upon reviewing\\nthe incorrect responses, we find that Mixtral-8x7B\\nfails to accurately handle logical negation, leading\\nto misinterpretation of statements and thus a low\\nperformance in the comparison queries. In addi-\\ntion, Mixtral-8x7B often fails to correctly identify\\n\\nFigure 3: Generation accuracy for different query types.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Figure 3: Generation accuracy for different query types.\\n\\nthe chronological order of events, which is crucial\\nfor answering temporal queries where timing is a\\nkey factor. Taken together, this experiment demon-\\nstrates that there is still room for improvement in\\nthe reasoning capabilities of LLMs, particularly\\nthose that are open-source, for multi-hop queries.\\n\\n4.3 Other Use Cases'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='4.3 Other Use Cases\\n\\nBeyond embedding models and LLM generation,\\nthere are other areas worth exploring. For exam-\\nple, query decomposition is a widely utilized tech-\\nnique in RAG frameworks, such as LLamaIndex.\\nThis process involves breaking down the query\\ninto smaller segments; it targets a single document\\nfor retrieval and integrates the information subse-\\nquently, thereby potentially enhancing retrieval ac-\\ncuracy. Another advanced and promising approach\\ninvolves building LLM-based agents that can au-\\ntomatically plan and execute multi-hop queries,\\nsuch as AutoGPT (Gravitas, 2023). Another area\\nof interest is the hybrid retrieval approach, which\\ncombines keyword and embedding matching tech-\\n\\n\\x0cniques. We believe that there are many potential\\nareas for enhancing RAG’s performance on multi-\\nhop queries, and the curated dataset MultiHop-\\nRAG can be a valuable resource to the community.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='niques. We believe that there are many potential\\nareas for enhancing RAG’s performance on multi-\\nhop queries, and the curated dataset MultiHop-\\nRAG can be a valuable resource to the community.\\n\\n5 Related Work'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='RAG Evaluation: As RAG systems gain increas-\\ning popularity, a variety of RAG benchmarking\\ndatasets and evaluation tools have been developed.\\nFor instance, RGB (Chen et al., 2023) and RE-\\nCALL (Liu et al., 2023) evaluate the performance\\nof LLMs in generating responses for RAG systems\\nunder conditions involving noisy, integrative, and\\ncounterfactual queries. However, both datasets pri-\\nmarily focus on evaluating the generation aspect\\nof RAG systems without specifically addressing\\ntheir retrieval accuracy.\\nIn addition, recent ad-\\nvancements have been made in automated RAG\\nevaluation tools, such as ARES (Saad-Falcon et al.,\\n2023) and RAGAS (Es et al., 2023). These tools\\nutilize LLMs to automatically assess the quality of\\nRAG generation, yet they do not introduce bench-\\nmarking datasets. Our work introduces one of the\\nfirst RAG benchmarking datasets, consisting of a'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='of RAG systems without specifically addressing\\ntheir retrieval accuracy.\\nIn addition, recent ad-\\nvancements have been made in automated RAG\\nevaluation tools, such as ARES (Saad-Falcon et al.,\\n2023) and RAGAS (Es et al., 2023). These tools\\nutilize LLMs to automatically assess the quality of\\nRAG generation, yet they do not introduce bench-\\nmarking datasets. Our work introduces one of the\\nfirst RAG benchmarking datasets, consisting of a\\nknowledge base, a large collection of multi-hop\\nqueries, their ground-truth answers, and the associ-\\nated supporting evidence, thereby complementing\\nexisting RAG evaluations.\\nRetrieval datasets: Apart from the context of\\nRAG, several benchmarking datasets exist for in-\\nformation retrieval evaluation. The FEVER (Fact\\nExtraction and VERification) dataset, for instance,\\ncontains claims classified as Supported, Refuted,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='knowledge base, a large collection of multi-hop\\nqueries, their ground-truth answers, and the associ-\\nated supporting evidence, thereby complementing\\nexisting RAG evaluations.\\nRetrieval datasets: Apart from the context of\\nRAG, several benchmarking datasets exist for in-\\nformation retrieval evaluation. The FEVER (Fact\\nExtraction and VERification) dataset, for instance,\\ncontains claims classified as Supported, Refuted,\\nor NotEnoughInfo by the given Wikipedia article\\n(Thorne et al., 2018). Similarly, the SciFact dataset\\ncomprises scientific claims paired with evidence-\\ncontaining abstracts (Wadden et al., 2020). How-\\never, the claims in both datasets are single-hop\\nstatements, and the supporting evidence is from one\\nsingle article, in contrast to the multi-hop queries\\ndiscussed in this paper. Another dataset, HoVer,\\ninvolves claims that require extracting and reason-'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='(Thorne et al., 2018). Similarly, the SciFact dataset\\ncomprises scientific claims paired with evidence-\\ncontaining abstracts (Wadden et al., 2020). How-\\never, the claims in both datasets are single-hop\\nstatements, and the supporting evidence is from one\\nsingle article, in contrast to the multi-hop queries\\ndiscussed in this paper. Another dataset, HoVer,\\ninvolves claims that require extracting and reason-\\ning from multiple Wikipedia articles (Jiang et al.,\\n2020). However, unlike our dataset, HoVer focuses\\nsolely on classifying claims as either supported or\\nnot supported by the articles without evaluating\\nan LLM generation step. Moreover, in HoVer, the\\nWikipedia articles from which evidence is drawn\\nare given for claim verification, which is signifi-\\ncantly different from our setting, where relevant\\npieces of evidence need to be extracted from a'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='large knowledge base. Separately, (Kamalloo et al.,\\n2023) evaluates a range of commercial embedding\\nAPIs for information retrieval, but this evaluation\\nis not contextualized within the framework of RAG\\nsystems either.\\nMulti-document QA datasets:\\nQuestion-\\nanswering (QA) is a fundamental task in NLP, and\\nseveral popular benchmarks, such as HotpotQA\\n(Yang et al., 2018), MultiRC (Khashabi et al.,\\n2018), and 2WikiMultiHopQA (Ho et al., 2020),\\naim to achieve QA from multiple sources of\\ndocuments. This task is similar to our multi-hop\\nquery RAG task, as both involve reasoning from\\nmultiple sources of information. However, these\\ndatasets primarily focus on assessing a model’s\\nreasoning skills, and they do not emphasize the\\nretrieval of evidence from a knowledge base.\\nAdditionally, their primary data sources Wikipedia,\\nsignificantly overlap with the training data of'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='aim to achieve QA from multiple sources of\\ndocuments. This task is similar to our multi-hop\\nquery RAG task, as both involve reasoning from\\nmultiple sources of information. However, these\\ndatasets primarily focus on assessing a model’s\\nreasoning skills, and they do not emphasize the\\nretrieval of evidence from a knowledge base.\\nAdditionally, their primary data sources Wikipedia,\\nsignificantly overlap with the training data of\\nmost existing LLMs. If we use these sources for\\nbenchmarking RAG systems, there is a potential\\nconcern that LLM responses might rely on training\\nknowledge rather than reasoning from the retrieved\\nknowledge base.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='6 Conclusion\\n\\nIn this work, we introduce MultiHop-RAG, a novel\\nand unique dataset designed for queries that re-\\nquire retrieval and reasoning from multiple pieces\\nof supporting evidence. These types of multi-hop\\nqueries represent user queries commonly encoun-\\ntered in real-world scenarios. MultiHop-RAG con-\\nsists of a knowledge base, a large collection of\\nmulti-hop queries, their ground-truth answers, and\\nthe associated supporting evidence. This paper\\ndetails the creation process of MultiHop-RAG, em-\\nploying a hybrid approach that integrates human\\neffort with GPT-4. Additionally, we explore two\\nuse cases of MultiHop-RAG in the benchmarking\\nof RAG systems, thereby highlighting the potential\\napplications of this dataset. By publicly releas-\\ning MultiHop-RAG, we aim to provide a valuable\\nresource to the community, contributing to the ad-\\nvancement and benchmarking of RAG systems.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Limitations\\n\\nThis work has several limitations that can be im-\\nproved in future research. First, our ground truth\\nanswers are restricted to simple responses such as\\n“yes\", “no\", entity names, or temporal indicators\\nlike “before\" or “after\" to facilitate the use of a\\n\\n\\x0cstraightforward accuracy metric for evaluating gen-\\neration performance. Future work could consider\\nallowing free text as answers and employing more\\nsophisticated metrics to assess generation quality.\\nSecond, the current dataset limits supporting ev-\\nidence for a query to a maximum of four pieces.\\nFuture work can extend the dataset by including\\nqueries that require retrieving and reasoning from\\neven more evidence. Lastly, while our experiments\\nutilize a basic RAG framework using LlamaIndex,\\nfuture work could involve evaluating the answering\\nof multi-hop queries using more advanced RAG\\nframeworks or LLM-agent frameworks.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='References\\n\\nAnthropic. 2023. Claude 2.1 (May version). https:\\n//api.anthropic.com/v1/messages. Claude 2.1.\\n\\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\\nChen. 2023. Retrieval-based language models and\\napplications. In Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 6: Tutorial Abstracts), pages 41–46.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, Diego\\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\\nElsen, and Laurent Sifre. 2022. Improving language\\nmodels by retrieving from trillions of tokens.\\nIn\\nProceedings of the 39th International Conference\\non Machine Learning, volume 162 of Proceedings\\nof Machine Learning Research, pages 2206–2240.\\nPMLR.\\n\\nHarrison Chase. 2022. LangChain.\\n\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\\n2023. Benchmarking large language models in\\nretrieval-augmented generation.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Harrison Chase. 2022. LangChain.\\n\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\\n2023. Benchmarking large language models in\\nretrieval-augmented generation.\\n\\nShahul Es, Jithin James, Luis Espinosa-Anke, and\\nSteven Schockaert. 2023. Ragas: Automated evalua-\\ntion of retrieval augmented generation.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023. Enabling large language models to generate\\ntext with citations.\\n\\nGoogle.\\n\\n2023.\\n\\nPaLM 2\\n\\n(May\\n\\nversion).\\n\\nhttps://generativelanguage.googleapis.\\ncom/v1beta2/models/. Chat-bison-002.\\n\\nSignificant Gravitas. 2023. Autogpt. https://github.\\n\\ncom/Significant-Gravitas/AutoGPT.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023. Enabling large language models to generate\\ntext with citations.\\n\\nGoogle.\\n\\n2023.\\n\\nPaLM 2\\n\\n(May\\n\\nversion).\\n\\nhttps://generativelanguage.googleapis.\\ncom/v1beta2/models/. Chat-bison-002.\\n\\nSignificant Gravitas. 2023. Autogpt. https://github.\\n\\ncom/Significant-Gravitas/AutoGPT.\\n\\nMichael Günther, Jackmin Ong, Isabelle Mohr, Alaed-\\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\\nAkram, Susana Guzman, Georgios Mastrapas, Saba\\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\\nand Han Xiao. 2023.\\nJina embeddings 2: 8192-\\ntoken general-purpose text embeddings for long doc-\\numents.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Significant Gravitas. 2023. Autogpt. https://github.\\n\\ncom/Significant-Gravitas/AutoGPT.\\n\\nMichael Günther, Jackmin Ong, Isabelle Mohr, Alaed-\\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\\nAkram, Susana Guzman, Georgios Mastrapas, Saba\\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\\nand Han Xiao. 2023.\\nJina embeddings 2: 8192-\\ntoken general-purpose text embeddings for long doc-\\numents.\\n\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing a multi-\\nhop QA dataset for comprehensive evaluation of\\nreasoning steps. In Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics,\\npages 6609–6625, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing a multi-\\nhop QA dataset for comprehensive evaluation of\\nreasoning steps. In Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics,\\npages 6609–6625, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\\nRoux, Arthur Mensch, Blanche Savary, Chris\\nBamford, Devendra Singh Chaplot, Diego de las\\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\\nanna Lengyel, Guillaume Bour, Guillaume Lam-\\nple, Lélio Renard Lavaud, Lucile Saulnier, Marie-\\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\\nSophia Yang, Szymon Antoniak, Teven Le Scao,\\nThéophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimothée Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles\\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\\nHoVer: A dataset for many-hop fact extraction and\\nclaim verification. In Findings of the Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP).\\n\\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,\\nNandan Thakur, David Alfonso-Hermelo, Mehdi\\nRezagholizadeh, and Jimmy Lin. 2023. Evaluat-\\ning embedding apis for information retrieval. arXiv\\npreprint arXiv:2305.06300.\\n\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\\nShyam Upadhyay, and Dan Roth. 2018. Looking\\nBeyond the Surface: A Challenge Set for Reading\\nComprehension over Multiple Sentences. In Proc. of\\nthe Annual Conference of the North American Chap-\\nter of the Association for Computational Linguistics\\n(NAACL).\\n\\nJerry Liu. 2022. LlamaIndex.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,\\nShyam Upadhyay, and Dan Roth. 2018. Looking\\nBeyond the Surface: A Challenge Set for Reading\\nComprehension over Multiple Sentences. In Proc. of\\nthe Annual Conference of the North American Chap-\\nter of the Association for Computational Linguistics\\n(NAACL).\\n\\nJerry Liu. 2022. LlamaIndex.\\n\\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao\\nZhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.\\nRecall: A benchmark for llms robustness against\\nexternal counterfactual knowledge.\\n\\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat.\\n\\nopenai.com/chat. gpt-4-1106-preview.\\n\\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\\nMatei Zaharia. 2023. Ares: An automated evalua-\\ntion framework for retrieval-augmented generation\\nsystems.\\n\\n\\x0cJiawei Han. 2022.\\ndimensional evaluator for text generation.\\n\\nTowards a unified multi-\\n\\nA Appendix A: GPT-4 Prompts Used for'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='OpenAI. 2023. GPT4 (Nov 7 version). https://chat.\\n\\nopenai.com/chat. gpt-4-1106-preview.\\n\\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\\nMatei Zaharia. 2023. Ares: An automated evalua-\\ntion framework for retrieval-augmented generation\\nsystems.\\n\\n\\x0cJiawei Han. 2022.\\ndimensional evaluator for text generation.\\n\\nTowards a unified multi-\\n\\nA Appendix A: GPT-4 Prompts Used for\\n\\nData Generation\\n\\nWe present the prompts used for guiding GPT-4 for\\ndata generation. Table 7 shows the prompt used for\\nclaim generation, along with the corresponding top-\\nics and entities within these claims. Table 8, Table\\n9, and Table 10 respectively show the prompts used\\nfor generating multi-hop queries of the inference,\\ncomparison, and temporal types.\\n\\nB Appendix B: Dataset Examples'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='B Appendix B: Dataset Examples\\n\\nIn this appendix, we present an example of each\\ntype of multi-hop query included in the MultiHop-\\nRAG dataset. These examples are illustrated in the\\nrespective tables: Table 12 for Inference Queries,\\nTable 13 for Comparison Queries, Table 14 for\\nTemporal Queries, and Table 15 for Null Queries.\\nEach query is paired with a ground-truth answer\\nfor the evaluation of generation accuracy, while\\nmultiple pieces of supporting evidence are included\\nfor assessing retrieval performance. Additionally,\\nmetadata such as the title, source, and publication\\ntime of the news articles are provided as references.\\n\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\\nembedder, any task: Instruction-finetuned text em-\\nbeddings.\\n\\nJames\\n\\nAndreas Vlachos,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\\nembedder, any task: Instruction-finetuned text em-\\nbeddings.\\n\\nJames\\n\\nAndreas Vlachos,\\n\\nChristos\\nThorne,\\nChristodoulopoulos,\\nand Arpit Mittal. 2018.\\nFever: a large-scale dataset for fact extraction and\\nverification.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Isabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\\nWang, Madeleine van Zuylen, Arman Cohan, and\\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\\nscientific claims. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 7534–7550, Online. As-\\nsociation for Computational Linguistics.\\n\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\\nand Furu Wei. 2022. Text embeddings by weakly-\\nsupervised contrastive pre-training. arXiv preprint\\narXiv:2212.03533.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\\nand Furu Wei. 2022. Text embeddings by weakly-\\nsupervised contrastive pre-training. arXiv preprint\\narXiv:2212.03533.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\nto advance general chinese embedding.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\\ngio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. 2018. HotpotQA: A dataset\\nfor diverse, explainable multi-hop question answer-\\ning. In Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP).\\n\\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\\nment large language models.\\n\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\\nment large language models.\\n\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\n\\n\\x0cA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given\\nevidence from the original context, please extract one claim and its associated topics.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\n\\n\\x0cA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given\\nevidence from the original context, please extract one claim and its associated topics.\\n\\nNote: The claim should not contain ambiguous references, such as ’he’,’ she,’ and’ it’, and should use\\ncomplete names. If there are multiple topics, give the most dominant one. The target of the claim (one\\nentity)is the specific individual, group, or organization that the statement or assertion within a text is\\ndirected towards or about which it is making a case. The topic of the claim should be a simple phrase\\nrepresenting the claim’s central argument concept. If there is no claim, please leave it blank. Please\\ngenerate a claim based on the given evidence. Don’t generate the evidence yourself.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Please give the response following this format:\\nEvidence: [original context]\\nClaims: [extract claim]\\nClaim Target: [target]\\nClaim Topic: [topic]\\n\\nHere are examples:\\n<examples>\\nNow, it’s your turn.\\n<News>\\n<evidence>\\n\\nTable 7: Claim Generation Prompting'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\\ninformation from different locations or sources to arrive at an answer. The following are news articles’\\nmetadata and claims come from the articles. All the claims from the article are related to a similar\\ntarget. Your task is to generate one multi-hop inference question based on the claims. Here are some\\ninstructions:\\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of\\ninformation are related or how they can be combined to form a more complex idea.\\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the\\nsentences but instead requires understanding and linking the information from all of the sources. The\\nanswer is <target>.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='instructions:\\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of\\ninformation are related or how they can be combined to form a more complex idea.\\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the\\nsentences but instead requires understanding and linking the information from all of the sources. The\\nanswer is <target>.\\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is\\nclear and unambiguous.\\n4. Use the keywords: <key set>'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='<examples>\\nContext:\\n<Context>\\n\\nTable 8: Inference Query Generation Prompting\\n\\n\\x0c<Context>\\n\\nThe above are news articles’ metadata and claims come from the articles. All the claims from the\\narticles are related to a similar target. Your task is to generate one comparison question based on all the\\nclaims from different sources. This question needs to compare some factual elements of the claims that\\nare explicitly stated to find where they agree or differ. The correct answer to this question is expressed\\nas a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative\\nquestion from claims, you need to use the following keywords: <key set>\\n\\nThe Good Comparison Questions:\\n<examples>\\nYour Comparison Question:\\n\\nTable 9: Comparison Query Generation Prompting\\n\\n<Context>'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='The Good Comparison Questions:\\n<examples>\\nYour Comparison Question:\\n\\nTable 9: Comparison Query Generation Prompting\\n\\n<Context>\\n\\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news\\narticles. That is to compare the consistency or sequence of reports on similar topics at multiple different\\ntime points. If it is to compare the consistency, please clearly mention the news source and time in the\\nquestion using <time frame>. If it is to compare sequences of reports, just clearly mention the news\\nsource and do not mention the timeline. Utilize the following keywords provided in the <key set> to\\nconstruct the question. The correct answer should based on the factual excerpts and is only one word.\\n\\n<examples>\\nYour time-sensitive comparison question:\\n\\nTable 10: Temporal Query Generation Prompting'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='<examples>\\nYour time-sensitive comparison question:\\n\\nTable 10: Temporal Query Generation Prompting\\n\\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\\ninformation from different locations or sources to arrive at an answer. Considering you have read\\nat least two news articles on <entity>, construct a multi-hop question that incorporates all the news\\nsources. The source of the news should be stated in the question. Also, ensure that the answer to the\\nquestion is a single word/entity. Do not answer this question directly. Just give me the question:\\n\\nTable 11: Null Query Generation Prompting'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Table 11: Null Query Generation Prompting\\n\\n\\x0cQuery: Which platform is at the center of discussions in articles from Music Business Worldwide,\\nPolygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate\\nover \"reaction\" content, and being the most used app overnight by young people?\\nAnswer: YouTube\\nEvidence List:\\nTitle: Sony Music’s artists aren’t involved in YouTube’s new voice-cloning AI experiment.\\nSource: Music Business Worldwide\\nPublished Time: 2023-11-23T18:48:48+00:00\\nFact: During this period of discussion, YouTube has made a number of positive announcements\\nregarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their\\nability to police it.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Title: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\\nSource: Polygon\\nPublished Time: 2023-10-25T18:18:06+00:00\\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident\\nbetween two creators has refueled the urgency of the conversation.\\n\\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study\\nSource: FOX News - Health\\nPublished Time: 2023-10-01T09:05:26+00:00\\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube\\nappeared to be the longest-running app because videos were often left playing during the night.\\n\\nTable 12: The example of inference questions'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Table 12: The example of inference questions\\n\\nQuery: Did the Cnbc | World Business News Leader report on Nike’s net income and the article from\\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics?\\nAnswer: Yes\\nEvidence List:\\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\\nmargin\\nSource: Cnbc | World Business News Leader\\nPublished Time: 2023-09-28T20:31:00+00:00\\nFact: The company’s reported net income for the three-month period that ended August 31 was $1.45\\nbillion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Title: ASX set to open higher as Wall Street rebounds; $A rises\\nSource: The Age\\nPublished Time: 2023-10-04T21:01:01+00:00\\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from\\nits highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\\n\\nTable 13: The example of comparison questions\\n\\n\\x0cQuery: Was the performance of the Chicago Bears’ defense reported as improved by Yardbarker after\\nSporting News highlighted a sack by the Bears’ defense on Joshua Dobbs during the NFL ’Monday\\nNight Football’ game?\\nAnswer: Yes\\nEvidence List:\\nTitle: Bears vs. Vikings live score, updates, highlights from NFL ’Monday Night Football’ game\\nSource: Sporting News\\nPublished Time: 2023-11-27T23:32:04+00:00\\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='Title: Hottest seat on each NFC team: Buns burning for these four head coaches\\nSource: Yardbarker\\nPublished Time: 2023-11-30T22:29:33+00:00\\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by\\nbehind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\\n\\nTable 14: The example of time-sensitive questions\\n\\nQuery: What is the first letter of the CEO’s last name in the news article from Bloomberg on TomTom,\\nand what is the first letter of the city where the company’s headquarters is located in the news article\\nfrom Reuters?\\nAnswer: Insufficient information.\\n\\nTable 15: The example of negative rejection questions')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.21059v1', 'Published': datetime.date(2024, 7, 26), 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang'}, page_content='Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2409.01666v1', 'Published': datetime.date(2024, 9, 3), 'Title': 'In Defense of RAG in the Era of Long-Context Language Models', 'Authors': 'Tan Yu, Anbang Xu, Rama Akkiraju'}, page_content='Overcoming the limited context limitations in early-generation LLMs,\\nretrieval-augmented generation (RAG) has been a reliable solution for\\ncontext-based answer generation in the past. Recently, the emergence of\\nlong-context LLMs allows the models to incorporate much longer text sequences,\\nmaking RAG less attractive. Recent studies show that long-context LLMs\\nsignificantly outperform RAG in long-context applications. Unlike the existing\\nworks favoring the long-context LLM over RAG, we argue that the extremely long\\ncontext in LLMs suffers from a diminished focus on relevant information and\\nleads to potential degradation in answer quality. This paper revisits the RAG\\nin long-context answer generation. We propose an order-preserve\\nretrieval-augmented generation (OP-RAG) mechanism, which significantly improves\\nthe performance of RAG for long-context question-answer applications. With\\nOP-RAG, as the number of retrieved chunks increases, the answer quality\\ninitially rises, and then declines, forming an inverted U-shaped curve. There\\nexist sweet points where OP-RAG could achieve higher answer quality with much\\nless tokens than long-context LLM taking the whole context as input. Extensive\\nexperiments on public benchmark demonstrate the superiority of our OP-RAG.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2401.15391v1', 'Published': datetime.date(2024, 1, 27), 'Title': 'MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries', 'Authors': 'Yixuan Tang, Yi Yang'}, page_content='Retrieval-augmented generation (RAG) augments large language models (LLM) by\\nretrieving relevant knowledge, showing promising potential in mitigating LLM\\nhallucinations and enhancing response quality, thereby facilitating the great\\nadoption of LLMs in practice. However, we find that existing RAG systems are\\ninadequate in answering multi-hop queries, which require retrieving and\\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-hop queries, their ground-truth\\nanswers, and the associated supporting evidence. We detail the procedure of\\nbuilding the dataset, utilizing an English news article dataset as the\\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\\nMultiHop-RAG in two experiments. The first experiment compares different\\nembedding models for retrieving evidence for multi-hop queries. In the second\\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\\nqueries given the evidence. Both experiments reveal that existing RAG methods\\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\\nMultiHop-RAG will be a valuable resource for the community in developing\\neffective RAG systems, thereby facilitating greater adoption of LLMs in\\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\\nhttps://github.com/yixuantt/MultiHop-RAG/.')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_arxxiv = extract_text_from_arxiv('RAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "credentials = os.getenv('credentials')\n",
    "llm = GigaChat(auth_url = 'https://sm-auth-sd.prom-88-89-apps.ocp-geo.ocp.sigma.sbrf.ru/api/v2/oauth',credentials=credentials, verify_ssl_certs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Учитывая историю чата и последний вопрос пользователя, \"\n",
    "    \"который может ссылаться на контекст в истории чата,\"\n",
    "    \"сформулируйте отдельный вопрос, который можно понять \"\n",
    "    \"без истории чата. НЕ отвечайте на вопрос - просто переформулируйте его,\"\n",
    "    \"если нужно, а в противном случае верните как есть. Всегда отвечай на русском языке\"\n",
    "    # \"Given a chat history and the latest user question \"\n",
    "    # \"which might reference context in the chat history, \"\n",
    "    # \"formulate a standalone question which can be understood \"\n",
    "    # \"without the chat history. Do NOT answer the question, \"\n",
    "    # \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever_arxiv, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    # \"Вы являетесь помощником при выполнении заданий по поиску ответов на вопросы.\"\n",
    "    # \"Используйте приведенные ниже фрагменты из извлеченного контекста для ответа\"\n",
    "    # \"на вопрос. Если вы не знаете ответа, скажите, что вы\"\n",
    "    # \"не знаете. Используйте максимум три предложения и старайтесь,\"\n",
    "    # \"чтобы ответ был кратким. \"\n",
    "    # \"\\n\\n\"\n",
    "    \"Твоя роль: Аналитик специализирующийся на быстром поиске информации в предоставленном контексте.\"\n",
    "    \"Краткая инструкция: Анализировать предложенный контекст и отвечать на вопросы, опираясь исключительно на информацию из этого контекста.\"\n",
    "    \"Что тебе делать: При получении вопроса и соответствующего контекста тщательно изучи предоставленную информацию, выяви ключевые факты и данные,\\n\"\n",
    "    \"затем используй их для формирования точного ответа на заданный вопрос. Всегда отвечай на русском языке.\"\n",
    "    \"Твоя цель: Обеспечить абсолютно точный ответ, полностью основанный на информации из предложенного контекста, без внесения внешних данных или предположений.\"\n",
    "    \"Результат: Ответ должен быть четким и точным, содержать только информацию из предложенного контекста.\\n\"\n",
    "    \"Ожидается, что ответ будет логически обоснованным и последовательным.\"\n",
    "    \"Ограничения: Строго придерживайся информации из предложенного контекста при ответе на вопрос, избегай допущений или добавления информации,\\n\"\n",
    "    \"не содержащейся в контексте.\"\n",
    "    \"Дополнительные техники: Используй технику цепочки мыслей для объяснения твоего рассуждения и логического процесса, приводящего к ответу.\\n\"\n",
    "    \"Кроме того, применяй критическое мышление для оценки достоверности и релевантности информации в контексте, задавая вопросы и проверяя предположения, которые могут влиять на твой ответ.\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = 'Расскажи про retrieval evalation и response evolation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Расскажи про retrieval evalation и response evolation',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2409.01666v1'}, page_content='Meanwhile, using the proposed order-preserve\\nRAG, as the number of retrieved chunks increases,\\nthe answer quality initially rises and then declines.\\nThis is because, with more retrieved chunks, the\\nmodel has access to more potentially relevant in-\\nformation, which improves the chances of retriev-\\ning the correct context needed to generate a high-\\nquality answer. However, as more chunks are re-\\ntrieved, the likelihood of introducing irrelevant or\\ndistracting information also increases. This excess\\ninformation can confuse the model, leading to a\\ndecline in answer quality. The trade-off, therefore,\\nis between improving recall by retrieving more\\ncontext and maintaining precision by limiting dis-\\ntractions. The optimal point is where the balance\\nbetween relevant and irrelevant information maxi-\\nmizes the quality of the answer. Beyond this point,'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\n\\nR : topk\\ndi∈D\\n\\nSim(q, di) → Dq\\n\\n(2)\\n\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\n\\nSim(q, di) = eq · edi\\n\\nor\\n\\neq · edi\\n∥eq∥ · ∥edi∥\\n\\n(3)\\n\\n3) Generation. After getting the relevant documents. The\\nquery q and the retrieved document Dq chunks are inputted\\ntogether to the LLM to generate the final answer, where [·, ·]\\nstands for concatenation.\\n\\nI = {e1, e2, . . . , en} and ei = fe(di) ∈ Rd\\n\\n(1)\\n\\ny = LLM([Dq, q])\\n\\n(4)\\n\\n\\x0c5\\n\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='the list of outputs\\n\\n20: return ˆy\\n\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A\\n\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\n\\nTermination of the loop is determined by a predefined number\\nof iterations.\\n\\nRecursive retrieval The characteristic feature of recursive\\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\\nits clear dependency on the previous step and its continuous\\ndeepening of retrieval. Typically, it follows a tree-like structure\\nand there is a clear termination mechanism as an exit condition\\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\\nally involves query transform, relying on the newly rewritten\\nquery for each retrieval.')],\n",
       " 'answer': 'Оценка ретроспективы (Retrieval evaluation) - это процесс оценки качества поиска информации в контексте запроса. Это включает в себя оценку точности, полноты и релевантности результатов поиска. Оценка может проводиться вручную или автоматически с использованием метрик, таких как F-мера, MAP (средний показатель позиции) и другие.\\n\\nЭволюция ответа (Response evolution) - это процесс улучшения качества ответа на основе обратной связи от пользователя. Это может включать в себя улучшение генеративной способности модели, обучение на основе примеров пользовательских ответов или корректировку алгоритмов обработки запросов.'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question2},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "#900 450 new prompt + chain of thought pdfminer Вопрос №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='2.3 Evaluation Metrics\\n\\nAn RAG system handling multi-hop queries can be\\nassessed from two key aspects: retrieval evaluation\\nand generation evaluation.'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1'}, page_content='• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\n\\n3\\n\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1'}, page_content='6 Conclusion\\n\\nIn this work, we introduce MultiHop-RAG, a novel\\nand unique dataset designed for queries that re-\\nquire retrieval and reasoning from multiple pieces\\nof supporting evidence. These types of multi-hop\\nqueries represent user queries commonly encoun-\\ntered in real-world scenarios. MultiHop-RAG con-\\nsists of a knowledge base, a large collection of\\nmulti-hop queries, their ground-truth answers, and\\nthe associated supporting evidence. This paper\\ndetails the creation process of MultiHop-RAG, em-\\nploying a hybrid approach that integrates human\\neffort with GPT-4. Additionally, we explore two\\nuse cases of MultiHop-RAG in the benchmarking\\nof RAG systems, thereby highlighting the potential\\napplications of this dataset. By publicly releas-\\ning MultiHop-RAG, we aim to provide a valuable\\nresource to the community, contributing to the ad-\\nvancement and benchmarking of RAG systems.')],\n",
       " 'answer': 'Сравнительный анализ системы RAG с использованием MultiHop-RAG включает оценку эффективности системы по двум ключевым аспектам: ретроспективная оценка и генеративная оценка.'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "#900 450 new prompt + chain of thought pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 7}, page_content='niques. We believe that there are many potential\\nareas for enhancing RAG’s performance on multi-\\nhop queries, and the curated dataset MultiHop-\\nRAG can be a valuable resource to the community.\\n5 Related Work\\nRAG Evaluation: As RAG systems gain increas-\\ning popularity, a variety of RAG benchmarking\\ndatasets and evaluation tools have been developed.\\nFor instance, RGB (Chen et al., 2023) and RE-\\nCALL (Liu et al., 2023) evaluate the performance\\nof LLMs in generating responses for RAG systems\\nunder conditions involving noisy, integrative, and\\ncounterfactual queries. However, both datasets pri-\\nmarily focus on evaluating the generation aspect\\nof RAG systems without specifically addressing\\ntheir retrieval accuracy. In addition, recent ad-\\nvancements have been made in automated RAG\\nevaluation tools, such as ARES (Saad-Falcon et al.,\\n2023) and RAGAS (Es et al., 2023). These tools'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1', 'page': 15}, page_content='tecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 1}, page_content='the same bridge-topic or bridge-entity, accompa-\\nnied by the correct answers. Lastly, we undertake\\na validation step to ensure the data quality.\\nWe demonstrate the benchmarking capabilities\\nof MultiHop-RAG using two experiments, utilizing\\na RAG system implemented with LlamaIndex (Liu,\\n2022). The first experiment involves a comparison\\nof different embedding models for retrieving rele-\\nvant evidence for multi-hop queries. In the second\\nexperiment, we assess the reasoning and answering\\nabilities of various state-of-the-art LLMs, including\\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\\nand Mixtral-8x7B, for multi-hop queries when re-\\ntrieved text is provided. The results from both ex-\\nperiments indicate that the current RAG implemen-\\ntations are inadequate for effectively retrieving and\\nanswering multi-hop queries. We publicly release')],\n",
       " 'answer': 'В сравнительный анализ системы RAG с использованием MultiHop-RAG входят следующие аспекты:\\n1. Оценка производительности RAG систем при выполнении мульти-хоповых запросов.\\n2. Исследование эффективности различных подходов к оценке RAG систем, таких как RGB и RE-CALL.\\n3. Анализ возможностей автоматизированных инструментов для оценки RAG систем, включая ARES и RAGAS.\\n4. Изучение модульной структуры RAG систем и ее влияния на эффективность работы.\\n5. Исследование примеров использования RAG систем и выявление общих дизайн-паттернов.\\n6. Определение потенциала для будущего развития RAG технологий через модульную структуру.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "#900 450 new prompt + chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 7}, page_content='niques. We believe that there are many potential\\nareas for enhancing RAG’s performance on multi-\\nhop queries, and the curated dataset MultiHop-\\nRAG can be a valuable resource to the community.\\n5 Related Work\\nRAG Evaluation: As RAG systems gain increas-\\ning popularity, a variety of RAG benchmarking\\ndatasets and evaluation tools have been developed.\\nFor instance, RGB (Chen et al., 2023) and RE-\\nCALL (Liu et al., 2023) evaluate the performance\\nof LLMs in generating responses for RAG systems\\nunder conditions involving noisy, integrative, and\\ncounterfactual queries. However, both datasets pri-\\nmarily focus on evaluating the generation aspect\\nof RAG systems without specifically addressing\\ntheir retrieval accuracy. In addition, recent ad-\\nvancements have been made in automated RAG\\nevaluation tools, such as ARES (Saad-Falcon et al.,\\n2023) and RAGAS (Es et al., 2023). These tools'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1', 'page': 15}, page_content='tecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 1}, page_content='the same bridge-topic or bridge-entity, accompa-\\nnied by the correct answers. Lastly, we undertake\\na validation step to ensure the data quality.\\nWe demonstrate the benchmarking capabilities\\nof MultiHop-RAG using two experiments, utilizing\\na RAG system implemented with LlamaIndex (Liu,\\n2022). The first experiment involves a comparison\\nof different embedding models for retrieving rele-\\nvant evidence for multi-hop queries. In the second\\nexperiment, we assess the reasoning and answering\\nabilities of various state-of-the-art LLMs, including\\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\\nand Mixtral-8x7B, for multi-hop queries when re-\\ntrieved text is provided. The results from both ex-\\nperiments indicate that the current RAG implemen-\\ntations are inadequate for effectively retrieving and\\nanswering multi-hop queries. We publicly release')],\n",
       " 'answer': 'В сравнительный анализ системы RAG с использованием MultiHop-RAG входят следующие аспекты:\\n1. Оценка производительности RAG систем: MultiHop-RAG предоставляет возможность оценить производительность RAG систем при выполнении мульти-хоповых запросов.\\n2. Разработка и тестирование RAG систем: MultiHop-RAG может использоваться для разработки и тестирования новых RAG систем, а также для оценки их эффективности.\\n3. Исследование архитектуры RAG систем: MultiHop-RAG позволяет провести детальное исследование архитектуры RAG систем, включая их модульную структуру и операционные функции.\\n4. Оценка RAG систем по критериям: MultiHop-RAG предоставляет возможность оценить RAG системы по различным критериям, таким как точность поиска, полнота ответов и другие.\\n5. Валидация данных: MultiHop-RAG включает в себя процесс валидации данных, чтобы убедиться в их качестве и надежности.'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "#900 450 new prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 7}, page_content='niques. We believe that there are many potential\\nareas for enhancing RAG’s performance on multi-\\nhop queries, and the curated dataset MultiHop-\\nRAG can be a valuable resource to the community.\\n5 Related Work\\nRAG Evaluation: As RAG systems gain increas-\\ning popularity, a variety of RAG benchmarking\\ndatasets and evaluation tools have been developed.\\nFor instance, RGB (Chen et al., 2023) and RE-\\nCALL (Liu et al., 2023) evaluate the performance\\nof LLMs in generating responses for RAG systems\\nunder conditions involving noisy, integrative, and\\ncounterfactual queries. However, both datasets pri-\\nmarily focus on evaluating the generation aspect\\nof RAG systems without specifically addressing\\ntheir retrieval accuracy. In addition, recent ad-\\nvancements have been made in automated RAG\\nevaluation tools, such as ARES (Saad-Falcon et al.,\\n2023) and RAGAS (Es et al., 2023). These tools'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1', 'page': 15}, page_content='tecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 1}, page_content='the same bridge-topic or bridge-entity, accompa-\\nnied by the correct answers. Lastly, we undertake\\na validation step to ensure the data quality.\\nWe demonstrate the benchmarking capabilities\\nof MultiHop-RAG using two experiments, utilizing\\na RAG system implemented with LlamaIndex (Liu,\\n2022). The first experiment involves a comparison\\nof different embedding models for retrieving rele-\\nvant evidence for multi-hop queries. In the second\\nexperiment, we assess the reasoning and answering\\nabilities of various state-of-the-art LLMs, including\\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\\nand Mixtral-8x7B, for multi-hop queries when re-\\ntrieved text is provided. The results from both ex-\\nperiments indicate that the current RAG implemen-\\ntations are inadequate for effectively retrieving and\\nanswering multi-hop queries. We publicly release')],\n",
       " 'answer': 'Сравнительный анализ системы RAG с использованием MultiHop-RAG включает в себя сравнение различных методов и подходов к оценке работы систем RAG, а также анализ их эффективности при решении многошаговых запросов. В рамках этого анализа проводится оценка различных моделей машинного обучения и языковых моделей, которые используются для генерации ответов на запросы. Также проводится анализ данных и проверка качества информации, используемой для обучения и тестирования систем RAG.'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "# 900 450 старый промпт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Что входит в Сравнительный анализ системы RAG с использованием MultiHop-RAG?',\n",
       " 'chat_history': [HumanMessage(content='What is Step-back Prompting?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Step-back Prompting is a technique used in RAG systems, which involves abstracting the original query into a high-level concept question (step-back question) and using both the step-back question and the original query for retrieval. Their results are combined to generate the language model's answer.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Что такое Step-back Prompting?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Step-back Prompting - это техника, используемая в системах RAG, которая включает абстрагирование исходного запроса в высокоуровневый вопрос о концепции (вопрос отступления). Оба вопроса отступления и исходный запрос используются для поиска, а их результаты комбинируются для генерации ответа языковой модели.', additional_kwargs={}, response_metadata={})],\n",
       " 'context': [Document(metadata={'source': 'http://arxiv.org/pdf/2401.15391v1', 'page': 1}, page_content='of LLMs for complex multi-hop queries. To ad-\\ndress this gap and make RAG benchmarking more\\nclosely resemble real-world scenarios, in this paper,\\nwe introduce MultiHop-RAG . To our knowledge,\\nMultiHop-RAG is one of the first RAG datasets\\nfocusing specifically on multi-hop queries.\\nBased on the RAG queries commonly encoun-\\ntered in real-world scenarios, we first categorize\\nmulti-hop queries into four types: Inference query ,\\nComparison query ,Temporal query , and Null\\nquery . The first three types — Inference, Com-\\nparison, and Temporal — require the retrieval and\\nanalysis of evidence from multiple sources, encom-\\npassing tasks like inferring relationships, compar-\\ning data points, and sequencing events over time.\\nThe Null query represents a scenario where the\\nquery cannot be derived from the knowledge base.\\nThis category is crucial for assessing whether an'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1', 'page': 3}, page_content='disadvantages. Zhao et al., [32]analyze the applications of\\nRAG technology in various fields such as text generation,\\ncode generation, image generation, and video generation from\\nthe perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with\\noptimizing the retrieval and generation stages. However, it has\\nnot turned its attention to the new characteristics that RAG\\nresearch is continuously evolving, namely the characteristics\\nof process scheduling and functional componentization. There\\nis currently a lack of comprehensive analysis of the overall\\nRAG system, which has led to research on paradigms lagging\\nbehind the development of RAG technology.\\nIII. F RAMEWORK AND NOTATION\\nFor query Q={qi}, a typical RAG system mainly consists'),\n",
       "  Document(metadata={'source': 'http://arxiv.org/pdf/2407.21059v1', 'page': 14}, page_content='patterns that are tailored to various application contexts and\\ndomains. This approach not only streamlines the development\\nprocess but also enriches the functionality and versatility of\\nRAG applications.\\nB. Compatibility with new methods\\nModular RAG paradigm demonstrates exceptional compati-\\nbility with new developments. To gain a deeper understandingof this, we list three typical scalability cases, which clearly\\nshows that Modular RAG paradigm provides robust support\\nand flexibility for the innovation and development of RAG\\ntechnology.\\n1) Recombination of the current modules: In this scenario,\\nno new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\\nand classifier selection mechanism, incorporating a branching')],\n",
       " 'answer': 'Сравнительный анализ системы RAG с использованием MultiHop-RAG включает сравнение характеристик и возможностей MultiHop-RAG с другими существующими системами RAG. Это может включать оценку точности, эффективности, производительности и других параметров.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "# 900 300 old prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U g4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install curl_cffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from g4f.client import Client\n",
    "\n",
    "# client = Client()\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"What is RAG?\"}],\n",
    "# )\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'export.arxiv.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Work\\Rag\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'export.arxiv.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The given context does not provide any information about MultiHop-RAG.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "retriever = ArxivRetriever(\n",
    "load_max_docs=2,\n",
    "get_ful_documents=True,)\n",
    "docs_arxiv = retriever.invoke('RAG')\n",
    "docs = []\n",
    "for doc in docs_arxiv:\n",
    "    pdf_path = doc.metadata[\"Entry ID\"].replace('abs','pdf')\n",
    "    response_load = requests.get(pdf_path)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response_load.content)\n",
    "            tmp_file_path = tmp_file.name\n",
    "\n",
    "    loader = PyPDFLoader(tmp_file_path)\n",
    "    # loader = PyPDFLoader(temp_file, extract_images=True) #если PDF в виде скана мб нужно боковой тогл добавить\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=300)\n",
    "    splitted_data = text_splitter.split_documents(documents)\n",
    "    for i in splitted_data:\n",
    "            i.metadata[\"source\"] = pdf_path\n",
    "    docs.extend(splitted_data)\n",
    "    os.remove(tmp_file_path)\n",
    "# retriever = ArxivRetriever(\n",
    "#     load_max_docs=2,\n",
    "#     get_ful_documents=True,\n",
    "# )\n",
    "# docs = retriever.invoke(\"What is the ImageBind model?\")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the context provided.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "credentials = os.getenv('credentials')\n",
    "llm = GigaChat(auth_url = 'https://sm-auth-sd.prom-88-89-apps.ocp-geo.ocp.sigma.sbrf.ru/api/v2/oauth',credentials=credentials, verify_ssl_certs=False)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is MultiHop-RAG?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
